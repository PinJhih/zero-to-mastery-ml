{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to the Zero to Mastery Data Science and Machine Learning Bootcamp!","text":"<p>This documentation is a work in progress and will be updated rapidly over the next few weeks.</p> <p>In the meantime, check out the following:</p> <ul> <li>\ud83d\udcda Get the course materials on the course GitHub</li> <li>\ud83c\udfa5 Watch the first 10 hours of the course on YouTube</li> <li>\ud83e\udd13 Read more on the course page</li> <li>\ud83d\udcbb Sign up to the course on Zero to Mastery and start coding</li> </ul> <p>Currently working on: Updating the Dog Vision project (classifying images of dogs with computer vision) for 2024.</p> <p>Best, </p> <p>Daniel Bourke</p> <p>Last update: 17 November 2023</p>"},{"location":"a-6-step-framework-for-approaching-machine-learning-projects/","title":"A 6 Step Framework for Approaching Machine Learning Projects","text":"<p>Machine learning is broad. The media makes it sound like magic. Reading this article will change that. It will give you an overview of the most common types of problems machine learning can be used for. And at the same time give you a framework to approach your future machine learning proof of concept projects.</p> <p>First, we\u2019ll clear up some definitions.</p> <p>How is machine learning, artificial intelligence and data science different?</p> <p>These three topics can be hard to understand because there are no formal definitions. Even after being a machine learning engineer for over a year, I don\u2019t have a good answer to this question. I\u2019d be suspicious of anyone who claims they do.</p> <p>To avoid confusion, we\u2019ll keep it simple. For this article, you can consider machine learning the process of finding patterns in data to understand something more or to predict some kind of future event.</p> <p>The following steps have a bias towards building something and seeing how it works. Learning by doing.</p>"},{"location":"a-6-step-framework-for-approaching-machine-learning-projects/#6-steps-for-your-next-machine-learning-project","title":"6 steps for your next machine learning project","text":"<p>A machine learning pipeline can be broken down into three major steps. Data collection, data modelling and deployment. All influence one another.</p> <p>You may start a project by collecting data, model it, realise the data you collected was poor, go back to collecting data, model it again, find a good model, deploy it, find it doesn\u2019t work, make another model, deploy it, find it doesn\u2019t work again, go back to data collection. It\u2019s a cycle.</p> <p>Wait, what does model mean? What\u2019s does deploy mean? How do I collect data?</p> <p>Great questions.</p> <p>How you collect data will depend on your problem. We will look at examples in a minute. But one way could be your customer purchases in a spreadsheet.</p> <p>Modelling refers to using a machine learning algorithm to find insights within your collected data.</p> <p>What\u2019s the difference between a normal algorithm and a machine learning algorithm?</p> <p>Like a cooking recipe for your favourite chicken dish, a normal algorithm is a set of instructions on how to turn a set of ingredients into that honey mustard masterpiece.</p> <p>What makes a machine learning algorithm different is instead of having the set of instructions, you start with the ingredients and the final dish ready to go. The machine learning algorithm then looks at the ingredients and the final dish and works out the set of instructions.</p> <p>There are many different types of machine learning algorithms and some perform better than others on different problems. But the premise remains, they all have the goal of finding patterns or sets of instructions in data.</p> <p>Deployment is taking your set of instructions and using it in an application. This application could be anything from recommending products to customers on your online store to a hospital trying to better predict disease presence.</p> <p>The specifics of these steps will be different for each project. But the principles within each remain similar.</p> <p>This article focuses on data modelling. It assumes you have already collected data, and are looking to build a machine learning proof of concept with it. Let\u2019s break down how you might approach it.</p> Machine learning projects can be broken into three steps, data collection, data modelling and deployment. This article focuses on steps within the data modelling phase and assumes you already have data. Full version on Whimsical. <ol> <li>Problem definition\u200a\u2014\u200aWhat business problem are we trying to solve? How can it be phrased as a machine learning problem?</li> <li>Data\u200a\u2014\u200aIf machine learning is getting insights out of data, what data we have? How does it match the problem definition? Is our data structured or unstructured? Static or streaming?</li> <li>Evaluation\u200a\u2014\u200aWhat defines success? Is a 95% accurate machine learning model good enough?</li> <li>Features\u200a\u2014\u200aWhat parts of our data are we going to use for our model? How can what we already know influence this?</li> <li>Modelling\u200a\u2014\u200aWhich model should you choose? How can you improve it? How do you compare it with other models?</li> <li>Experimentation\u200a\u2014\u200aWhat else could we try? Does our deployed model do as we expected? How do the other steps change based on what we\u2019ve found?</li> </ol> <p>Let\u2019s dive a little deeper in each.</p>"},{"location":"a-6-step-framework-for-approaching-machine-learning-projects/#1-problem-definition-rephrase-your-business-problem-as-a-machine-learning-problem","title":"1. Problem definition\u200a\u2014\u200aRephrase your business problem as a machine learning problem","text":"<p>To help decide whether or not your business could use machine learning, the first step is to match the business problem you\u2019re trying to solve a machine learning problem.</p> <p>The four major types of machine learning are supervised learning, unsupervised learning, transfer learning and reinforcement learning (there\u2019s semi-supervised as well but I\u2019ve left it out for brevity). The three most used in business applications are supervised learning, unsupervised learning and transfer learning.</p>"},{"location":"a-6-step-framework-for-approaching-machine-learning-projects/#supervised-learning","title":"Supervised learning","text":"<p>Supervised learning, is called supervised because you have data and labels. A machine learning algorithm tries to learn what patterns in the data lead to the labels. The supervised part happens during training. If the algorithm guesses a wrong label, it tries to correct itself.</p> <p>For example, if you were trying to predict heart disease in a new patient. You may have the anonymised medical records of 100 patients as the data and whether or not they had heart disease as the label.</p> <p>A machine learning algorithm could look at the medical records (inputs) and whether or not a patient had heart disease (outputs) and then figure out what patterns in the medical records lead to heart disease.</p> <p>Once you\u2019ve got a trained algorithm, you could pass through the medical records (input) of a new patient through it and get a prediction of whether or not they have heart disease (output). It\u2019s important to remember this prediction isn\u2019t certain. It comes back as a probability.</p> <p>The algorithm says, \u201cbased on what I\u2019ve seen before, it looks like this new patients medical records are 70% aligned to those who have heart disease.\u201d</p>"},{"location":"a-6-step-framework-for-approaching-machine-learning-projects/#unsupervised-learning","title":"Unsupervised learning","text":"<p>Unsupervised learning is when you have data but no labels. The data could be the purchase history of your online video game store customers. Using this data, you may want to group similar customers together so you can offer them specialised deals. You could use a machine learning algorithm to group your customers by purchase history.</p> <p>After inspecting the groups, you provide the labels. There may be a group interested in computer games, another group who prefer console games and another which only buy discounted older games. This is called clustering.</p> <p>What\u2019s important to remember here is the algorithm did not provide these labels. It found the patterns between similar customers and using your domain knowledge, you provided the labels.</p>"},{"location":"a-6-step-framework-for-approaching-machine-learning-projects/#transfer-learning","title":"Transfer learning","text":"<p>Transfer learning is when you take the information an existing machine learning model has learned and adjust it to your own problem.</p> <p>Training a machine learning model from scratch can be expensive and time-consuming. The good news is, you don\u2019t always have to. When machine learning algorithms find patterns in one kind of data, these patterns can be used in another type of data.</p> <p>Let\u2019s say you\u2019re a car insurance company and wanted to build a text classification model to classify whether or not someone submitting an insurance claim for a car accident is at fault (caused the accident) or not at fault (didn\u2019t cause the accident).</p> <p>You could start with an existing text model, one which has read all of Wikipedia and has remembered all the patterns between different words, such as, which word is more likely to come next after another. Then using your car insurance claims (data) along with their outcomes (labels), you could tweak the existing text model to your own problem.</p> <p>If machine learning can be used in your business, it\u2019s likely it\u2019ll fall under one of these three types of learning. But let\u2019s break them down further into classification, regression and recommendation.</p> <ul> <li>Classification\u200a\u2014\u200aDo you want to predict whether something is one thing or another? Such as whether a customer will churn or not churn? Or whether a patient has heart disease or not? Note, there can be more than two things. Two classes is called binary classification, more than two classes is called multi-class classification. Multi-label is when an item can belong to more than one class.</li> <li>Regression\u200a\u2014\u200aDo you want to predict a specific number of something? Such as how much a house will sell for? Or how many customers will visit your site next month?</li> <li>Recommendation\u200a\u2014\u200aDo you want to recommend something to someone? Such as products to buy based on their previous purchases? Or articles to read based on their reading history?</li> </ul> <p>Now you know these things, your next step is to define your business problem in machine learning terms.</p> <p>Let\u2019s use the car insurance example from before. You receive thousands of claims per day which your staff read and decide whether or not the person sending in the claim is at fault or not.</p> <p>But now the number of claims are starting to come in faster than your staff can handle them. You\u2019ve got thousands of examples of past claims which are labelled at fault or not at fault.</p> <p>Can machine learning help?</p> <p>You already know the answer. But let\u2019s see. Does this problem fit into any of the three above? Classification, regression or recommendation?</p> <p>Let\u2019s rephrase it.</p> <p>We\u2019re a car insurance company who want to classify incoming car insurance claims into at fault or not at fault.</p> <p>See the keyword? Classify.</p> <p>It turns out, this could potentially be a machine learning classification problem. I say potentially because there\u2019s a chance it might not work.</p> <p>When it comes to defining your business problem as a machine learning problem, start simple, more than one sentence is too much. Add complexity when required.</p>"},{"location":"a-6-step-framework-for-approaching-machine-learning-projects/#2-data-if-machine-learning-is-getting-insights-out-of-data-what-data-do-you-have","title":"2. Data\u200a\u2014\u200aIf machine learning is getting insights out of data, what data do you have?","text":"<p>The data you have or need to collect will depend on the problem you want to solve.</p> <p>If you already have data, it\u2019s likely it will be in one of two forms. Structured or unstructured. Within each of these, you have static or streaming data.</p> <ul> <li>Structured data\u200a\u2014\u200aThink a table of rows and columns, an Excel spreadsheet of customer transactions, a database of patient records. Columns can be numerical, such as average heart rate, categorical, such as sex, or ordinal, such as chest pain intensity.</li> <li>Unstructured data\u200a\u2014\u200aAnything not immediately able to be put into row and column format, images, audio files, natural language text.</li> <li>Static data\u200a\u2014\u200aExisting historical data which is unlikely to change. Your companies customer purchase history is a good example.</li> <li>Streaming data\u200a\u2014\u200aData which is constantly updated, older records may be changed, newer records are constantly being added.</li> </ul> <p>There are overlaps.</p> <p>Your static structured table of information may have columns which contain natural language text and photos and be updated constantly.</p> <p>For predicting heart disease, one column may be sex, another average heart rate, another average blood pressure, another chest pain intensity.</p> <p>For the insurance claim example, one column may be the text a customer has sent in for the claim, another may be the image they\u2019ve sent in along with the text and a final a column being the outcome of the claim. This table gets updated with new claims or altered results of old claims daily.</p> Two examples of structured data with different kinds of data within it. Table 1.0 has numerical and categorical data. Table 2.0 has unstructured data with images and natural language text but is presented in a structured manner. <p>The principle remains. You want to use the data you have to gains insights or predict something.</p> <p>For supervised learning, this involves using the feature variable(s) to predict the target variable(s). A feature variable for predicting heart disease could be sex with the target variable being whether or not the patient has heart disease.</p> Table 1.0 broken into ID column (yellow, not used for building machine learning model), feature variables (orange) and target variables (green). A machine learning model finds the patterns in the feature variables and predicts the target variables. <p>For unsupervised learning, you won\u2019t have labels. But you\u2019ll still want to find patterns. Meaning, grouping together similar samples and finding samples which are outliers.</p> <p>For transfer learning, your problem stays a supervised learning problem, except you\u2019re leveraging the patterns machine learning algorithms have learned from other data sources separate from your own.</p> <p>Remember, if you\u2019re using a customers data to improve your business or to offer them a better service, it\u2019s important to let them know. This is why you see \u201cthis site uses cookies\u201d popups everywhere. The website uses how you browse the site, likely along with some kind of machine learning to improve their offering.</p>"},{"location":"a-6-step-framework-for-approaching-machine-learning-projects/#3-evaluation-what-defines-success-is-a-95-accurate-machine-learning-model-good-enough","title":"3. Evaluation\u200a\u2014\u200aWhat defines success? Is a 95% accurate machine learning model good enough?","text":"<p>You\u2019ve defined your business problem in machine learning terms and you have data. Now define what defines success. There are different evaluation metrics for classification, regression and recommendation problems. Which one you choose will depend on your goal.</p> <p>For this project to be successful, the model needs to be over 95% accurate at whether someone is at fault or not at fault.</p> <p>A 95% accurate model may sound pretty good for predicting who\u2019s at fault in an insurance claim. But for predicting heart disease, you\u2019ll likely want better results.</p> <p>Other things you should take into consideration for classification problems.</p> <ul> <li>False negatives\u200a\u2014\u200aModel predicts negative, actually positive. In some cases, like email spam prediction, false negatives aren\u2019t too much to worry about. But if a self-driving cars computer vision system predicts no pedestrian when there was one, this is not good.</li> <li>False positives\u200a\u2014\u200aModel predicts positive, actually negative. Predicting someone has heart disease when they don\u2019t, might seem okay. Better to be safe right? Not if it negatively affects the person\u2019s lifestyle or sets them on a treatment plan they don\u2019t need.</li> <li>True negatives\u200a\u2014\u200aModel predicts negative, actually negative. This is good.</li> <li>True positives\u200a\u2014\u200aModel predicts positive, actually positive. This is good.</li> <li>Precision\u200a\u2014\u200aWhat proportion of positive predictions were actually correct? A model that produces no false positives has a precision of 1.0.</li> <li>Recall\u200a\u2014\u200aWhat proportion of actual positives were predicted correctly? A model that produces no false negatives has a recall of 1.0.</li> <li>F1 score\u200a\u2014\u200aA combination of precision and recall. The closer to 1.0, the better.</li> <li>Receiver operating characteristic (ROC) curve &amp; Area under the curve (AUC)\u200a\u2014\u200aThe ROC curve is a plot comparing true positive and false positive rate. The AUC metric is the area under the ROC curve. A model whose predictions are 100% wrong has an AUC of 0.0, one whose predictions are 100% right has an AUC of 1.0.</li> </ul> <p>For regression problems (where you want to predict a number), you\u2019ll want to minimise the difference between what your model predicts and what the actual value is. If you\u2019re trying to predict the price a house will sell for, you\u2019ll want your model to get as close as possible to the actual price. To do this, use MAE or RMSE.</p> <ul> <li>Mean absolute error (MAE)\u200a\u2014\u200aThe average difference between your model's predictions and the actual numbers.</li> <li>Root mean square error (RMSE)\u200a\u2014\u200aThe square root of the average of squared differences between your model's predictions and the actual numbers.</li> </ul> <p>Use RMSE if you want large errors to be more significant. Such as, predicting a house to be sold at $300,000 instead of $200,000 and being off by $100,000 is more than twice as bad as being off by $50,000. Or MAE if being off by $100,000 is twice as bad as being off by $50,000.</p> <p>Recommendation problems are harder to test in experimentation. One way to do so is to take a portion of your data and hide it away. When your model is built, use it to predict recommendations for the hidden data and see how it lines up.</p> <p>Let\u2019s say you\u2019re trying to recommend customers products on your online store. You have historical purchase data from 2010\u20132019. You could build a model on the 2010\u20132018 data and then use it to predict 2019 purchases. Then it becomes a classification problem because you\u2019re trying to classify whether or not someone is likely to buy an item.</p> <p>However, traditional classification metrics aren\u2019t the best for recommendation problems. Precision and recall have no concept of ordering.</p> <p>If your machine learning model returned back a list of 10 recommendations to be displayed to a customer on your website, you\u2019d want the best ones to be displayed first right?</p> <ul> <li>Precision @ k (precision up to k)\u200a\u2014\u200aSame as regular precision, however, you choose the cutoff, k. For example, precision at 5, means we only care about the top 5 recommendations. You may have 10,000 products. But you can\u2019t recommend them all to your customers.</li> </ul> <p>To begin with, you may not have an exact figure for each of these. But knowing what metrics you should be paying attention to gives you an idea of how to evaluate your machine learning project.</p>"},{"location":"a-6-step-framework-for-approaching-machine-learning-projects/#4-features-what-features-does-your-data-have-and-which-can-you-use-to-build-your-model","title":"4. Features\u200a\u2014\u200aWhat features does your data have and which can you use to build your model?","text":"<p>Not all data is the same. And when you hear someone referring to features, they\u2019re referring to different kinds of data within data.</p> <p>The three main types of features are categorical, continuous (or numerical) and derived.</p> <ul> <li> <p>Categorical features\u200a\u2014\u200aOne or the other(s). For example, in our heart disease problem, the sex of the patient. Or for an online store, whether or not someone has made a purchase or not.</p> </li> <li> <p>Continuous (or numerical) features\u200a\u2014\u200aA numerical value such as average heart rate or the number of times logged in. Derived features\u200a\u2014\u200aFeatures you create from the data. Often referred to as feature engineering. Feature engineering is how a subject matter expert takes their knowledge and encodes it into the data. You might combine the number of times logged in with timestamps to make a feature called time since last login. Or turn dates from numbers into \u201cis a weekday (yes)\u201d and \u201cis a weekday (no)\u201d.</p> </li> </ul> <p>Text, images and almost anything you can imagine can also be a feature. Regardless, they all get turned into numbers before a machine learning algorithm can model them.</p> <p>Some important things to remember when it comes to features.</p> <ul> <li>Keep them the same during experimentation (training) and production (testing)\u200a\u2014\u200aA machine learning model should be trained on features which represent as close as possible to what it will be used for in a real system.</li> <li>Work with subject matter experts\u200a\u2014\u200aWhat do you already know about the problem, how can that influence what features you use? Let your machine learning engineers and data scientists know this.</li> <li>Are they worth it?\u200a\u2014\u200aIf only 10% of your samples have a feature, is it worth incorporating it in a model? Have a preference for features with the most coverage. The ones where lots of samples have data for.</li> <li>Perfect equals broken\u200a\u2014\u200aIf your model is achieving perfect performance, you\u2019ve likely got feature leakage somewhere. Which means the data your model has trained on is being used to test it. No model is perfect.</li> </ul> <p>You can use features to create a simple baseline metric. A subject matter expert on customer churn may know someone is 80% likely to cancel their membership after 3 weeks of not logging in.</p> <p>Or a real estate agent who knows the sale prices of houses might know houses with over 5 bedrooms and 4 bathrooms sell for over $500,000.</p> <p>These are simplified and don\u2019t have to be exact. But it\u2019s what you\u2019re going to use to see whether machine learning can improve upon or not.</p>"},{"location":"a-6-step-framework-for-approaching-machine-learning-projects/#5-modelling-which-model-should-you-choose-how-can-you-improve-it-how-do-you-compare-it-with-other-models","title":"5. Modelling\u200a\u2014\u200aWhich model should you choose? How can you improve it? How do you compare it with other models?","text":"<p>Once you\u2019ve defined your problem, prepared your data, evaluation criteria and features it\u2019s time to model.</p> <p>Modelling breaks into three parts, choosing a model, improving a model, comparing it with others.</p>"},{"location":"a-6-step-framework-for-approaching-machine-learning-projects/#choosing-a-model","title":"Choosing a model","text":"<p>When choosing a model, you\u2019ll want to take into consideration, interpretability and ease to debug, amount of data, training and prediction limitations.</p> <ul> <li>Interpretability and ease to debug\u200a\u2014\u200aWhy did a model make a decision it made? How can the errors be fixed?</li> <li>Amount of data\u200a\u2014\u200aHow much data do you have? Will this change?</li> <li>Training and prediction limitations\u200a\u2014\u200aThis ties in with the above, how much time and resources do you have for training and prediction?</li> </ul> <p>To address these, start simple. A state of the art model can be tempting to reach for. But if it requires 10x the compute resources to train and prediction times are 5x longer for a 2% boost in your evaluation metric, it might not be the best choice.</p> <p>Linear models such as logistic regression are usually easier to interpret, are very fast for training and predict faster than deeper models such as neural networks.</p> <p>But it\u2019s likely your data is from the real world. Data from the real world isn\u2019t always linear.</p> <p>What then?</p> <p>Ensembles of decision trees and gradient boosted algorithms (fancy words, definitions not important for now) usually work best on structured data, like Excel tables and dataframes. Look into random forests, XGBoost and CatBoost.</p> A non-exhaustive example of all the different tools you can use for machine learning/data science. <p>Deep models such as neural networks generally work best on unstructured data like images, audio files and natural language text. However, the trade-off is they usually take longer to train, are harder to debug and prediction time takes longer. But this doesn\u2019t mean you shouldn\u2019t use them.</p> <p>Transfer learning is an approach which takes advantage of deep models and linear models. It involves taking a pre-trained deep model and using the patterns it has learned as the inputs to your linear model. This saves dramatically on training time and allows you to experiment faster.</p> <p>Where do I find pre-trained models?</p> <p>Pre-trained models are available on PyTorch hub, TensorFlow hub, model zoo and within the fast.ai framework. This is a good place to look first for building any kind of proof of concept.</p> <p>What about the other kinds of models?</p> <p>For building a proof of concept, it\u2019s unlikely you\u2019ll have to ever build your own machine learning model. People have already written code for these.</p> <p>What you\u2019ll be focused on is preparing your inputs and outputs in a way they can be used with an existing model. This means having your data and labels strictly defined and understanding what problem you\u2019re trying to solve.</p>"},{"location":"a-6-step-framework-for-approaching-machine-learning-projects/#tuning-and-improving-a-model","title":"Tuning and improving a model","text":"<p>A model's first results isn\u2019t its last. Like tuning a car, machine learning models can be tuned to improve performance.</p> <p>Tuning a model involves changing hyperparameters such as learning rate or optimizer. Or model-specific architecture factors such as number of trees for random forests and number of and type of layers for neural networks.</p> <p>These used to be something a practitioner would have to tune by hand but are increasingly becoming automated. And should be wherever possible.</p> <p>Using a pre-trained model through transfer learning often has the added benefit of all of these steps been done.</p> <p>The priority for tuning and improving models should be reproducibility and efficiency. Someone should be able to reproduce the steps you\u2019ve taken to improve performance. And because your main bottleneck will be model training time, not new ideas to improve, your efforts should be dedicated towards efficiency.</p>"},{"location":"a-6-step-framework-for-approaching-machine-learning-projects/#comparing-models","title":"Comparing models","text":"<p>Compare apples to apples.</p> <ul> <li>Model 1, trained on data X, evaluated on data Y.</li> <li>Model 2, trained on data X, evaluated on data Y.</li> </ul> <p>Where model 1 and 2 can vary but not data X or data Y.</p>"},{"location":"a-6-step-framework-for-approaching-machine-learning-projects/#6-experimentation-what-else-could-we-try-how-do-the-other-steps-change-based-on-what-weve-found-does-our-deployed-model-do-as-we-expected","title":"6. Experimentation\u200a\u2014\u200aWhat else could we try? How do the other steps change based on what we\u2019ve found? Does our deployed model do as we expected?","text":"<p>This step involves all the other steps. Because machine learning is a highly iterative process, you\u2019ll want to make sure your experiments are actionable.</p> <p>Your biggest goal should be minimising the time between offline experiments and online experiments.</p> <p>Offline experiments are steps you take when your project isn\u2019t customer-facing yet. Online experiments happen when your machine learning model is in production.</p> <p>All experiments should be conducted on different portions of your data.</p> <ul> <li>Training data set\u200a\u2014\u200aUse this set for model training, 70\u201380% of your data is the standard.</li> <li>Validation/development data set\u200a\u2014\u200aUse this set for model tuning, 10\u201315% of your data is the standard.</li> <li>Test data set\u200a\u2014\u200aUse this set for model testing and comparison, 10\u201315% of your data is the standard.</li> </ul> <p>These amounts can fluctuate slightly, depending on your problem and the data you have.</p> <p>Poor performance on training data means the model hasn\u2019t learned properly. Try a different model, improve the existing one, collect more data, collect better data.</p> <p>Poor performance on test data means your model doesn\u2019t generalise well. Your model may be overfitting the training data. Use a simpler model or collect more data.</p> <p>Poor performance once deployed (in the real world) means there\u2019s a difference in what you trained and tested your model on and what is actually happening. Revisit step 1 &amp; 2. Ensure your data matches up with the problem you\u2019re trying to solve.</p> <p>When you implement a large experimental change, document what and why. Remember, like model tuning, someone, including your future self, should be able to reproduce what you\u2019ve done.</p> <p>This means saving updated models and updated datasets regularly.</p>"},{"location":"a-6-step-framework-for-approaching-machine-learning-projects/#putting-it-together-in-a-proof-of-concept","title":"Putting it together in a proof of concept","text":"<p>Many businesses have heard of machine learning but aren\u2019t sure where to start. One of the best places to start is to use the six steps above to build a proof of concept.</p> <p>A proof of concept should not be seen as something to fundamentally change how your business operates but as an exploration into whether machine learning can bring your business value.</p> <p>After all, you\u2019re not after fancy solutions to keep up with the hype. You\u2019re after solutions which add value.</p> <p>Put a timeline on a proof of concept, 2, 6 and 12 weeks are good amounts. With good data, a good machine learning and data science practitioner can get 80\u201390% of the final modelling results in a relatively small timeframe.</p> <p>Have your subject matter experts and machine learning engineers and data scientists work together. There is nothing worse than a machine learning engineer building a great model which models the wrong thing.</p> <p>If a web designer could improve the layout of an online store to help a machine learning experiment, they should know.</p> <p>Remember, due to the nature of proof of concepts, it may turn out machine learning isn\u2019t something your business can take advantage of (unlikely). As a project manager, ensure you\u2019re aware of this. If you are a machine learning engineer or data scientist, be willing to accept your conclusions lead nowhere.</p> <p>But all is not lost.</p> <p>The value in something not working is now you know what doesn\u2019t work and can direct your efforts elsewhere. This is why setting a timeframe for experiments is helpful. There is never enough time but deadlines work wonders.</p> <p>If a machine learning proof of concept turns out well, take another step, if not, step back. Learning by doing is a faster process than thinking about something.</p>"},{"location":"a-6-step-framework-for-approaching-machine-learning-projects/#things-this-article-has-missed","title":"Things this article has missed","text":"<p>Each of these steps could deserve an article on their own. I\u2019ll work on it.</p> <p>In the meantime, there are some things to note.</p> <p>It\u2019s always about the data. Without good data to begin with, no machine learning model will help you. If you want to use machine learning in your business, it starts with good data collection.</p> <p>Deployment changes everything. A good model offline doesn\u2019t always mean a good model online. This article has focused on data modelling. Once you deploy a model, there\u2019s infrastructure management, data verification, model retraining, analysis and more. Any cloud provider has services for these but putting them together is still a bit of a dark art. Pay your data engineers well. If you\u2019re data engineer, share what you know.</p> <p>Data collection and model deployment are the longest parts of a machine learning pipeline. This article has only focused on modelling. And even then, it misses specifics on how to get your data ready to be modelled (other sections in this repo cover that).</p> <p>Tools of the trade vary. Machine learning is big tool comprised of many other tools. From code libraries and frameworks to different deployment architectures. There\u2019s usually several different ways to do the same thing. Best practice is continually being changed. This article focuses on things which don\u2019t.</p>"},{"location":"communicating-your-work/","title":"Communicating and Sharing Your Work as a Data Scientist/Machine Learning Engineer","text":"<p>This article is nearly 3000 words long but you can summarise it in 3.</p> <p>3 words in the form of a question.</p> <p>Whenever you're communicating your work, ask yourself, \"Who's it for?\".</p> <p>That's your start. Build upon it. Dig deeper. Got an idea of who your work is for? What questions will they have? What needs do they have? What concerns can you address before they arise?</p> <p>You'll never be able to fully answer these questions but it pays to think about them in advance.</p> <p>Having a conversation with your potential audience is a warm up for the actual conversation.</p> <p>Communicating your work is an unsolved challenge. But that's what makes it fun. What may make complete sense in your head could be a complete mystery to someone else.</p> <p>If you want your message to be heard, it's not enough for you to deliver it in a way someone can hear it. You have to deliver it in a way it can be understood.</p> <p>Imagine a man yelling in the middle of the street. His message can be heard. But no matter what he's talking about, it's unlikely it'll be understood.</p> <p>Let's break this down.</p> <p>After asking yourself, \"Who's it for?\", you'll start to realise there are two main audiences for your work. Those on your team, your boss, your manager, the people you sit next to and those who aren't, your clients, your customers, your fans. These can be broken down further and have plenty of overlaps but they're where we'll start.</p> <p>You'll also start to realise, your work isn't for everyone. A beginner's mistake is thinking too broadly. A message which appeals to everyone may convey information but it'll lack substance. You want the gut punch reaction.</p> <p>To begin, let's pretend you've asked yourself, \"Who's it for?\", and your answer is someone you work with, your teammates, your manager, someone on the internet reading about your latest technical project.</p>"},{"location":"communicating-your-work/#communicating-with-people-on-your-team","title":"Communicating with people on your team","text":"<p>All non-technical problems are communication problems. Often, you'll find these harder to solve than the technical problems. Technical problems, unless bounded by the laws of physics, have a finite solution. Communication problems don't.</p>"},{"location":"communicating-your-work/#what-do-they-need-to-know","title":"What do they need to know?","text":"<p>After asking yourself, \"Who's it for?\", a question you should follow up with is, \"What do they need to know?\".</p> <p>What your teammate may need to know might be different to what your manager needs to know.</p> <p>When answering this for yourself, lean on the side of excess. Write it down for later. The worst case is, you figure out what's not needed.</p> Start with \"Who's it for?\" and follow it up with \"What do they need to know?\". When answering these questions, write your questions and answers down. Writing helps to clear your thinking. It also gives you a resource you can refer to later."},{"location":"communicating-your-work/#the-project-manager-boss-senior-lead","title":"The Project Manager, Boss, Senior, Lead","text":"<p>Your project manager, Amber, has a mission. Aside from taking care of you and the team, she's determined to keep the project running on time and on budget.</p> <p>This translates to: keeping obstacles out of your way.</p> <p>So her questions will often come in some form of \"What's holding you back?\".</p> <p>It should go without saying, honesty is your best friend here. Life happens. When challenges come up, Amber should know about them.</p> <p>That's what Amber is there for. She's there to help oversee and figure out the challenges, she's there to connect you with people who might be able to help.</p> <p>When preparing a report, align it to the questions and concerns Amber may have. If you\u2019ve asked yourself, \u201cWhat does Amber need to know?\u201d, start with the answers.</p> Your bosses primary job is to take care of you and challenging you (if not, get a new boss). After this, it's in their best interest for projects to run on budget and time. This means keeping obstacles out of your way. If something is holding you back, you should let them know."},{"location":"communicating-your-work/#the-people-youre-working-with-sitting-next-to-in-the-group-chat","title":"The People You're Working With, Sitting Next to, in the Group Chat","text":"<p>It saddens me how many communication channels there are now. Most of them encourage communicating too often. Unless it's an emergency, \"Now\" is often never the best time.</p> <p>Projects you work on will have arbitrarily long timelines, with many milestones, plans and steps. Keep in mind the longer the timescale, the worse humans are at dealing with them.</p> <p>Break it down. Days and weeks are much easier units of time to understand.</p> Example of how a 6-month project becomes a day-by-day project. <p>What are you working on this week? Write it down, share it with the team. This not only consolidates your thinking, it gives your team an opportunity to ask questions and offer advice.</p> <p>Set a reminder for the end of each day. Have it ask, \"What did you work on today?\". Your response doesn't have to be long but it should be written down.</p> <p>You could use the following template.</p> <p>What I worked on today (1-3 points on what you did):</p> <ul> <li>What's working?</li> <li>What's not working?</li> <li>What could be improved?</li> </ul> <p>What I'm working on next:</p> <ul> <li>What's your next course of action? (based on the above)</li> <li>Why?</li> <li>What's holding you back?</li> </ul> <p>After you've written down answers, you should share them with your team.</p> <p>The beauty of a daily reflection like this is you've got a history, a playbook, a thought process. Plus, this style of communication is far better than little bits and pieces scattered throughout the day.</p> <p>You may be tempted to hold something back because it's not perfect, not fully thought out, but that's what your teammates are for. To help you figure it out. The same goes for the reverse. Help each other.</p> <p>Relate these daily and weekly communications back to the overall project goal. A 6-month project seems like a lot to begin with but breaking it down week by week, day by day, helps you and the people around you know what's going on.</p> <p>Take note of questions which arise. If a question gets asked more than 3 times, it should be documented somewhere for others to reference.</p> <p>You'll see some of the communication points for the people you're sitting with crossover with your project manager and vice versa. You're smart enough to figure out when to use each.</p>"},{"location":"communicating-your-work/#start-the-job-before-you-have-it","title":"Start the job before you have it","text":"<p>It can be hard to communicate with a project manager, boss or teammates if you don't have a job. And if you've recently learned some skills through an online course, it can be tempting to jump straight into the next one.</p> <p>But what are you really chasing?</p> <p>Are you after more certificates or more skills?</p> <p>No matter how good the course, you can assume the skills you learn there will be commoditised. That means, many other people will have gone through the same course, acquired the skills and then will be looking for similar jobs to what you are.</p> <p>If Janet posts a job vacancy and receives 673 applicants through an online form, you can imagine how hard it is for your resume to stand out.</p> <p>This isn't to say you shouldn't apply through an online form but if you're really serious about getting a role somewhere, start the job before you have it.</p> <p>How?</p> <p>By working on and sharing your own projects which relate to the role you're applying for.</p> <p>I call this the weekend project principle. During the week you're building foundational skills through various courses. But on the weekend, you design your own projects, projects inline with the role you're after and work on them.</p> <p>Let\u2019s see it in practice.</p> <p>Melissa and Henry apply for a data scientist role. They both make it through to interviews and are sitting with Janet. Janet looks at both their resumes and notices they've both done similar style courses.</p> <p>She asks Joe if he's worked on any of his own projects and he tells her, no he's only had a chance to work on coursework but has plenty of ideas.</p> <p>She asks Melissa the same. She pulls out her phone and tells Janet she's built a small app to help read food labels. Her daughter can't have gluten and got confused every time she tried to figure out what was in the food she was eating. The app isn't perfect but Melissa tells the story of how her daughter has figured out a few foods she should avoid and a few others which are fine.</p> <p>If you were Janet, who would you lean towards?</p> <p>Working on your own projects helps you build specific knowledge, they're what compound knowledge into skill, skill which can't be taught in courses.</p> <p>What should you work on?</p> <p>The hard part is you've unlimited options. The best part is you've got unlimited options.</p> <p>One method is to find the ideal company and ideal role you're going for. And then do your research.</p> <p>What does a person in that position day-to-day? Figure it out and then replicate it. Design yourself a 6-week project based on what you find.</p> <p>Why 6-weeks? The worst case is, if it doesn't work out, it's only 6 weeks. The best case is, you'll surprise yourself at what you can accomplish in 42-days.</p> <p>If you're still stuck, follow your interests. Use the same timeline except this time, choose something which excites you and see where it goes. Remember, the worst case is, after 6-weeks, you'll know whether to pursue it (another 6 weeks) or move onto the next thing.</p> <p>Now instead of only having a collection of certificates, you've got a story to tell. You've got evidence of you trying to put what you've learned into practice (exactly what you'll be doing in a job).</p> <p>And if you're wondering where the evidence comes from, it comes from you documenting your work.</p> <p>Where?</p> <p>On your own blog.</p> <p>Why a blog?</p> <p>We've discussed this before but it's worth repeating. Writing down what you're working on, helps solidify your thinking. It also helps others learn what you\u2019ve figured out.</p> <p>You could start with a post per week detailing how your 6-week project is going, what you've figured out, what you're doing next. Again, your project doesn't have to be perfect, none are, and your writing doesn't have to be perfect either.</p> <p>By the end of the 6-weeks, you'll have a series of articles detailing your work. </p> <p>Something you can point to and say, \"This is what I've done.\"</p> <p>If you're looking for resources to start a blog, Devblog by Hashnode and fast_template by the fast.ai team are both free and require almost zero setup. Medium is the next best place.</p> <p>Share your articles on Twitter, LinkedIn or even better, send them directly to the person in charge of hiring for the role you're after. You're crafty enough to find them.</p>"},{"location":"communicating-your-work/#communicating-with-those-outside-your-team","title":"Communicating with those outside your team","text":"<p>When answering \"Who's it for?\u201d results in someone who doesn't think like you, customers, clients, fans, it's also important to follow up with \"What do they need to know?\".</p> <p>A reminder: The line between people on your team and outside your team isn\u2019t set in stone. The goal of these exercises and techniques are to get you thinking from the perspective of the person you are trying to communicate with.</p>"},{"location":"communicating-your-work/#clients-customers-fans","title":"Clients, Customers &amp; Fans","text":"<p>I made a presentation for a board meeting once. We were there to present our results on a recent software proof of concept to some executives. Being an engineer, my presentation slides were clogged with detailed text, barely large enough to read. It contained every detail of the project, the techniques used, theories, code, acronyms with no definition. The presentation looked great to other engineers but caused the executives to squint, lean in and ignore everything being said in an attempt to read them.</p> <p>Once we made it through to the end, a slide with a visual appeared, to which, I palmed off as unnecessary but immediately sparked the interest of the executives.</p> <p>\"What's that?\", one asked.</p> <p>We spent the next 45-minutes discussing that one slide in detail. The slide which to me, didn\u2019t matter.</p> <p>The lesson here is what you think is important may be the opposite to others. And what's obvious to you could be amazing to others.</p> <p>Knowing this, you'll start to realise, unless they directly tell you, figuring out what your clients, customers and fans want to know is a challenge.</p> <p>There's a simple solution to this. </p> <p>Ask.</p> <p>Most people have a lot to offer but rarely volunteer it. Ask if what you're saying is clear, ask if there is anything else they'd like to see.</p> <p>You may get something left of field or things you're not sure of. In these cases, it's up to you to address them before they become larger issues.</p> <p>Don't forget, sometimes the best answer is \"I don't know, but I'll figure it out and get back to you,\" or \"that's not what we're focused on for now...\" (then bringing it back to what you are focused on).</p>"},{"location":"communicating-your-work/#what-story-are-you-telling","title":"What story are you telling?","text":"<p>You're going to underestimate and overestimate your work at the same time. This is a good thing. No one is going to care as much about your work as you. It's up to you to be your own biggest fan and harshest critique at the same time.</p> The first step of any creation is to make something you're proud of. The next step is to figure out how you could improve it. In other words, being your own biggest fan and harshest critique at the same time. <p>When sharing your work, you could drop the facts in. Nothing but a list of exactly what you did. But everyone else can do that too.</p> <p>Working what you've done into a story, sharing what worked, what didn't, why you went one direction and not another is hard. But that's exactly why it's worth it.</p> <p>I will say it until I go hoarse, how you deliver your message will depend on who your audience is.</p>"},{"location":"communicating-your-work/#being-specific-is-brave-put-it-in-writing-and-heres-what-ive-done","title":"Being specific is brave, put it in writing and here's what I've done","text":"<p>Starting with \"Who's it for?\", and following up with, \"What do they need to know?\", means you're going to have to be specific. And being specific means having to say, \u201cIt's not for you\" to a lot of people. Doing this takes courage but it also means the ones who do receive your message will engage with it more.</p> <p>You'll get lost in thought but found in the words. Writing is nature's way of showing how sloppy your thinking is. Break your larger projects down into a series of sub projects. </p> <p>What's on today? What's on this week? Tell yourself, tell your team.</p> <p>Take advantage of Cunningham's Law: Sometimes the best way to figure out the right answer isn't to ask a question, it's to put the wrong answer out there.</p> <p>Finally, remind yourself, you're not going for perfection. You're going for progress. Going for perfection gets in the way of progress.</p> <p>You know you should have your own blog, you know you should be building specific knowledge by working on your own your projects, you know you should be documenting what you've been working on.</p> <p>The upside of being able to say, \"Here's what I've done\", far outweighs the downside of potentially being wrong.</p>"},{"location":"communicating-your-work/#recommended-further-reading-and-resources","title":"Recommended Further Reading and Resources","text":"<p>This article was inspired by experience and a handful of other resources worth your time.</p> <ul> <li>Basecamp\u2019s guide to internal communication \u2013 if you're working on a team, this should be required reading for everyone.</li> <li>You Should Blog by Jeremy Howard from fast.ai \u2013 The fast.ai team not only teach amazing artificial intelligence and other technical skills, they teach you how to communicate them. The best thing is, they live and breath what they teach.</li> <li>How to Start Your Own Machine Learning Projects by Daniel Bourke \u2013 After learning foundational skills using courses, one of the hardest things to do next is to use the skills you've learned in your own projects. This article by yours truly gives a deeper breakdown into how to approach your own projects.</li> <li>Why you (yes, you) should blog by Rachel Thomas from fast.ai \u2013 Rachel Thomas not only has incredible technical skills, she's a phenomenal communicator. If you aren't convinced to start your own blog yet, this article will have you writing in no time.</li> <li>Fast Template by fast.ai \u2013 Starting a blog should be required for everyone learning some kind of skill. Fast Template by the fast.ai team makes it free and easy.</li> <li>Devblog by Hasnode \u2013 Your own blog, your own domain, readers ready to go, you own your content (automatic backups on GitHub), all ready to go. Start writing.</li> </ul>"},{"location":"end-to-end-dog-vision-v2/","title":"Introduction to TensorFlow","text":"In\u00a0[\u00a0]: Copied! <pre># Quick timestamp\nimport datetime\nprint(f\"Last updated: {datetime.datetime.now()}\")\n</pre> # Quick timestamp import datetime print(f\"Last updated: {datetime.datetime.now()}\") <pre>Last updated: 2023-11-09 01:52:47.025946\n</pre> In\u00a0[\u00a0]: Copied! <pre># TK - TODO: Check compatibility with Keras 3.0 by installing tf-nightly, see: https://x.com/fchollet/status/1719448117064659352?s=20\nimport tensorflow as tf\ntf.__version__\n</pre> # TK - TODO: Check compatibility with Keras 3.0 by installing tf-nightly, see: https://x.com/fchollet/status/1719448117064659352?s=20 import tensorflow as tf tf.__version__ Out[\u00a0]: <pre>'2.14.0'</pre> <p>Nice!</p> <p>Note: If you want to run TensorFlow locally, you can follow the TensorFlow installation guide.</p> <p>Now let's check to see if TensorFlow has access to a GPU (this isn't 100% required to complete this project but will speed things up dramatically).</p> <p>We can do so with the method <code>tf.config.list_physical_devices()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Do we have access to a GPU?\ndevice_list = tf.config.list_physical_devices()\nif \"GPU\" in [device.device_type for device in device_list]:\n  print(f\"[INFO] TensorFlow has GPU available to use. Woohoo!! Computing will be sped up!\")\n  print(f\"[INFO] Accessible devices:\\n{device_list}\")\nelse:\n  print(f\"[INFO] TensorFlow does not have GPU available to use. Models may take a while to train.\")\n  print(f\"[INFO] Accessible devices:\\n{device_list}\")\n</pre> # Do we have access to a GPU? device_list = tf.config.list_physical_devices() if \"GPU\" in [device.device_type for device in device_list]:   print(f\"[INFO] TensorFlow has GPU available to use. Woohoo!! Computing will be sped up!\")   print(f\"[INFO] Accessible devices:\\n{device_list}\") else:   print(f\"[INFO] TensorFlow does not have GPU available to use. Models may take a while to train.\")   print(f\"[INFO] Accessible devices:\\n{device_list}\") <pre>[INFO] TensorFlow has GPU available to use. Woohoo!! Computing will be sped up!\n[INFO] Accessible devices:\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n</pre> In\u00a0[\u00a0]: Copied! <pre># Download the dataset into train and test split using TensorFlow Datasets\n# import tensorflow_datasets as tfds\n# ds_train, ds_test = tfds.load('stanford_dogs', split=['train', 'test'])\n</pre> # Download the dataset into train and test split using TensorFlow Datasets # import tensorflow_datasets as tfds # ds_train, ds_test = tfds.load('stanford_dogs', split=['train', 'test']) In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\nfrom google.colab import drive\n\n# 1. Mount Google Drive (this will bring up a pop-up to sign-in/authenticate)\n# Note: This step is specifically for Google Colab, if you're working locally, you may need a different setup\ndrive.mount(\"/content/drive\")\n\n# 2. Setup constants\n# Note: For constants like this, you'll often see them created as variables with all capitals\nTARGET_DRIVE_PATH = Path(\"drive/MyDrive/tensorflow/dog_vision_data\")\nTARGET_FILES = [\"images.tar\", \"annotation.tar\", \"lists.tar\"]\nTARGET_URL = \"http://vision.stanford.edu/aditya86/ImageNetDogs\"\n\n# 3. Setup local path\nlocal_dir = Path(\"dog_vision_data\")\n\n# 4. Check if the target files exist in Google Drive, if so, copy them to Google Colab\nif all((TARGET_DRIVE_PATH / file).is_file() for file in TARGET_FILES):\n  print(f\"[INFO] Copying Dog Vision files from Google Drive to local directory...\")\n  print(f\"[INFO] Source dir: {TARGET_DRIVE_PATH} -&gt; Target dir: {local_dir}\")\n  !cp -r {TARGET_DRIVE_PATH} .\n  print(\"[INFO] Good to go!\")\n\nelse:\n  # 5. If the files don't exist in Google Drive, download them\n  print(f\"[INFO] Target files not found in Google Drive.\")\n  print(f\"[INFO] Downloading the target files... this shouldn't take too long...\")\n  for file in TARGET_FILES:\n    # wget is short for \"world wide web get\", as in \"get a file from the web\"\n    # -nc or --no-clobber = don't download files that already exist locally\n    # -P = save the target file to a specified prefix, in our case, local_dir\n    !wget -nc {TARGET_URL}/{file} -P {local_dir} # the \"!\" means to execute the command on the command line rather than in Python\n\n  print(f\"[INFO] Saving the target files to Google Drive, so they can be loaded later...\")\n\n  # 6. Ensure target directory in Google Drive exists\n  TARGET_DRIVE_PATH.mkdir(parents=True, exist_ok=True)\n\n  # 7. Copy downloaded files to Google Drive (so we can use them later and not have to re-download them)\n  !cp -r {local_dir}/* {TARGET_DRIVE_PATH}/\n</pre> from pathlib import Path from google.colab import drive  # 1. Mount Google Drive (this will bring up a pop-up to sign-in/authenticate) # Note: This step is specifically for Google Colab, if you're working locally, you may need a different setup drive.mount(\"/content/drive\")  # 2. Setup constants # Note: For constants like this, you'll often see them created as variables with all capitals TARGET_DRIVE_PATH = Path(\"drive/MyDrive/tensorflow/dog_vision_data\") TARGET_FILES = [\"images.tar\", \"annotation.tar\", \"lists.tar\"] TARGET_URL = \"http://vision.stanford.edu/aditya86/ImageNetDogs\"  # 3. Setup local path local_dir = Path(\"dog_vision_data\")  # 4. Check if the target files exist in Google Drive, if so, copy them to Google Colab if all((TARGET_DRIVE_PATH / file).is_file() for file in TARGET_FILES):   print(f\"[INFO] Copying Dog Vision files from Google Drive to local directory...\")   print(f\"[INFO] Source dir: {TARGET_DRIVE_PATH} -&gt; Target dir: {local_dir}\")   !cp -r {TARGET_DRIVE_PATH} .   print(\"[INFO] Good to go!\")  else:   # 5. If the files don't exist in Google Drive, download them   print(f\"[INFO] Target files not found in Google Drive.\")   print(f\"[INFO] Downloading the target files... this shouldn't take too long...\")   for file in TARGET_FILES:     # wget is short for \"world wide web get\", as in \"get a file from the web\"     # -nc or --no-clobber = don't download files that already exist locally     # -P = save the target file to a specified prefix, in our case, local_dir     !wget -nc {TARGET_URL}/{file} -P {local_dir} # the \"!\" means to execute the command on the command line rather than in Python    print(f\"[INFO] Saving the target files to Google Drive, so they can be loaded later...\")    # 6. Ensure target directory in Google Drive exists   TARGET_DRIVE_PATH.mkdir(parents=True, exist_ok=True)    # 7. Copy downloaded files to Google Drive (so we can use them later and not have to re-download them)   !cp -r {local_dir}/* {TARGET_DRIVE_PATH}/ <pre>Mounted at /content/drive\n[INFO] Copying Dog Vision files from Google Drive to local directory...\n[INFO] Source dir: drive/MyDrive/tensorflow/dog_vision_data -&gt; Target dir: dog_vision_data\n[INFO] Good to go!\n</pre> <p>Data downloaded!</p> <p>Nice work!</p> <p>Now if we get the contents of <code>local_dir</code> (<code>dog_vision_data</code>), what do we get?</p> <p>We can first make sure it exists with <code>Path.exists()</code> and then we can iterate through its contents with <code>Path.iterdir()</code> and print out the <code>.name</code> attribute of each file.</p> In\u00a0[\u00a0]: Copied! <pre>if local_dir.exists():\n  print(str(local_dir) + \"/\")\n  for item in local_dir.iterdir():\n    print(\"  \", item.name)\n</pre> if local_dir.exists():   print(str(local_dir) + \"/\")   for item in local_dir.iterdir():     print(\"  \", item.name) <pre>dog_vision_data/\n   annotation.tar\n   images.tar\n   lists.tar\n</pre> <p>Excellent! That's exactly the format we wanted.</p> <p>Now you might've noticed that each file ends in <code>.tar</code>.</p> <p>What's this?</p> <p>Searching \"what is .tar?\", I found:</p> <p>In computing, tar is a computer software utility for collecting many files into one archive file, often referred to as a tarball, for distribution or backup purposes.</p> <p>Source: Wikipedia tar page).</p> <p>Exploring a bit more, I found that the <code>.tar</code> format is similar to <code>.zip</code>, however, <code>.zip</code> offers compression, where as <code>.tar</code> mostly combines many files into one.</p> <p>So how do we \"untar\" the files in <code>images.tar</code>, <code>annotation.tar</code> and <code>lists.tar</code>?</p> <p>We can use the <code>!tar</code> command (or just <code>tar</code> from outside of a Jupyter Cell)!</p> <p>Doing this will expand all of the files within each of the <code>.tar</code> archives.</p> <p>We'll also use a couple of flags to help us out:</p> <ul> <li>The <code>-x</code> flag tells <code>tar</code> to extract files from an archive.</li> <li>The <code>-f</code> flag specifies that the following argument is the name of the archive file.</li> <li>You can combine flags by putting them together <code>-xf</code>.</li> </ul> <p>Let's try it out!</p> In\u00a0[\u00a0]: Copied! <pre># Untar images\n# -x = extract files from the zipped file\n# -v = verbose\n# -z = decompress files\n# -f = tell tar which file to deal with\n!tar -xf dog_vision_data/images.tar\n!tar -xf dog_vision_data/annotation.tar\n!tar -xf dog_vision_data/lists.tar\n</pre> # Untar images # -x = extract files from the zipped file # -v = verbose # -z = decompress files # -f = tell tar which file to deal with !tar -xf dog_vision_data/images.tar !tar -xf dog_vision_data/annotation.tar !tar -xf dog_vision_data/lists.tar <p>What new files did we get?</p> <p>We can check in Google Colab by inspecting the \"Files\" tab on the left.</p> <p>Or with Python by using <code>os.listdir(\".\")</code> where <code>\".\"</code> means \"the current directory\".</p> In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.listdir(\".\") # \".\" stands for \"here\" or \"current directory\"\n</pre> import os  os.listdir(\".\") # \".\" stands for \"here\" or \"current directory\" Out[\u00a0]: <pre>['.config',\n 'test_list.mat',\n 'dog_vision_data',\n 'Annotation',\n 'train_list.mat',\n 'drive',\n 'Images',\n 'file_list.mat',\n 'sample_data']</pre> <p>Ooooh!</p> <p>Looks like we've got some new files!</p> <p>Specifically:</p> <ul> <li><code>train_list.mat</code> - a list of all the training set images.</li> <li><code>test_list.mat</code> - a list of all the testing set images.</li> <li><code>Images/</code> - a folder containing all of the images of dogs.</li> <li><code>Annotation/</code> - a folder containing all of the annotations for each image.</li> <li><code>file_list.mat</code> - a list of all the files (training and test list combined).</li> </ul> <p>Our next step is to go through them and see what we've got.</p> In\u00a0[\u00a0]: Copied! <pre>import scipy\n\n# Open lists of train and test .mat\ntrain_list = scipy.io.loadmat(\"train_list.mat\")\ntest_list = scipy.io.loadmat(\"test_list.mat\")\nfile_list = scipy.io.loadmat(\"file_list.mat\")\n\n# Let's inspect the output and type of the train_list\ntrain_list, type(train_list)\n</pre> import scipy  # Open lists of train and test .mat train_list = scipy.io.loadmat(\"train_list.mat\") test_list = scipy.io.loadmat(\"test_list.mat\") file_list = scipy.io.loadmat(\"file_list.mat\")  # Let's inspect the output and type of the train_list train_list, type(train_list) Out[\u00a0]: <pre>({'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct  9 08:36:13 2011',\n  '__version__': '1.0',\n  '__globals__': [],\n  'file_list': array([[array(['n02085620-Chihuahua/n02085620_5927.jpg'], dtype='&lt;U38')],\n         [array(['n02085620-Chihuahua/n02085620_4441.jpg'], dtype='&lt;U38')],\n         [array(['n02085620-Chihuahua/n02085620_1502.jpg'], dtype='&lt;U38')],\n         ...,\n         [array(['n02116738-African_hunting_dog/n02116738_6754.jpg'], dtype='&lt;U48')],\n         [array(['n02116738-African_hunting_dog/n02116738_9333.jpg'], dtype='&lt;U48')],\n         [array(['n02116738-African_hunting_dog/n02116738_2503.jpg'], dtype='&lt;U48')]],\n        dtype=object),\n  'annotation_list': array([[array(['n02085620-Chihuahua/n02085620_5927'], dtype='&lt;U34')],\n         [array(['n02085620-Chihuahua/n02085620_4441'], dtype='&lt;U34')],\n         [array(['n02085620-Chihuahua/n02085620_1502'], dtype='&lt;U34')],\n         ...,\n         [array(['n02116738-African_hunting_dog/n02116738_6754'], dtype='&lt;U44')],\n         [array(['n02116738-African_hunting_dog/n02116738_9333'], dtype='&lt;U44')],\n         [array(['n02116738-African_hunting_dog/n02116738_2503'], dtype='&lt;U44')]],\n        dtype=object),\n  'labels': array([[  1],\n         [  1],\n         [  1],\n         ...,\n         [120],\n         [120],\n         [120]], dtype=uint8)},\n dict)</pre> <p>Okay, looks like we get a dictionary with several fields we may be interested in.</p> <p>Let's check out the keys of the dictionary.</p> In\u00a0[\u00a0]: Copied! <pre>train_list.keys()\n</pre> train_list.keys() Out[\u00a0]: <pre>dict_keys(['__header__', '__version__', '__globals__', 'file_list', 'annotation_list', 'labels'])</pre> <p>My guess is that the <code>file_list</code> key is what we're after, as this looks like a large array of image names (the files all end in <code>.jpg</code>).</p> <p>How about we see how many files are in each <code>file_list</code> key?</p> In\u00a0[\u00a0]: Copied! <pre># Check the length of the file_list key\nprint(f\"Number of files in training list: {len(train_list['file_list'])}\")\nprint(f\"Number of files in testing list: {len(test_list['file_list'])}\")\nprint(f\"Number of files in full list: {len(file_list['file_list'])}\")\n</pre> # Check the length of the file_list key print(f\"Number of files in training list: {len(train_list['file_list'])}\") print(f\"Number of files in testing list: {len(test_list['file_list'])}\") print(f\"Number of files in full list: {len(file_list['file_list'])}\") <pre>Number of files in training list: 12000\nNumber of files in testing list: 8580\nNumber of files in full list: 20580\n</pre> <p>Beautiful! Looks like these lists contain our training and test splits and the full list has a list of all the files in the dataset.</p> <p>Let's inspect the <code>train_list['file_list']</code> further.</p> In\u00a0[\u00a0]: Copied! <pre>train_list['file_list']\n</pre> train_list['file_list'] Out[\u00a0]: <pre>array([[array(['n02085620-Chihuahua/n02085620_5927.jpg'], dtype='&lt;U38')],\n       [array(['n02085620-Chihuahua/n02085620_4441.jpg'], dtype='&lt;U38')],\n       [array(['n02085620-Chihuahua/n02085620_1502.jpg'], dtype='&lt;U38')],\n       ...,\n       [array(['n02116738-African_hunting_dog/n02116738_6754.jpg'], dtype='&lt;U48')],\n       [array(['n02116738-African_hunting_dog/n02116738_9333.jpg'], dtype='&lt;U48')],\n       [array(['n02116738-African_hunting_dog/n02116738_2503.jpg'], dtype='&lt;U48')]],\n      dtype=object)</pre> <p>Looks like we've got an array of arrays.</p> <p>How about we turn them into a Python list for easier handling?</p> <p>We can do so by extracting each individual item via indexing and list comprehension.</p> <p>Let's see what it's like to get a single file name.</p> In\u00a0[\u00a0]: Copied! <pre># Get a single filename\ntrain_list['file_list'][0][0][0]\n</pre> # Get a single filename train_list['file_list'][0][0][0] Out[\u00a0]: <pre>'n02085620-Chihuahua/n02085620_5927.jpg'</pre> <p>Now let's get a Python list of all the individual file names (e.g. <code>n02097130-giant_schnauzer/n02097130_2866.jpg</code>) so we can use them later.</p> In\u00a0[\u00a0]: Copied! <pre># Get a Python list of all file names for each list\ntrain_file_list = list([item[0][0] for item in train_list[\"file_list\"]])\ntest_file_list = list([item[0][0] for item in test_list[\"file_list\"]])\nfull_file_list = list([item[0][0] for item in file_list[\"file_list\"]])\n\nlen(train_file_list), len(test_file_list), len(full_file_list)\n</pre> # Get a Python list of all file names for each list train_file_list = list([item[0][0] for item in train_list[\"file_list\"]]) test_file_list = list([item[0][0] for item in test_list[\"file_list\"]]) full_file_list = list([item[0][0] for item in file_list[\"file_list\"]])  len(train_file_list), len(test_file_list), len(full_file_list) Out[\u00a0]: <pre>(12000, 8580, 20580)</pre> <p>Wonderful!</p> <p>How about we view a random sample of the filenames we extracted?</p> <p>Note: One of my favourite things to do whilst exploring data is to continually view random samples of it. Whether it be file names or images or text snippets. Why? You can always view the first X number of samples, however, I find that continually viewing random samples of the data gives you a better of overview of the different kinds of data you're working with. It also gives you the small chance of stumbling upon a potential error.</p> <p>We can view random samples of the data using Python's <code>random.sample()</code> method.</p> In\u00a0[\u00a0]: Copied! <pre>import random\n\nrandom.sample(train_file_list, k=10)\n</pre> import random  random.sample(train_file_list, k=10) Out[\u00a0]: <pre>['n02113712-miniature_poodle/n02113712_578.jpg',\n 'n02109047-Great_Dane/n02109047_6008.jpg',\n 'n02106030-collie/n02106030_10246.jpg',\n 'n02098413-Lhasa/n02098413_2308.jpg',\n 'n02085620-Chihuahua/n02085620_9414.jpg',\n 'n02109047-Great_Dane/n02109047_2475.jpg',\n 'n02102318-cocker_spaniel/n02102318_8482.jpg',\n 'n02091244-Ibizan_hound/n02091244_2050.jpg',\n 'n02093754-Border_terrier/n02093754_1845.jpg',\n 'n02106382-Bouvier_des_Flandres/n02106382_501.jpg']</pre> <p>Now let's do a quick check to make sure none of the training image file names appear in the testing image file names list.</p> <p>This is important because the number 1 rule in machine learning is: always keep the test set separate from the training set.</p> <p>We can check that there are no overlaps by turning <code>train_file_list</code> into a Python <code>set()</code> and using the <code>intersection()</code> method.</p> In\u00a0[\u00a0]: Copied! <pre># How many files in the training set intersect with the testing set?\nlen(set(train_file_list).intersection(test_file_list))\n</pre> # How many files in the training set intersect with the testing set? len(set(train_file_list).intersection(test_file_list)) Out[\u00a0]: <pre>0</pre> <p>Excellent! Looks like there are no overlaps.</p> <p>We could even put an <code>assert</code> check to raise an error if there are any overlaps (e.g. the length of the intersection is greater than 0).</p> <p><code>assert</code> works in the fashion: <code>assert expression, message_if_expression_fails</code>.</p> <p>If the <code>assert</code> check doesn't output anything, we're good to go!</p> In\u00a0[\u00a0]: Copied! <pre># Make an assertion statement to check there are no overlaps (try changing test_file_list to train_file_list to see how it works)\nassert len(set(train_file_list).intersection(test_file_list)) == 0, \"There are overlaps between the training and test set files, please check them.\"\n</pre> # Make an assertion statement to check there are no overlaps (try changing test_file_list to train_file_list to see how it works) assert len(set(train_file_list).intersection(test_file_list)) == 0, \"There are overlaps between the training and test set files, please check them.\" <p>Woohoo!</p> <p>Looks like there's no overlaps, let's keep exploring the data.</p> In\u00a0[\u00a0]: Copied! <pre>os.listdir(\"Annotation\")[:10]\n</pre> os.listdir(\"Annotation\")[:10] Out[\u00a0]: <pre>['n02086240-Shih-Tzu',\n 'n02113978-Mexican_hairless',\n 'n02086646-Blenheim_spaniel',\n 'n02108089-boxer',\n 'n02107908-Appenzeller',\n 'n02105251-briard',\n 'n02097474-Tibetan_terrier',\n 'n02097209-standard_schnauzer',\n 'n02089973-English_foxhound',\n 'n02109525-Saint_Bernard']</pre> <p>Looks like there are files each with a dog breed name with several numbered files inside.</p> <p>Each of the files contains a HTML version of an annotation relating to an image.</p> <p>For example, <code>Annotation/n02085620-Chihuahua/n02085620_10074</code>:</p> <pre><code>&lt;annotation&gt;\n\t&lt;folder&gt;02085620&lt;/folder&gt;\n\t&lt;filename&gt;n02085620_10074&lt;/filename&gt;\n\t&lt;source&gt;\n\t\t&lt;database&gt;ImageNet database&lt;/database&gt;\n\t&lt;/source&gt;\n\t&lt;size&gt;\n\t\t&lt;width&gt;333&lt;/width&gt;\n\t\t&lt;height&gt;500&lt;/height&gt;\n\t\t&lt;depth&gt;3&lt;/depth&gt;\n\t&lt;/size&gt;\n\t&lt;segment&gt;0&lt;/segment&gt;\n\t&lt;object&gt;\n\t\t&lt;name&gt;Chihuahua&lt;/name&gt;\n\t\t&lt;pose&gt;Unspecified&lt;/pose&gt;\n\t\t&lt;truncated&gt;0&lt;/truncated&gt;\n\t\t&lt;difficult&gt;0&lt;/difficult&gt;\n\t\t&lt;bndbox&gt;\n\t\t\t&lt;xmin&gt;25&lt;/xmin&gt;\n\t\t\t&lt;ymin&gt;10&lt;/ymin&gt;\n\t\t\t&lt;xmax&gt;276&lt;/xmax&gt;\n\t\t\t&lt;ymax&gt;498&lt;/ymax&gt;\n\t\t&lt;/bndbox&gt;\n\t&lt;/object&gt;\n&lt;/annotation&gt;\n</code></pre> <p>The fields include the name of the image, the size of the image, the label of the object and where it is (bounding box coordinates).</p> <p>If we were performing object detection (finding the location of a thing in an image), we'd pay attention to the <code>&lt;bndbox&gt;</code> coordinates.</p> <p>However, since we're focused on classification, our main consideration is the mapping of image name to class name.</p> <p>Since we're dealing with 120 classes of dog breed, let's write a function to check the number of subfolders in the <code>Annotation</code> directory (there should be 120 subfolders, one for each breed of dog).</p> <p>To do so, we can use Python's <code>pathlib.Path</code> class, along with <code>Path.iterdir()</code> to loop over the contents of <code>Annotation</code> and <code>Path.is_dir()</code> to check if the target item is a directory.</p> In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n\ndef count_subfolders(directory_path: str) -&gt; int:\n    \"\"\"\n    Count the number of subfolders in a given directory.\n\n    Args:\n    directory_path (str): The path to the directory in which to count subfolders.\n\n    Returns:\n    int: The number of subfolders in the specified directory.\n\n    Examples:\n    &gt;&gt;&gt; count_subfolders('/path/to/directory')\n    3  # if there are 3 subfolders in the specified directory\n    \"\"\"\n    return len([name for name in Path(directory_path).iterdir() if name.is_dir()])\n\n\ndirectory_path = \"Annotation\"\nfolder_count = count_subfolders(directory_path)\nprint(f\"Number of subfolders in {directory_path} directory: {folder_count}\")\n</pre> from pathlib import Path  def count_subfolders(directory_path: str) -&gt; int:     \"\"\"     Count the number of subfolders in a given directory.      Args:     directory_path (str): The path to the directory in which to count subfolders.      Returns:     int: The number of subfolders in the specified directory.      Examples:     &gt;&gt;&gt; count_subfolders('/path/to/directory')     3  # if there are 3 subfolders in the specified directory     \"\"\"     return len([name for name in Path(directory_path).iterdir() if name.is_dir()])   directory_path = \"Annotation\" folder_count = count_subfolders(directory_path) print(f\"Number of subfolders in {directory_path} directory: {folder_count}\") <pre>Number of subfolders in Annotation directory: 120\n</pre> <p>Perfect!</p> <p>There are 120 subfolders of annotations, one for each class of dog we'd like to identify.</p> <p>But on further inspection of our file lists, it looks like the class name is already in the filepath.</p> In\u00a0[\u00a0]: Copied! <pre># View a single training file pathname\ntrain_file_list[0]\n</pre> # View a single training file pathname train_file_list[0] Out[\u00a0]: <pre>'n02085620-Chihuahua/n02085620_5927.jpg'</pre> <p>With this information we know, that image <code>n02085620_5927.jpg</code> should contain a <code>Chihuahua</code>.</p> <p>Let's check.</p> <p>I searched \"how to display an image in Google Colab\" and found another answer on Stack Overflow.</p> <p>Turns out you can use <code>IPython.display.Image()</code>, as Google Colab comes with IPython (Interactive Python) built-in.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Image\nImage(Path(\"Images\", train_file_list[0]))\n</pre> from IPython.display import Image Image(Path(\"Images\", train_file_list[0])) Out[\u00a0]: <p>Woah!</p> <p>We get an image of a dog!</p> In\u00a0[\u00a0]: Copied! <pre># Get a list of all image folders\nimage_folders = os.listdir(\"Images\")\nimage_folders[:10]\n</pre> # Get a list of all image folders image_folders = os.listdir(\"Images\") image_folders[:10] Out[\u00a0]: <pre>['n02086240-Shih-Tzu',\n 'n02113978-Mexican_hairless',\n 'n02086646-Blenheim_spaniel',\n 'n02108089-boxer',\n 'n02107908-Appenzeller',\n 'n02105251-briard',\n 'n02097474-Tibetan_terrier',\n 'n02097209-standard_schnauzer',\n 'n02089973-English_foxhound',\n 'n02109525-Saint_Bernard']</pre> <p>Excellent!</p> <p>Now let's make a dictionary which maps from the folder name to a simplified version of the class name, for example:</p> <pre><code>{'n02085782-Japanese_spaniel': 'japanese_spaniel',\n'n02106662-German_shepherd': 'german_shepherd',\n'n02093256-Staffordshire_bullterrier': 'staffordshire_bullterrier',\n...}\n</code></pre> In\u00a0[\u00a0]: Copied! <pre># Create folder name -&gt; class name dict\nfolder_to_class_name_dict = {}\nfor folder_name in image_folders:\n  # Turn folder name into class_name\n  # E.g. \"n02089078-black-and-tan_coonhound\" -&gt; \"black_and_tan_coonhound\"\n  # We'll split on the first \"-\" and join the rest of the string with \"_\" and then lower it\n  class_name = \"_\".join(folder_name.split(\"-\")[1:]).lower()\n  folder_to_class_name_dict[folder_name] = class_name\n\n# Make sure there are 120 entries in the dictionary\nassert len(folder_to_class_name_dict) == 120\n</pre> # Create folder name -&gt; class name dict folder_to_class_name_dict = {} for folder_name in image_folders:   # Turn folder name into class_name   # E.g. \"n02089078-black-and-tan_coonhound\" -&gt; \"black_and_tan_coonhound\"   # We'll split on the first \"-\" and join the rest of the string with \"_\" and then lower it   class_name = \"_\".join(folder_name.split(\"-\")[1:]).lower()   folder_to_class_name_dict[folder_name] = class_name  # Make sure there are 120 entries in the dictionary assert len(folder_to_class_name_dict) == 120 <p>Folder name to class name mapping created, let's view the first 10.</p> In\u00a0[\u00a0]: Copied! <pre>sorted(folder_to_class_name_dict.items())[:10]\n</pre> sorted(folder_to_class_name_dict.items())[:10] Out[\u00a0]: <pre>[('n02085620-Chihuahua', 'chihuahua'),\n ('n02085782-Japanese_spaniel', 'japanese_spaniel'),\n ('n02085936-Maltese_dog', 'maltese_dog'),\n ('n02086079-Pekinese', 'pekinese'),\n ('n02086240-Shih-Tzu', 'shih_tzu'),\n ('n02086646-Blenheim_spaniel', 'blenheim_spaniel'),\n ('n02086910-papillon', 'papillon'),\n ('n02087046-toy_terrier', 'toy_terrier'),\n ('n02087394-Rhodesian_ridgeback', 'rhodesian_ridgeback'),\n ('n02088094-Afghan_hound', 'afghan_hound')]</pre> <p>And we can get a list of unique dog names by getting the <code>values()</code> of the <code>folder_to_class_name_dict</code> and turning it into a list.</p> In\u00a0[\u00a0]: Copied! <pre>dog_names = sorted(list(folder_to_class_name_dict.values()))\ndog_names[:10]\n</pre> dog_names = sorted(list(folder_to_class_name_dict.values())) dog_names[:10] Out[\u00a0]: <pre>['affenpinscher',\n 'afghan_hound',\n 'african_hunting_dog',\n 'airedale',\n 'american_staffordshire_terrier',\n 'appenzeller',\n 'australian_terrier',\n 'basenji',\n 'basset',\n 'beagle']</pre> <p>Perfect!</p> <p>Now we've got:</p> <ol> <li><code>folder_to_class_name_dict</code> - a mapping from the folder name to the class name.</li> <li><code>dog_names</code> - a list of all the unique dog breeds we're working with.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>from typing import List\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport random\n\n# 1. Take in a select list of image paths\ndef plot_10_random_images_from_path_list(path_list: List[Path],\n                                         extract_title=True) -&gt; None:\n  # 2. Set up a grid of plots\n  fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20, 10))\n\n  # 3. Randomly sample 10 paths from the list\n  samples = random.sample(path_list, 10)\n\n  # 4. Iterate through the flattened axes and corresponding sample paths\n  for i, ax in enumerate(axes.flat):\n\n    # 5. Get the target sample path (e.g. \"Images/n02087394-Rhodesian_ridgeback/n02087394_1161.jpg\")\n    sample_path = samples[i]\n\n    # 6. Extract the parent directory name to use as the title (if necessary)\n    # (e.g. n02087394-Rhodesian_ridgeback/n02087394_1161.jpg -&gt; n02087394-Rhodesian_ridgeback -&gt; rhodesian_ridgeback)\n    if extract_title:\n      sample_title = folder_to_class_name_dict[sample_path.parent.stem]\n    else:\n      sample_title = sample_path.parent.stem\n\n    # 7. Read the image file and plot it on the corresponding axis\n    ax.imshow(plt.imread(sample_path))\n\n    # 8. Set the title of the axis and turn of the axis (for pretty plots)\n    ax.set_title(sample_title)\n    ax.axis(\"off\")\n\n  # 9. Display the plot\n  plt.show()\n\nplot_10_random_images_from_path_list(path_list=[Path(\"Images\") / Path(file) for file in train_file_list])\n</pre> from typing import List from pathlib import Path import matplotlib.pyplot as plt import random  # 1. Take in a select list of image paths def plot_10_random_images_from_path_list(path_list: List[Path],                                          extract_title=True) -&gt; None:   # 2. Set up a grid of plots   fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20, 10))    # 3. Randomly sample 10 paths from the list   samples = random.sample(path_list, 10)    # 4. Iterate through the flattened axes and corresponding sample paths   for i, ax in enumerate(axes.flat):      # 5. Get the target sample path (e.g. \"Images/n02087394-Rhodesian_ridgeback/n02087394_1161.jpg\")     sample_path = samples[i]      # 6. Extract the parent directory name to use as the title (if necessary)     # (e.g. n02087394-Rhodesian_ridgeback/n02087394_1161.jpg -&gt; n02087394-Rhodesian_ridgeback -&gt; rhodesian_ridgeback)     if extract_title:       sample_title = folder_to_class_name_dict[sample_path.parent.stem]     else:       sample_title = sample_path.parent.stem      # 7. Read the image file and plot it on the corresponding axis     ax.imshow(plt.imread(sample_path))      # 8. Set the title of the axis and turn of the axis (for pretty plots)     ax.set_title(sample_title)     ax.axis(\"off\")    # 9. Display the plot   plt.show()  plot_10_random_images_from_path_list(path_list=[Path(\"Images\") / Path(file) for file in train_file_list]) <p>Those are some nice looking dogs!</p> <p>What I like to do here is rerun the random visualizations until I've seen 100+ samples so I've got an idea of the data we're working with.</p> <p>Question: Here's something to think about, how would you code a system to differentiate between all the different breeds of dogs? Perhaps you write an algorithm to look at the shapes or the colours? You might be thinking \"that would take quite a long time...\" And you'd be right. Then how would we do it? Machine learning of course!</p> In\u00a0[\u00a0]: Copied! <pre># Create a dictionary of image counts\nfrom pathlib import Path\nfrom typing import List, Dict\n\n# 1. Take in a target directory\ndef count_images_in_subdirs(target_directory: str) -&gt; List[Dict[str, int]]:\n    \"\"\"\n    Counts the number of JPEG images in each subdirectory of the given directory.\n\n    Each subdirectory is assumed to represent a class, and the function counts\n    the number of '.jpg' files within each one. The result is a list of\n    dictionaries with the class name and corresponding image count.\n\n    Args:\n        target_directory (str): The path to the directory containing subdirectories.\n\n    Returns:\n        List[Dict[str, int]]: A list of dictionaries with 'class_name' and 'image_count' for each subdirectory.\n\n    Examples:\n        &gt;&gt;&gt; count_images_in_subdirs('/path/to/directory')\n        [{'class_name': 'beagle', 'image_count': 50}, {'class_name': 'poodle', 'image_count': 60}]\n    \"\"\"\n    # 2. Create a list of all the subdirectoires in the target directory (these contain our images)\n    images_dir = Path(target_directory)\n    image_class_dirs = [directory for directory in images_dir.iterdir() if directory.is_dir()]\n\n    # 3. Create an empty list to append image counts to\n    image_class_counts = []\n\n    # 4. Iterate through all of the subdirectories\n    for image_class_dir in image_class_dirs:\n\n        # 5. Get the class name from image directory (e.g. \"Images/n02116738-African_hunting_dog\" -&gt; \"n02116738-African_hunting_dog\")\n        class_name = image_class_dir.stem\n\n        # 6. Count the number of images in the target subdirectory\n        image_count = len(list(image_class_dir.rglob(\"*.jpg\")))  # get length all files with .jpg file extension\n\n        # 7. Append a dictionary of class name and image count to count list\n        image_class_counts.append({\"class_name\": class_name,\n                                   \"image_count\": image_count})\n\n    # 8. Return the list\n    return image_class_counts\n</pre> # Create a dictionary of image counts from pathlib import Path from typing import List, Dict  # 1. Take in a target directory def count_images_in_subdirs(target_directory: str) -&gt; List[Dict[str, int]]:     \"\"\"     Counts the number of JPEG images in each subdirectory of the given directory.      Each subdirectory is assumed to represent a class, and the function counts     the number of '.jpg' files within each one. The result is a list of     dictionaries with the class name and corresponding image count.      Args:         target_directory (str): The path to the directory containing subdirectories.      Returns:         List[Dict[str, int]]: A list of dictionaries with 'class_name' and 'image_count' for each subdirectory.      Examples:         &gt;&gt;&gt; count_images_in_subdirs('/path/to/directory')         [{'class_name': 'beagle', 'image_count': 50}, {'class_name': 'poodle', 'image_count': 60}]     \"\"\"     # 2. Create a list of all the subdirectoires in the target directory (these contain our images)     images_dir = Path(target_directory)     image_class_dirs = [directory for directory in images_dir.iterdir() if directory.is_dir()]      # 3. Create an empty list to append image counts to     image_class_counts = []      # 4. Iterate through all of the subdirectories     for image_class_dir in image_class_dirs:          # 5. Get the class name from image directory (e.g. \"Images/n02116738-African_hunting_dog\" -&gt; \"n02116738-African_hunting_dog\")         class_name = image_class_dir.stem          # 6. Count the number of images in the target subdirectory         image_count = len(list(image_class_dir.rglob(\"*.jpg\")))  # get length all files with .jpg file extension          # 7. Append a dictionary of class name and image count to count list         image_class_counts.append({\"class_name\": class_name,                                    \"image_count\": image_count})      # 8. Return the list     return image_class_counts <p>Ho ho, what a function!</p> <p>Let's run it on our target directory <code>Images</code> and view the first few indexes.</p> In\u00a0[\u00a0]: Copied! <pre>image_class_counts = count_images_in_subdirs(\"Images\")\nimage_class_counts[:3]\n</pre> image_class_counts = count_images_in_subdirs(\"Images\") image_class_counts[:3] Out[\u00a0]: <pre>[{'class_name': 'n02086240-Shih-Tzu', 'image_count': 214},\n {'class_name': 'n02113978-Mexican_hairless', 'image_count': 155},\n {'class_name': 'n02086646-Blenheim_spaniel', 'image_count': 188}]</pre> <p>Nice!</p> <p>Since our <code>image_class_counts</code> variable is the form of a list of dictionaries, we can turn it into a pandas <code>DataFrame</code>.</p> <p>Let's sort the <code>DataFrame</code> by <code>\"image_count\"</code> so the classes with the most images appear at the top, we can do so with <code>DataFrame.sort_values()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Create a DataFrame\nimport pandas as pd\nimage_counts_df = pd.DataFrame(image_class_counts).sort_values(by=\"image_count\", ascending=False)\nimage_counts_df.head()\n</pre> # Create a DataFrame import pandas as pd image_counts_df = pd.DataFrame(image_class_counts).sort_values(by=\"image_count\", ascending=False) image_counts_df.head() Out[\u00a0]: class_name image_count 71 n02085936-Maltese_dog 252 32 n02088094-Afghan_hound 239 91 n02092002-Scottish_deerhound 232 117 n02112018-Pomeranian 219 35 n02107683-Bernese_mountain_dog 218 <p>And let's cleanup the <code>\"class_name\"</code> column to be more readable by mapping the the values to our <code>folder_to_class_name_dict</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Make class name column easier to read\nimage_counts_df[\"class_name\"] = image_counts_df[\"class_name\"].map(folder_to_class_name_dict)\nimage_counts_df.head()\n</pre> # Make class name column easier to read image_counts_df[\"class_name\"] = image_counts_df[\"class_name\"].map(folder_to_class_name_dict) image_counts_df.head() Out[\u00a0]: class_name image_count 71 maltese_dog 252 32 afghan_hound 239 91 scottish_deerhound 232 117 pomeranian 219 35 bernese_mountain_dog 218 <p>Now we've got a <code>DataFrame</code> of image counts per class, we can make them more visual by turning them into a plot.</p> <p>We covered plotting data directly from pandas <code>DataFrame</code>'s in Section 3 of the Introduction to Matplotlib notebook: Plotting data directly with pandas.</p> <p>To do so, we can use <code>image_counts_df.plot(kind=\"bar\", ...)</code> along with some other customization.</p> In\u00a0[\u00a0]: Copied! <pre># Turn the image counts DataFrame into a graph\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(14, 7))\nimage_counts_df.plot(kind=\"bar\",\n                     x=\"class_name\",\n                     y=\"image_count\",\n                     legend=False,\n                     ax=plt.gca()) # plt.gca() = \"get current axis\", get the plt we setup above and put the data there\n\n# Add customization\nplt.ylabel(\"Image Count\")\nplt.title(\"Total Image Counts by Class\")\nplt.xticks(rotation=90, # Rotate the x labels for better visibility\n           fontsize=8) # Make the font size smaller for easier reading\nplt.tight_layout() # Ensure things fit nicely\nplt.show()\n</pre> # Turn the image counts DataFrame into a graph import matplotlib.pyplot as plt plt.figure(figsize=(14, 7)) image_counts_df.plot(kind=\"bar\",                      x=\"class_name\",                      y=\"image_count\",                      legend=False,                      ax=plt.gca()) # plt.gca() = \"get current axis\", get the plt we setup above and put the data there  # Add customization plt.ylabel(\"Image Count\") plt.title(\"Total Image Counts by Class\") plt.xticks(rotation=90, # Rotate the x labels for better visibility            fontsize=8) # Make the font size smaller for easier reading plt.tight_layout() # Ensure things fit nicely plt.show() <p>Beautiful! It looks like our classes are quite balanced. Each breed of dog has ~150 or more images.</p> <p>We can find out some other quick stats about our data with <code>DataFrame.describe()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Get various statistics about our data distribution\nimage_counts_df.describe()\n</pre> # Get various statistics about our data distribution image_counts_df.describe() Out[\u00a0]: image_count count 120.000000 mean 171.500000 std 23.220898 min 148.000000 25% 152.750000 50% 159.500000 75% 186.250000 max 252.000000 <p>And the table shows a similar story to the plot. We can see the minimum number of images per class is 148, where as the maximum number of images is 252.</p> <p>If one class had 10x less images than another class, we may look into collecting more data to improve the balance.</p> <p>The main takeaway(s):</p> <ul> <li>When working on a classification problem, ideally, all classes have a similar number of samples (however, in some problems this may be unattainable, such as fraud detection, where you may have 1000x more \"not fraud\" samples to \"fraud\" samples.</li> <li>If you wanted to add a new class of dog breed to the existing 120, ideally, you'd have at least ~150 images for it.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n\n# Define the target directory for image splits to go\nimages_split_dir = Path(\"images_split\")\n\n# Define the training and test directories\ntrain_dir = images_split_dir / \"train\"\ntest_dir = images_split_dir / \"test\"\n\n# Using Path.mkdir with exist_ok=True ensures the directory is created only if it doesn't exist\ntrain_dir.mkdir(parents=True, exist_ok=True)\ntest_dir.mkdir(parents=True, exist_ok=True)\nprint(f\"Directory {train_dir} is ensured to exist.\")\nprint(f\"Directory {test_dir} is ensured to exist.\")\n\n# Make a folder for each dog name\nfor dog_name in dog_names:\n  # Make training dir folder\n  train_class_dir = train_dir / dog_name\n  train_class_dir.mkdir(parents=True, exist_ok=True)\n  # print(f\"Making directory: {train_class_dir}\")\n\n  # Make testing dir folder\n  test_class_dir = test_dir / dog_name\n  test_class_dir.mkdir(parents=True, exist_ok=True)\n  # print(f\"Making directory: {test_class_dir}\")\n\n# Make sure there is 120 subfolders in each\nassert count_subfolders(train_dir) == len(dog_names)\nassert count_subfolders(test_dir) == len(dog_names)\n</pre> from pathlib import Path  # Define the target directory for image splits to go images_split_dir = Path(\"images_split\")  # Define the training and test directories train_dir = images_split_dir / \"train\" test_dir = images_split_dir / \"test\"  # Using Path.mkdir with exist_ok=True ensures the directory is created only if it doesn't exist train_dir.mkdir(parents=True, exist_ok=True) test_dir.mkdir(parents=True, exist_ok=True) print(f\"Directory {train_dir} is ensured to exist.\") print(f\"Directory {test_dir} is ensured to exist.\")  # Make a folder for each dog name for dog_name in dog_names:   # Make training dir folder   train_class_dir = train_dir / dog_name   train_class_dir.mkdir(parents=True, exist_ok=True)   # print(f\"Making directory: {train_class_dir}\")    # Make testing dir folder   test_class_dir = test_dir / dog_name   test_class_dir.mkdir(parents=True, exist_ok=True)   # print(f\"Making directory: {test_class_dir}\")  # Make sure there is 120 subfolders in each assert count_subfolders(train_dir) == len(dog_names) assert count_subfolders(test_dir) == len(dog_names) <pre>Directory images_split/train is ensured to exist.\nDirectory images_split/test is ensured to exist.\n</pre> <p>Excellent!</p> <p>We can check out the data split directories/folders we created by inspecting them in the files panel in Google Colab.</p> <p>Alternatively, we can check the names of each by list the subdirectories inside them.</p> In\u00a0[\u00a0]: Copied! <pre># See the first 10 directories in the training split dir\nsorted([str(dir_name) for dir_name in train_dir.iterdir() if dir_name.is_dir()])[:10]\n</pre> # See the first 10 directories in the training split dir sorted([str(dir_name) for dir_name in train_dir.iterdir() if dir_name.is_dir()])[:10] Out[\u00a0]: <pre>['images_split/train/affenpinscher',\n 'images_split/train/afghan_hound',\n 'images_split/train/african_hunting_dog',\n 'images_split/train/airedale',\n 'images_split/train/american_staffordshire_terrier',\n 'images_split/train/appenzeller',\n 'images_split/train/australian_terrier',\n 'images_split/train/basenji',\n 'images_split/train/basset',\n 'images_split/train/beagle']</pre> <p>You might've noticed that all of our dog breed directories are empty.</p> <p>Let's change that by getting some images in there.</p> <p>To do so, we'll create a function called <code>copy_files_to_target_dir()</code> which will copy images from the <code>Images</code> directory into their respective directories inside <code>images/train</code> and <code>images/test</code>.</p> <p>More specifically, it will:</p> <ol> <li>Take in a list of source files to copy (e.g. <code>train_file_list</code>) and a target directory to copy files to.</li> <li>Iterate through the list of sources files to copy (we'll use <code>tqdm</code> which comes installed with Google Colab to create a progress bar of how many files have been copied).</li> <li>Convert the source file path to a <code>Path</code> object.</li> <li>Split the source file path and create a <code>Path</code> object for the destination folder (e.g. \"n02112018-Pomeranian\" -&gt; \"pomeranian\").</li> <li>Get the target file name (e.g. \"n02112018-Pomeranian/n02112018_6208.jpg\" -&gt; \"n02112018_6208.jpg\").</li> <li>Create a destination path for the source file to be copied to (e.g. <code>images_split/train/pomeranian/n02112018_6208.jpg</code>).</li> <li>Ensure the destination directory exists, similar to the step we took in the previous section (you can't copy files to a directory that doesn't exist).</li> <li>Print out the progress of copying (if necessary).</li> <li>Copy the source file to the destination using Python's <code>shutil.copy2(src, dst)</code>.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\nfrom shutil import copy2\nfrom tqdm.auto import tqdm\n\n# 1. Take in a list of source files to copy and a target directory\ndef copy_files_to_target_dir(file_list: list[str],\n                             target_dir: str,\n                             images_dir: str = \"Images\",\n                             verbose: bool = False) -&gt; None:\n    \"\"\"\n    Copies a list of files from the images directory to a target directory.\n\n    Parameters:\n    file_list (list[str]): A list of file paths to copy.\n    target_dir (str): The destination directory path where files will be copied.\n    images_dir (str, optional): The directory path where the images are currently stored. Defaults to 'Images'.\n    verbose (bool, optional): If set to True, the function will print out the file paths as they are being copied. Defaults to False.\n\n    Returns:\n    None\n    \"\"\"\n    # 2. Iterate through source files\n    for file in tqdm(file_list):\n\n      # 3. Convert file path to a Path object\n      source_file_path = Path(images_dir) / Path(file)\n\n      # 4. Split the file path and create a Path object for the destination folder\n      # e.g. \"n02112018-Pomeranian\" -&gt; \"pomeranian\"\n      file_class_name = folder_to_class_name_dict[Path(file).parts[0]]\n\n      # 5. Get the name of the target image\n      file_image_name = Path(file).name\n\n      # 6. Create the destination path\n      destination_file_path = Path(target_dir) / file_class_name / file_image_name\n\n      # 7. Ensure the destination directory exists (this is a safety check, can't copy an image to a file that doesn't exist)\n      destination_file_path.parent.mkdir(parents=True, exist_ok=True)\n\n      # 8. Print out copy message if necessary\n      if verbose:\n        print(f\"[INFO] Copying: {source_file_path} to {destination_file_path}\")\n\n      # 9. Copy the original path to the destination path\n      copy2(src=source_file_path, dst=destination_file_path)\n</pre> from pathlib import Path from shutil import copy2 from tqdm.auto import tqdm  # 1. Take in a list of source files to copy and a target directory def copy_files_to_target_dir(file_list: list[str],                              target_dir: str,                              images_dir: str = \"Images\",                              verbose: bool = False) -&gt; None:     \"\"\"     Copies a list of files from the images directory to a target directory.      Parameters:     file_list (list[str]): A list of file paths to copy.     target_dir (str): The destination directory path where files will be copied.     images_dir (str, optional): The directory path where the images are currently stored. Defaults to 'Images'.     verbose (bool, optional): If set to True, the function will print out the file paths as they are being copied. Defaults to False.      Returns:     None     \"\"\"     # 2. Iterate through source files     for file in tqdm(file_list):        # 3. Convert file path to a Path object       source_file_path = Path(images_dir) / Path(file)        # 4. Split the file path and create a Path object for the destination folder       # e.g. \"n02112018-Pomeranian\" -&gt; \"pomeranian\"       file_class_name = folder_to_class_name_dict[Path(file).parts[0]]        # 5. Get the name of the target image       file_image_name = Path(file).name        # 6. Create the destination path       destination_file_path = Path(target_dir) / file_class_name / file_image_name        # 7. Ensure the destination directory exists (this is a safety check, can't copy an image to a file that doesn't exist)       destination_file_path.parent.mkdir(parents=True, exist_ok=True)        # 8. Print out copy message if necessary       if verbose:         print(f\"[INFO] Copying: {source_file_path} to {destination_file_path}\")        # 9. Copy the original path to the destination path       copy2(src=source_file_path, dst=destination_file_path) <p>Copying function created!</p> <p>Let's test it out by copying the files in the <code>train_file_list</code> to <code>train_dir</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Copy training images from Images to images_split/train/...\ncopy_files_to_target_dir(file_list=train_file_list,\n                         target_dir=train_dir,\n                         verbose=False) # set this to True to get an output of the copy process\n                                        # (warning: this will output a large amount of text)\n</pre> # Copy training images from Images to images_split/train/... copy_files_to_target_dir(file_list=train_file_list,                          target_dir=train_dir,                          verbose=False) # set this to True to get an output of the copy process                                         # (warning: this will output a large amount of text) <pre>  0%|          | 0/12000 [00:00&lt;?, ?it/s]</pre> <p>Woohoo!</p> <p>Looks like our copying function copied 12000 training images in their respective directories inside <code>images_split/train/</code>.</p> <p>How about we do the same for <code>test_file_list</code> and <code>test_dir</code>?</p> In\u00a0[\u00a0]: Copied! <pre>copy_files_to_target_dir(file_list=test_file_list,\n                         target_dir=test_dir,\n                         verbose=False)\n</pre> copy_files_to_target_dir(file_list=test_file_list,                          target_dir=test_dir,                          verbose=False) <pre>  0%|          | 0/8580 [00:00&lt;?, ?it/s]</pre> <p>Nice! 8580 testing images copied from <code>Images</code> to <code>images_split/test/</code>.</p> <p>Let's write some code to check that the number of files in the <code>train_file_list</code> is the same as the number of images files in <code>train_dir</code> (and the same for the test files).</p> In\u00a0[\u00a0]: Copied! <pre># Get list of of all .jpg paths in train and test image directories\ntrain_image_paths = list(train_dir.rglob(\"*.jpg\"))\ntest_image_paths = list(test_dir.rglob(\"*.jpg\"))\n\n# Make sure the number of images in the training and test directories equals the number of files in their original lists\nassert len(train_image_paths) == len(train_file_list)\nassert len(test_image_paths) == len(test_file_list)\n\nprint(f\"Number of images in {train_dir}: {len(train_image_paths)}\")\nprint(f\"Number of images in {test_dir}: {len(test_image_paths)}\")\n</pre> # Get list of of all .jpg paths in train and test image directories train_image_paths = list(train_dir.rglob(\"*.jpg\")) test_image_paths = list(test_dir.rglob(\"*.jpg\"))  # Make sure the number of images in the training and test directories equals the number of files in their original lists assert len(train_image_paths) == len(train_file_list) assert len(test_image_paths) == len(test_file_list)  print(f\"Number of images in {train_dir}: {len(train_image_paths)}\") print(f\"Number of images in {test_dir}: {len(test_image_paths)}\") <pre>Number of images in images_split/train: 12000\nNumber of images in images_split/test: 8580\n</pre> <p>And adhering to the data explorers motto of visualize, visualize, visualize!, let's plot some random images from the <code>train_image_paths</code> list.</p> In\u00a0[\u00a0]: Copied! <pre># Plot 10 random images from the train_image_paths\nplot_10_random_images_from_path_list(path_list=train_image_paths,\n                                     extract_title=False) # don't need to extract the title since the image directories are already named simply\n</pre> # Plot 10 random images from the train_image_paths plot_10_random_images_from_path_list(path_list=train_image_paths,                                      extract_title=False) # don't need to extract the title since the image directories are already named simply In\u00a0[\u00a0]: Copied! <pre># TK - get the train file paths\n# Get a random 10% of them\n# Copy them to a target directory, e.g. train_dir_10_percent\n\n# Create train_10_percent directory\ntrain_10_percent_dir = images_split_dir / \"train_10_percent\"\ntrain_10_percent_dir.mkdir(parents=True, exist_ok=True)\n\nimport random\nrandom.seed(42)\n\ntrain_image_paths_random_10_percent = random.sample(population=train_image_paths,\n                                                    k=int(0.1*len(train_image_paths)))\nlen(train_image_paths_random_10_percent)\n\nfor path in tqdm(train_image_paths_random_10_percent):\n  source_file_path = path\n  destination_file_path = train_10_percent_dir / Path(*path.parts[-2:])\n\n  print(destination_file_path.parent)\n  # If the target directory doesn't exist, make it\n  target_class_dir = destination_file_path.parent\n  if not target_class_dir.is_dir():\n    print(f\"Making directory: {target_class_dir}\")\n    target_class_dir.mkdir(parents=True,\n                           exist_ok=True)\n\n  print(f\"Copying: {source_file_path} to {destination_file_path}\")\n  copy2(src=source_file_path,\n        dst=destination_file_path)\n</pre> # TK - get the train file paths # Get a random 10% of them # Copy them to a target directory, e.g. train_dir_10_percent  # Create train_10_percent directory train_10_percent_dir = images_split_dir / \"train_10_percent\" train_10_percent_dir.mkdir(parents=True, exist_ok=True)  import random random.seed(42)  train_image_paths_random_10_percent = random.sample(population=train_image_paths,                                                     k=int(0.1*len(train_image_paths))) len(train_image_paths_random_10_percent)  for path in tqdm(train_image_paths_random_10_percent):   source_file_path = path   destination_file_path = train_10_percent_dir / Path(*path.parts[-2:])    print(destination_file_path.parent)   # If the target directory doesn't exist, make it   target_class_dir = destination_file_path.parent   if not target_class_dir.is_dir():     print(f\"Making directory: {target_class_dir}\")     target_class_dir.mkdir(parents=True,                            exist_ok=True)    print(f\"Copying: {source_file_path} to {destination_file_path}\")   copy2(src=source_file_path,         dst=destination_file_path) <pre>  0%|          | 0/1200 [00:00&lt;?, ?it/s]</pre> <pre>images_split/train_10_percent/clumber\nMaking directory: images_split/train_10_percent/clumber\nCopying: images_split/train/clumber/n02101556_7528.jpg to images_split/train_10_percent/clumber/n02101556_7528.jpg\nimages_split/train_10_percent/komondor\nMaking directory: images_split/train_10_percent/komondor\nCopying: images_split/train/komondor/n02105505_1406.jpg to images_split/train_10_percent/komondor/n02105505_1406.jpg\nimages_split/train_10_percent/newfoundland\nMaking directory: images_split/train_10_percent/newfoundland\nCopying: images_split/train/newfoundland/n02111277_10160.jpg to images_split/train_10_percent/newfoundland/n02111277_10160.jpg\nimages_split/train_10_percent/afghan_hound\nMaking directory: images_split/train_10_percent/afghan_hound\nCopying: images_split/train/afghan_hound/n02088094_4049.jpg to images_split/train_10_percent/afghan_hound/n02088094_4049.jpg\nimages_split/train_10_percent/briard\nMaking directory: images_split/train_10_percent/briard\nCopying: images_split/train/briard/n02105251_7805.jpg to images_split/train_10_percent/briard/n02105251_7805.jpg\nimages_split/train_10_percent/shetland_sheepdog\nMaking directory: images_split/train_10_percent/shetland_sheepdog\nCopying: images_split/train/shetland_sheepdog/n02105855_5719.jpg to images_split/train_10_percent/shetland_sheepdog/n02105855_5719.jpg\nimages_split/train_10_percent/malinois\nMaking directory: images_split/train_10_percent/malinois\nCopying: images_split/train/malinois/n02105162_9995.jpg to images_split/train_10_percent/malinois/n02105162_9995.jpg\nimages_split/train_10_percent/scottish_deerhound\nMaking directory: images_split/train_10_percent/scottish_deerhound\nCopying: images_split/train/scottish_deerhound/n02092002_15064.jpg to images_split/train_10_percent/scottish_deerhound/n02092002_15064.jpg\nimages_split/train_10_percent/appenzeller\nMaking directory: images_split/train_10_percent/appenzeller\nCopying: images_split/train/appenzeller/n02107908_2092.jpg to images_split/train_10_percent/appenzeller/n02107908_2092.jpg\nimages_split/train_10_percent/miniature_schnauzer\nMaking directory: images_split/train_10_percent/miniature_schnauzer\nCopying: images_split/train/miniature_schnauzer/n02097047_6813.jpg to images_split/train_10_percent/miniature_schnauzer/n02097047_6813.jpg\nimages_split/train_10_percent/ibizan_hound\nMaking directory: images_split/train_10_percent/ibizan_hound\nCopying: images_split/train/ibizan_hound/n02091244_5818.jpg to images_split/train_10_percent/ibizan_hound/n02091244_5818.jpg\nimages_split/train_10_percent/australian_terrier\nMaking directory: images_split/train_10_percent/australian_terrier\nCopying: images_split/train/australian_terrier/n02096294_1231.jpg to images_split/train_10_percent/australian_terrier/n02096294_1231.jpg\nimages_split/train_10_percent/bernese_mountain_dog\nMaking directory: images_split/train_10_percent/bernese_mountain_dog\nCopying: images_split/train/bernese_mountain_dog/n02107683_1421.jpg to images_split/train_10_percent/bernese_mountain_dog/n02107683_1421.jpg\nimages_split/train_10_percent/maltese_dog\nMaking directory: images_split/train_10_percent/maltese_dog\nCopying: images_split/train/maltese_dog/n02085936_16565.jpg to images_split/train_10_percent/maltese_dog/n02085936_16565.jpg\nimages_split/train_10_percent/newfoundland\nCopying: images_split/train/newfoundland/n02111277_3896.jpg to images_split/train_10_percent/newfoundland/n02111277_3896.jpg\nimages_split/train_10_percent/blenheim_spaniel\nMaking directory: images_split/train_10_percent/blenheim_spaniel\nCopying: images_split/train/blenheim_spaniel/n02086646_3059.jpg to images_split/train_10_percent/blenheim_spaniel/n02086646_3059.jpg\nimages_split/train_10_percent/weimaraner\nMaking directory: images_split/train_10_percent/weimaraner\nCopying: images_split/train/weimaraner/n02092339_1796.jpg to images_split/train_10_percent/weimaraner/n02092339_1796.jpg\nimages_split/train_10_percent/dingo\nMaking directory: images_split/train_10_percent/dingo\nCopying: images_split/train/dingo/n02115641_5815.jpg to images_split/train_10_percent/dingo/n02115641_5815.jpg\nimages_split/train_10_percent/border_collie\nMaking directory: images_split/train_10_percent/border_collie\nCopying: images_split/train/border_collie/n02106166_1204.jpg to images_split/train_10_percent/border_collie/n02106166_1204.jpg\nimages_split/train_10_percent/flat_coated_retriever\nMaking directory: images_split/train_10_percent/flat_coated_retriever\nCopying: images_split/train/flat_coated_retriever/n02099267_2127.jpg to images_split/train_10_percent/flat_coated_retriever/n02099267_2127.jpg\nimages_split/train_10_percent/newfoundland\nCopying: images_split/train/newfoundland/n02111277_939.jpg to images_split/train_10_percent/newfoundland/n02111277_939.jpg\nimages_split/train_10_percent/african_hunting_dog\nMaking directory: images_split/train_10_percent/african_hunting_dog\nCopying: images_split/train/african_hunting_dog/n02116738_4758.jpg to images_split/train_10_percent/african_hunting_dog/n02116738_4758.jpg\nimages_split/train_10_percent/english_springer\nMaking directory: images_split/train_10_percent/english_springer\nCopying: images_split/train/english_springer/n02102040_3096.jpg to images_split/train_10_percent/english_springer/n02102040_3096.jpg\nimages_split/train_10_percent/silky_terrier\nMaking directory: images_split/train_10_percent/silky_terrier\nCopying: images_split/train/silky_terrier/n02097658_4402.jpg to images_split/train_10_percent/silky_terrier/n02097658_4402.jpg\nimages_split/train_10_percent/mexican_hairless\nMaking directory: images_split/train_10_percent/mexican_hairless\nCopying: images_split/train/mexican_hairless/n02113978_356.jpg to images_split/train_10_percent/mexican_hairless/n02113978_356.jpg\nimages_split/train_10_percent/english_foxhound\nMaking directory: images_split/train_10_percent/english_foxhound\nCopying: images_split/train/english_foxhound/n02089973_1078.jpg to images_split/train_10_percent/english_foxhound/n02089973_1078.jpg\nimages_split/train_10_percent/miniature_schnauzer\nCopying: images_split/train/miniature_schnauzer/n02097047_1825.jpg to images_split/train_10_percent/miniature_schnauzer/n02097047_1825.jpg\nimages_split/train_10_percent/bull_mastiff\nMaking directory: images_split/train_10_percent/bull_mastiff\nCopying: images_split/train/bull_mastiff/n02108422_207.jpg to images_split/train_10_percent/bull_mastiff/n02108422_207.jpg\nimages_split/train_10_percent/shetland_sheepdog\nCopying: images_split/train/shetland_sheepdog/n02105855_7495.jpg to images_split/train_10_percent/shetland_sheepdog/n02105855_7495.jpg\nimages_split/train_10_percent/rottweiler\nMaking directory: images_split/train_10_percent/rottweiler\nCopying: images_split/train/rottweiler/n02106550_1742.jpg to images_split/train_10_percent/rottweiler/n02106550_1742.jpg\nimages_split/train_10_percent/australian_terrier\nCopying: images_split/train/australian_terrier/n02096294_7295.jpg to images_split/train_10_percent/australian_terrier/n02096294_7295.jpg\nimages_split/train_10_percent/afghan_hound\nCopying: images_split/train/afghan_hound/n02088094_8682.jpg to images_split/train_10_percent/afghan_hound/n02088094_8682.jpg\nimages_split/train_10_percent/saluki\nMaking directory: images_split/train_10_percent/saluki\nCopying: images_split/train/saluki/n02091831_3760.jpg to images_split/train_10_percent/saluki/n02091831_3760.jpg\nimages_split/train_10_percent/papillon\nMaking directory: images_split/train_10_percent/papillon\nCopying: images_split/train/papillon/n02086910_4883.jpg to images_split/train_10_percent/papillon/n02086910_4883.jpg\nimages_split/train_10_percent/english_foxhound\nCopying: images_split/train/english_foxhound/n02089973_3119.jpg to images_split/train_10_percent/english_foxhound/n02089973_3119.jpg\nimages_split/train_10_percent/bernese_mountain_dog\nCopying: images_split/train/bernese_mountain_dog/n02107683_4350.jpg to images_split/train_10_percent/bernese_mountain_dog/n02107683_4350.jpg\nimages_split/train_10_percent/brittany_spaniel\nMaking directory: images_split/train_10_percent/brittany_spaniel\nCopying: images_split/train/brittany_spaniel/n02101388_10527.jpg to images_split/train_10_percent/brittany_spaniel/n02101388_10527.jpg\nimages_split/train_10_percent/afghan_hound\nCopying: images_split/train/afghan_hound/n02088094_9523.jpg to images_split/train_10_percent/afghan_hound/n02088094_9523.jpg\nimages_split/train_10_percent/borzoi\nMaking directory: images_split/train_10_percent/borzoi\nCopying: images_split/train/borzoi/n02090622_6976.jpg to images_split/train_10_percent/borzoi/n02090622_6976.jpg\nimages_split/train_10_percent/weimaraner\nCopying: images_split/train/weimaraner/n02092339_205.jpg to images_split/train_10_percent/weimaraner/n02092339_205.jpg\nimages_split/train_10_percent/brittany_spaniel\nCopying: images_split/train/brittany_spaniel/n02101388_2529.jpg to images_split/train_10_percent/brittany_spaniel/n02101388_2529.jpg\nimages_split/train_10_percent/scottish_deerhound\nCopying: images_split/train/scottish_deerhound/n02092002_14858.jpg to images_split/train_10_percent/scottish_deerhound/n02092002_14858.jpg\nimages_split/train_10_percent/blenheim_spaniel\nCopying: images_split/train/blenheim_spaniel/n02086646_669.jpg to images_split/train_10_percent/blenheim_spaniel/n02086646_669.jpg\nimages_split/train_10_percent/whippet\nMaking directory: images_split/train_10_percent/whippet\nCopying: images_split/train/whippet/n02091134_18392.jpg to images_split/train_10_percent/whippet/n02091134_18392.jpg\nimages_split/train_10_percent/blenheim_spaniel\nCopying: images_split/train/blenheim_spaniel/n02086646_3315.jpg to images_split/train_10_percent/blenheim_spaniel/n02086646_3315.jpg\nimages_split/train_10_percent/walker_hound\nMaking directory: images_split/train_10_percent/walker_hound\nCopying: images_split/train/walker_hound/n02089867_3456.jpg to images_split/train_10_percent/walker_hound/n02089867_3456.jpg\nimages_split/train_10_percent/german_short_haired_pointer\nMaking directory: images_split/train_10_percent/german_short_haired_pointer\nCopying: images_split/train/german_short_haired_pointer/n02100236_156.jpg to images_split/train_10_percent/german_short_haired_pointer/n02100236_156.jpg\nimages_split/train_10_percent/flat_coated_retriever\nCopying: images_split/train/flat_coated_retriever/n02099267_198.jpg to images_split/train_10_percent/flat_coated_retriever/n02099267_198.jpg\nimages_split/train_10_percent/cocker_spaniel\nMaking directory: images_split/train_10_percent/cocker_spaniel\nCopying: images_split/train/cocker_spaniel/n02102318_9822.jpg to images_split/train_10_percent/cocker_spaniel/n02102318_9822.jpg\nimages_split/train_10_percent/bloodhound\nMaking directory: images_split/train_10_percent/bloodhound\nCopying: images_split/train/bloodhound/n02088466_9287.jpg to images_split/train_10_percent/bloodhound/n02088466_9287.jpg\nimages_split/train_10_percent/bouvier_des_flandres\nMaking directory: images_split/train_10_percent/bouvier_des_flandres\nCopying: images_split/train/bouvier_des_flandres/n02106382_2087.jpg to images_split/train_10_percent/bouvier_des_flandres/n02106382_2087.jpg\nimages_split/train_10_percent/pomeranian\nMaking directory: images_split/train_10_percent/pomeranian\nCopying: images_split/train/pomeranian/n02112018_12953.jpg to images_split/train_10_percent/pomeranian/n02112018_12953.jpg\nimages_split/train_10_percent/irish_setter\nMaking directory: images_split/train_10_percent/irish_setter\nCopying: images_split/train/irish_setter/n02100877_3006.jpg to images_split/train_10_percent/irish_setter/n02100877_3006.jpg\nimages_split/train_10_percent/miniature_pinscher\nMaking directory: images_split/train_10_percent/miniature_pinscher\nCopying: images_split/train/miniature_pinscher/n02107312_5299.jpg to images_split/train_10_percent/miniature_pinscher/n02107312_5299.jpg\nimages_split/train_10_percent/whippet\nCopying: images_split/train/whippet/n02091134_9398.jpg to images_split/train_10_percent/whippet/n02091134_9398.jpg\nimages_split/train_10_percent/golden_retriever\nMaking directory: images_split/train_10_percent/golden_retriever\nCopying: images_split/train/golden_retriever/n02099601_142.jpg to images_split/train_10_percent/golden_retriever/n02099601_142.jpg\nimages_split/train_10_percent/boxer\nMaking directory: images_split/train_10_percent/boxer\nCopying: images_split/train/boxer/n02108089_926.jpg to images_split/train_10_percent/boxer/n02108089_926.jpg\nimages_split/train_10_percent/standard_schnauzer\nMaking directory: images_split/train_10_percent/standard_schnauzer\nCopying: images_split/train/standard_schnauzer/n02097209_2423.jpg to images_split/train_10_percent/standard_schnauzer/n02097209_2423.jpg\nimages_split/train_10_percent/shih_tzu\nMaking directory: images_split/train_10_percent/shih_tzu\nCopying: images_split/train/shih_tzu/n02086240_6032.jpg to images_split/train_10_percent/shih_tzu/n02086240_6032.jpg\nimages_split/train_10_percent/airedale\nMaking directory: images_split/train_10_percent/airedale\nCopying: images_split/train/airedale/n02096051_3088.jpg to images_split/train_10_percent/airedale/n02096051_3088.jpg\nimages_split/train_10_percent/german_shepherd\nMaking directory: images_split/train_10_percent/german_shepherd\nCopying: images_split/train/german_shepherd/n02106662_22730.jpg to images_split/train_10_percent/german_shepherd/n02106662_22730.jpg\nimages_split/train_10_percent/welsh_springer_spaniel\nMaking directory: images_split/train_10_percent/welsh_springer_spaniel\nCopying: images_split/train/welsh_springer_spaniel/n02102177_1798.jpg to images_split/train_10_percent/welsh_springer_spaniel/n02102177_1798.jpg\nimages_split/train_10_percent/norwich_terrier\nMaking directory: images_split/train_10_percent/norwich_terrier\nCopying: images_split/train/norwich_terrier/n02094258_188.jpg to images_split/train_10_percent/norwich_terrier/n02094258_188.jpg\nimages_split/train_10_percent/gordon_setter\nMaking directory: images_split/train_10_percent/gordon_setter\nCopying: images_split/train/gordon_setter/n02101006_1297.jpg to images_split/train_10_percent/gordon_setter/n02101006_1297.jpg\nimages_split/train_10_percent/greater_swiss_mountain_dog\nMaking directory: images_split/train_10_percent/greater_swiss_mountain_dog\nCopying: images_split/train/greater_swiss_mountain_dog/n02107574_2005.jpg to images_split/train_10_percent/greater_swiss_mountain_dog/n02107574_2005.jpg\nimages_split/train_10_percent/bloodhound\nCopying: images_split/train/bloodhound/n02088466_654.jpg to images_split/train_10_percent/bloodhound/n02088466_654.jpg\nimages_split/train_10_percent/west_highland_white_terrier\nMaking directory: images_split/train_10_percent/west_highland_white_terrier\nCopying: images_split/train/west_highland_white_terrier/n02098286_5057.jpg to images_split/train_10_percent/west_highland_white_terrier/n02098286_5057.jpg\nimages_split/train_10_percent/border_terrier\nMaking directory: images_split/train_10_percent/border_terrier\nCopying: images_split/train/border_terrier/n02093754_1102.jpg to images_split/train_10_percent/border_terrier/n02093754_1102.jpg\nimages_split/train_10_percent/eskimo_dog\nMaking directory: images_split/train_10_percent/eskimo_dog\nCopying: images_split/train/eskimo_dog/n02109961_8835.jpg to images_split/train_10_percent/eskimo_dog/n02109961_8835.jpg\nimages_split/train_10_percent/japanese_spaniel\nMaking directory: images_split/train_10_percent/japanese_spaniel\nCopying: images_split/train/japanese_spaniel/n02085782_23.jpg to images_split/train_10_percent/japanese_spaniel/n02085782_23.jpg\nimages_split/train_10_percent/dingo\nCopying: images_split/train/dingo/n02115641_726.jpg to images_split/train_10_percent/dingo/n02115641_726.jpg\nimages_split/train_10_percent/scottish_deerhound\nCopying: images_split/train/scottish_deerhound/n02092002_1592.jpg to images_split/train_10_percent/scottish_deerhound/n02092002_1592.jpg\nimages_split/train_10_percent/whippet\nCopying: images_split/train/whippet/n02091134_15846.jpg to images_split/train_10_percent/whippet/n02091134_15846.jpg\nimages_split/train_10_percent/afghan_hound\nCopying: images_split/train/afghan_hound/n02088094_3233.jpg to images_split/train_10_percent/afghan_hound/n02088094_3233.jpg\nimages_split/train_10_percent/italian_greyhound\nMaking directory: images_split/train_10_percent/italian_greyhound\nCopying: images_split/train/italian_greyhound/n02091032_5134.jpg to images_split/train_10_percent/italian_greyhound/n02091032_5134.jpg\nimages_split/train_10_percent/clumber\nCopying: images_split/train/clumber/n02101556_7574.jpg to images_split/train_10_percent/clumber/n02101556_7574.jpg\nimages_split/train_10_percent/german_shepherd\nCopying: images_split/train/german_shepherd/n02106662_22764.jpg to images_split/train_10_percent/german_shepherd/n02106662_22764.jpg\nimages_split/train_10_percent/papillon\nCopying: images_split/train/papillon/n02086910_4373.jpg to images_split/train_10_percent/papillon/n02086910_4373.jpg\nimages_split/train_10_percent/norwegian_elkhound\nMaking directory: images_split/train_10_percent/norwegian_elkhound\nCopying: images_split/train/norwegian_elkhound/n02091467_4761.jpg to images_split/train_10_percent/norwegian_elkhound/n02091467_4761.jpg\nimages_split/train_10_percent/walker_hound\nCopying: images_split/train/walker_hound/n02089867_90.jpg to images_split/train_10_percent/walker_hound/n02089867_90.jpg\nimages_split/train_10_percent/kuvasz\nMaking directory: images_split/train_10_percent/kuvasz\nCopying: images_split/train/kuvasz/n02104029_1214.jpg to images_split/train_10_percent/kuvasz/n02104029_1214.jpg\nimages_split/train_10_percent/lakeland_terrier\nMaking directory: images_split/train_10_percent/lakeland_terrier\nCopying: images_split/train/lakeland_terrier/n02095570_916.jpg to images_split/train_10_percent/lakeland_terrier/n02095570_916.jpg\nimages_split/train_10_percent/cocker_spaniel\nCopying: images_split/train/cocker_spaniel/n02102318_10178.jpg to images_split/train_10_percent/cocker_spaniel/n02102318_10178.jpg\nimages_split/train_10_percent/english_foxhound\nCopying: images_split/train/english_foxhound/n02089973_2068.jpg to images_split/train_10_percent/english_foxhound/n02089973_2068.jpg\nimages_split/train_10_percent/tibetan_mastiff\nMaking directory: images_split/train_10_percent/tibetan_mastiff\nCopying: images_split/train/tibetan_mastiff/n02108551_143.jpg to images_split/train_10_percent/tibetan_mastiff/n02108551_143.jpg\nimages_split/train_10_percent/mexican_hairless\nCopying: images_split/train/mexican_hairless/n02113978_3670.jpg to images_split/train_10_percent/mexican_hairless/n02113978_3670.jpg\nimages_split/train_10_percent/greater_swiss_mountain_dog\nCopying: images_split/train/greater_swiss_mountain_dog/n02107574_169.jpg to images_split/train_10_percent/greater_swiss_mountain_dog/n02107574_169.jpg\nimages_split/train_10_percent/toy_poodle\nMaking directory: images_split/train_10_percent/toy_poodle\nCopying: images_split/train/toy_poodle/n02113624_5723.jpg to images_split/train_10_percent/toy_poodle/n02113624_5723.jpg\nimages_split/train_10_percent/clumber\nCopying: images_split/train/clumber/n02101556_823.jpg to images_split/train_10_percent/clumber/n02101556_823.jpg\nimages_split/train_10_percent/american_staffordshire_terrier\nMaking directory: images_split/train_10_percent/american_staffordshire_terrier\nCopying: images_split/train/american_staffordshire_terrier/n02093428_5165.jpg to images_split/train_10_percent/american_staffordshire_terrier/n02093428_5165.jpg\nimages_split/train_10_percent/irish_setter\nCopying: images_split/train/irish_setter/n02100877_3195.jpg to images_split/train_10_percent/irish_setter/n02100877_3195.jpg\nimages_split/train_10_percent/bouvier_des_flandres\nCopying: images_split/train/bouvier_des_flandres/n02106382_2529.jpg to images_split/train_10_percent/bouvier_des_flandres/n02106382_2529.jpg\nimages_split/train_10_percent/briard\nCopying: images_split/train/briard/n02105251_6907.jpg to images_split/train_10_percent/briard/n02105251_6907.jpg\nimages_split/train_10_percent/papillon\nCopying: images_split/train/papillon/n02086910_4999.jpg to images_split/train_10_percent/papillon/n02086910_4999.jpg\nimages_split/train_10_percent/pomeranian\nCopying: images_split/train/pomeranian/n02112018_9028.jpg to images_split/train_10_percent/pomeranian/n02112018_9028.jpg\nimages_split/train_10_percent/whippet\nCopying: images_split/train/whippet/n02091134_10918.jpg to images_split/train_10_percent/whippet/n02091134_10918.jpg\nimages_split/train_10_percent/pekinese\nMaking directory: images_split/train_10_percent/pekinese\nCopying: images_split/train/pekinese/n02086079_12321.jpg to images_split/train_10_percent/pekinese/n02086079_12321.jpg\nimages_split/train_10_percent/clumber\nCopying: images_split/train/clumber/n02101556_8352.jpg to images_split/train_10_percent/clumber/n02101556_8352.jpg\nimages_split/train_10_percent/tibetan_terrier\nMaking directory: images_split/train_10_percent/tibetan_terrier\nCopying: images_split/train/tibetan_terrier/n02097474_3180.jpg to images_split/train_10_percent/tibetan_terrier/n02097474_3180.jpg\nimages_split/train_10_percent/african_hunting_dog\nCopying: images_split/train/african_hunting_dog/n02116738_1097.jpg to images_split/train_10_percent/african_hunting_dog/n02116738_1097.jpg\nimages_split/train_10_percent/weimaraner\nCopying: images_split/train/weimaraner/n02092339_6752.jpg to images_split/train_10_percent/weimaraner/n02092339_6752.jpg\nimages_split/train_10_percent/tibetan_terrier\nCopying: images_split/train/tibetan_terrier/n02097474_8540.jpg to images_split/train_10_percent/tibetan_terrier/n02097474_8540.jpg\nimages_split/train_10_percent/irish_water_spaniel\nMaking directory: images_split/train_10_percent/irish_water_spaniel\nCopying: images_split/train/irish_water_spaniel/n02102973_2805.jpg to images_split/train_10_percent/irish_water_spaniel/n02102973_2805.jpg\nimages_split/train_10_percent/norfolk_terrier\nMaking directory: images_split/train_10_percent/norfolk_terrier\nCopying: images_split/train/norfolk_terrier/n02094114_2739.jpg to images_split/train_10_percent/norfolk_terrier/n02094114_2739.jpg\nimages_split/train_10_percent/border_terrier\nCopying: images_split/train/border_terrier/n02093754_7109.jpg to images_split/train_10_percent/border_terrier/n02093754_7109.jpg\nimages_split/train_10_percent/maltese_dog\nCopying: images_split/train/maltese_dog/n02085936_1288.jpg to images_split/train_10_percent/maltese_dog/n02085936_1288.jpg\nimages_split/train_10_percent/siberian_husky\nMaking directory: images_split/train_10_percent/siberian_husky\nCopying: images_split/train/siberian_husky/n02110185_12656.jpg to images_split/train_10_percent/siberian_husky/n02110185_12656.jpg\nimages_split/train_10_percent/standard_poodle\nMaking directory: images_split/train_10_percent/standard_poodle\nCopying: images_split/train/standard_poodle/n02113799_2321.jpg to images_split/train_10_percent/standard_poodle/n02113799_2321.jpg\nimages_split/train_10_percent/cocker_spaniel\nCopying: images_split/train/cocker_spaniel/n02102318_8504.jpg to images_split/train_10_percent/cocker_spaniel/n02102318_8504.jpg\nimages_split/train_10_percent/miniature_poodle\nMaking directory: images_split/train_10_percent/miniature_poodle\nCopying: images_split/train/miniature_poodle/n02113712_3138.jpg to images_split/train_10_percent/miniature_poodle/n02113712_3138.jpg\nimages_split/train_10_percent/kuvasz\nCopying: images_split/train/kuvasz/n02104029_4746.jpg to images_split/train_10_percent/kuvasz/n02104029_4746.jpg\nimages_split/train_10_percent/chihuahua\nMaking directory: images_split/train_10_percent/chihuahua\nCopying: images_split/train/chihuahua/n02085620_9654.jpg to images_split/train_10_percent/chihuahua/n02085620_9654.jpg\nimages_split/train_10_percent/silky_terrier\nCopying: images_split/train/silky_terrier/n02097658_2069.jpg to images_split/train_10_percent/silky_terrier/n02097658_2069.jpg\nimages_split/train_10_percent/siberian_husky\nCopying: images_split/train/siberian_husky/n02110185_3651.jpg to images_split/train_10_percent/siberian_husky/n02110185_3651.jpg\nimages_split/train_10_percent/kuvasz\nCopying: images_split/train/kuvasz/n02104029_4672.jpg to images_split/train_10_percent/kuvasz/n02104029_4672.jpg\nimages_split/train_10_percent/giant_schnauzer\nMaking directory: images_split/train_10_percent/giant_schnauzer\nCopying: images_split/train/giant_schnauzer/n02097130_1235.jpg to images_split/train_10_percent/giant_schnauzer/n02097130_1235.jpg\nimages_split/train_10_percent/cairn\nMaking directory: images_split/train_10_percent/cairn\nCopying: images_split/train/cairn/n02096177_1000.jpg to images_split/train_10_percent/cairn/n02096177_1000.jpg\nimages_split/train_10_percent/old_english_sheepdog\nMaking directory: images_split/train_10_percent/old_english_sheepdog\nCopying: images_split/train/old_english_sheepdog/n02105641_8421.jpg to images_split/train_10_percent/old_english_sheepdog/n02105641_8421.jpg\nimages_split/train_10_percent/sealyham_terrier\nMaking directory: images_split/train_10_percent/sealyham_terrier\nCopying: images_split/train/sealyham_terrier/n02095889_136.jpg to images_split/train_10_percent/sealyham_terrier/n02095889_136.jpg\nimages_split/train_10_percent/pomeranian\nCopying: images_split/train/pomeranian/n02112018_4639.jpg to images_split/train_10_percent/pomeranian/n02112018_4639.jpg\nimages_split/train_10_percent/pembroke\nMaking directory: images_split/train_10_percent/pembroke\nCopying: images_split/train/pembroke/n02113023_9001.jpg to images_split/train_10_percent/pembroke/n02113023_9001.jpg\nimages_split/train_10_percent/cocker_spaniel\nCopying: images_split/train/cocker_spaniel/n02102318_9488.jpg to images_split/train_10_percent/cocker_spaniel/n02102318_9488.jpg\nimages_split/train_10_percent/malinois\nCopying: images_split/train/malinois/n02105162_6449.jpg to images_split/train_10_percent/malinois/n02105162_6449.jpg\nimages_split/train_10_percent/briard\nCopying: images_split/train/briard/n02105251_2317.jpg to images_split/train_10_percent/briard/n02105251_2317.jpg\nimages_split/train_10_percent/african_hunting_dog\nCopying: images_split/train/african_hunting_dog/n02116738_7988.jpg to images_split/train_10_percent/african_hunting_dog/n02116738_7988.jpg\nimages_split/train_10_percent/collie\nMaking directory: images_split/train_10_percent/collie\nCopying: images_split/train/collie/n02106030_16290.jpg to images_split/train_10_percent/collie/n02106030_16290.jpg\nimages_split/train_10_percent/cocker_spaniel\nCopying: images_split/train/cocker_spaniel/n02102318_12877.jpg to images_split/train_10_percent/cocker_spaniel/n02102318_12877.jpg\nimages_split/train_10_percent/irish_wolfhound\nMaking directory: images_split/train_10_percent/irish_wolfhound\nCopying: images_split/train/irish_wolfhound/n02090721_6715.jpg to images_split/train_10_percent/irish_wolfhound/n02090721_6715.jpg\nimages_split/train_10_percent/samoyed\nMaking directory: images_split/train_10_percent/samoyed\nCopying: images_split/train/samoyed/n02111889_1968.jpg to images_split/train_10_percent/samoyed/n02111889_1968.jpg\nimages_split/train_10_percent/irish_wolfhound\nCopying: images_split/train/irish_wolfhound/n02090721_1862.jpg to images_split/train_10_percent/irish_wolfhound/n02090721_1862.jpg\nimages_split/train_10_percent/standard_poodle\nCopying: images_split/train/standard_poodle/n02113799_7092.jpg to images_split/train_10_percent/standard_poodle/n02113799_7092.jpg\nimages_split/train_10_percent/german_shepherd\nCopying: images_split/train/german_shepherd/n02106662_7122.jpg to images_split/train_10_percent/german_shepherd/n02106662_7122.jpg\nimages_split/train_10_percent/weimaraner\nCopying: images_split/train/weimaraner/n02092339_40.jpg to images_split/train_10_percent/weimaraner/n02092339_40.jpg\nimages_split/train_10_percent/malinois\nCopying: images_split/train/malinois/n02105162_1378.jpg to images_split/train_10_percent/malinois/n02105162_1378.jpg\nimages_split/train_10_percent/sussex_spaniel\nMaking directory: images_split/train_10_percent/sussex_spaniel\nCopying: images_split/train/sussex_spaniel/n02102480_5391.jpg to images_split/train_10_percent/sussex_spaniel/n02102480_5391.jpg\nimages_split/train_10_percent/english_setter\nMaking directory: images_split/train_10_percent/english_setter\nCopying: images_split/train/english_setter/n02100735_748.jpg to images_split/train_10_percent/english_setter/n02100735_748.jpg\nimages_split/train_10_percent/ibizan_hound\nCopying: images_split/train/ibizan_hound/n02091244_530.jpg to images_split/train_10_percent/ibizan_hound/n02091244_530.jpg\nimages_split/train_10_percent/bloodhound\nCopying: images_split/train/bloodhound/n02088466_8289.jpg to images_split/train_10_percent/bloodhound/n02088466_8289.jpg\nimages_split/train_10_percent/bedlington_terrier\nMaking directory: images_split/train_10_percent/bedlington_terrier\nCopying: images_split/train/bedlington_terrier/n02093647_2316.jpg to images_split/train_10_percent/bedlington_terrier/n02093647_2316.jpg\nimages_split/train_10_percent/borzoi\nCopying: images_split/train/borzoi/n02090622_7602.jpg to images_split/train_10_percent/borzoi/n02090622_7602.jpg\nimages_split/train_10_percent/shih_tzu\nCopying: images_split/train/shih_tzu/n02086240_447.jpg to images_split/train_10_percent/shih_tzu/n02086240_447.jpg\nimages_split/train_10_percent/papillon\nCopying: images_split/train/papillon/n02086910_6207.jpg to images_split/train_10_percent/papillon/n02086910_6207.jpg\nimages_split/train_10_percent/tibetan_mastiff\nCopying: images_split/train/tibetan_mastiff/n02108551_2025.jpg to images_split/train_10_percent/tibetan_mastiff/n02108551_2025.jpg\nimages_split/train_10_percent/bernese_mountain_dog\nCopying: images_split/train/bernese_mountain_dog/n02107683_2567.jpg to images_split/train_10_percent/bernese_mountain_dog/n02107683_2567.jpg\nimages_split/train_10_percent/redbone\nMaking directory: images_split/train_10_percent/redbone\nCopying: images_split/train/redbone/n02090379_4146.jpg to images_split/train_10_percent/redbone/n02090379_4146.jpg\nimages_split/train_10_percent/miniature_poodle\nCopying: images_split/train/miniature_poodle/n02113712_1590.jpg to images_split/train_10_percent/miniature_poodle/n02113712_1590.jpg\nimages_split/train_10_percent/schipperke\nMaking directory: images_split/train_10_percent/schipperke\nCopying: images_split/train/schipperke/n02104365_1325.jpg to images_split/train_10_percent/schipperke/n02104365_1325.jpg\nimages_split/train_10_percent/whippet\nCopying: images_split/train/whippet/n02091134_9793.jpg to images_split/train_10_percent/whippet/n02091134_9793.jpg\nimages_split/train_10_percent/redbone\nCopying: images_split/train/redbone/n02090379_1345.jpg to images_split/train_10_percent/redbone/n02090379_1345.jpg\nimages_split/train_10_percent/great_dane\nMaking directory: images_split/train_10_percent/great_dane\nCopying: images_split/train/great_dane/n02109047_18614.jpg to images_split/train_10_percent/great_dane/n02109047_18614.jpg\nimages_split/train_10_percent/basset\nMaking directory: images_split/train_10_percent/basset\nCopying: images_split/train/basset/n02088238_9446.jpg to images_split/train_10_percent/basset/n02088238_9446.jpg\nimages_split/train_10_percent/kelpie\nMaking directory: images_split/train_10_percent/kelpie\nCopying: images_split/train/kelpie/n02105412_3335.jpg to images_split/train_10_percent/kelpie/n02105412_3335.jpg\nimages_split/train_10_percent/boxer\nCopying: images_split/train/boxer/n02108089_78.jpg to images_split/train_10_percent/boxer/n02108089_78.jpg\nimages_split/train_10_percent/saluki\nCopying: images_split/train/saluki/n02091831_3067.jpg to images_split/train_10_percent/saluki/n02091831_3067.jpg\nimages_split/train_10_percent/tibetan_mastiff\nCopying: images_split/train/tibetan_mastiff/n02108551_1700.jpg to images_split/train_10_percent/tibetan_mastiff/n02108551_1700.jpg\nimages_split/train_10_percent/otterhound\nMaking directory: images_split/train_10_percent/otterhound\nCopying: images_split/train/otterhound/n02091635_2948.jpg to images_split/train_10_percent/otterhound/n02091635_2948.jpg\nimages_split/train_10_percent/komondor\nCopying: images_split/train/komondor/n02105505_4180.jpg to images_split/train_10_percent/komondor/n02105505_4180.jpg\nimages_split/train_10_percent/tibetan_mastiff\nCopying: images_split/train/tibetan_mastiff/n02108551_239.jpg to images_split/train_10_percent/tibetan_mastiff/n02108551_239.jpg\nimages_split/train_10_percent/irish_setter\nCopying: images_split/train/irish_setter/n02100877_378.jpg to images_split/train_10_percent/irish_setter/n02100877_378.jpg\nimages_split/train_10_percent/cocker_spaniel\nCopying: images_split/train/cocker_spaniel/n02102318_9755.jpg to images_split/train_10_percent/cocker_spaniel/n02102318_9755.jpg\nimages_split/train_10_percent/sealyham_terrier\nCopying: images_split/train/sealyham_terrier/n02095889_4116.jpg to images_split/train_10_percent/sealyham_terrier/n02095889_4116.jpg\nimages_split/train_10_percent/brittany_spaniel\nCopying: images_split/train/brittany_spaniel/n02101388_2737.jpg to images_split/train_10_percent/brittany_spaniel/n02101388_2737.jpg\nimages_split/train_10_percent/komondor\nCopying: images_split/train/komondor/n02105505_561.jpg to images_split/train_10_percent/komondor/n02105505_561.jpg\nimages_split/train_10_percent/standard_schnauzer\nCopying: images_split/train/standard_schnauzer/n02097209_1491.jpg to images_split/train_10_percent/standard_schnauzer/n02097209_1491.jpg\nimages_split/train_10_percent/leonberg\nMaking directory: images_split/train_10_percent/leonberg\nCopying: images_split/train/leonberg/n02111129_3595.jpg to images_split/train_10_percent/leonberg/n02111129_3595.jpg\nimages_split/train_10_percent/borzoi\nCopying: images_split/train/borzoi/n02090622_2055.jpg to images_split/train_10_percent/borzoi/n02090622_2055.jpg\nimages_split/train_10_percent/italian_greyhound\nCopying: images_split/train/italian_greyhound/n02091032_1400.jpg to images_split/train_10_percent/italian_greyhound/n02091032_1400.jpg\nimages_split/train_10_percent/black_and_tan_coonhound\nMaking directory: images_split/train_10_percent/black_and_tan_coonhound\nCopying: images_split/train/black_and_tan_coonhound/n02089078_1025.jpg to images_split/train_10_percent/black_and_tan_coonhound/n02089078_1025.jpg\nimages_split/train_10_percent/otterhound\nCopying: images_split/train/otterhound/n02091635_2482.jpg to images_split/train_10_percent/otterhound/n02091635_2482.jpg\nimages_split/train_10_percent/silky_terrier\nCopying: images_split/train/silky_terrier/n02097658_7474.jpg to images_split/train_10_percent/silky_terrier/n02097658_7474.jpg\nimages_split/train_10_percent/cocker_spaniel\nCopying: images_split/train/cocker_spaniel/n02102318_10818.jpg to images_split/train_10_percent/cocker_spaniel/n02102318_10818.jpg\nimages_split/train_10_percent/border_collie\nCopying: images_split/train/border_collie/n02106166_90.jpg to images_split/train_10_percent/border_collie/n02106166_90.jpg\nimages_split/train_10_percent/cardigan\nMaking directory: images_split/train_10_percent/cardigan\nCopying: images_split/train/cardigan/n02113186_11037.jpg to images_split/train_10_percent/cardigan/n02113186_11037.jpg\nimages_split/train_10_percent/sussex_spaniel\nCopying: images_split/train/sussex_spaniel/n02102480_6390.jpg to images_split/train_10_percent/sussex_spaniel/n02102480_6390.jpg\nimages_split/train_10_percent/bedlington_terrier\nCopying: images_split/train/bedlington_terrier/n02093647_2633.jpg to images_split/train_10_percent/bedlington_terrier/n02093647_2633.jpg\nimages_split/train_10_percent/shih_tzu\nCopying: images_split/train/shih_tzu/n02086240_3802.jpg to images_split/train_10_percent/shih_tzu/n02086240_3802.jpg\nimages_split/train_10_percent/standard_schnauzer\nCopying: images_split/train/standard_schnauzer/n02097209_213.jpg to images_split/train_10_percent/standard_schnauzer/n02097209_213.jpg\nimages_split/train_10_percent/clumber\nCopying: images_split/train/clumber/n02101556_7986.jpg to images_split/train_10_percent/clumber/n02101556_7986.jpg\nimages_split/train_10_percent/toy_poodle\nCopying: images_split/train/toy_poodle/n02113624_3093.jpg to images_split/train_10_percent/toy_poodle/n02113624_3093.jpg\nimages_split/train_10_percent/english_springer\nCopying: images_split/train/english_springer/n02102040_3693.jpg to images_split/train_10_percent/english_springer/n02102040_3693.jpg\nimages_split/train_10_percent/brabancon_griffon\nMaking directory: images_split/train_10_percent/brabancon_griffon\nCopying: images_split/train/brabancon_griffon/n02112706_1814.jpg to images_split/train_10_percent/brabancon_griffon/n02112706_1814.jpg\nimages_split/train_10_percent/papillon\nCopying: images_split/train/papillon/n02086910_1729.jpg to images_split/train_10_percent/papillon/n02086910_1729.jpg\nimages_split/train_10_percent/collie\nCopying: images_split/train/collie/n02106030_10377.jpg to images_split/train_10_percent/collie/n02106030_10377.jpg\nimages_split/train_10_percent/basset\nCopying: images_split/train/basset/n02088238_9324.jpg to images_split/train_10_percent/basset/n02088238_9324.jpg\nimages_split/train_10_percent/black_and_tan_coonhound\nCopying: images_split/train/black_and_tan_coonhound/n02089078_2962.jpg to images_split/train_10_percent/black_and_tan_coonhound/n02089078_2962.jpg\nimages_split/train_10_percent/flat_coated_retriever\nCopying: images_split/train/flat_coated_retriever/n02099267_3441.jpg to images_split/train_10_percent/flat_coated_retriever/n02099267_3441.jpg\nimages_split/train_10_percent/irish_water_spaniel\nCopying: images_split/train/irish_water_spaniel/n02102973_1714.jpg to images_split/train_10_percent/irish_water_spaniel/n02102973_1714.jpg\nimages_split/train_10_percent/english_setter\nCopying: images_split/train/english_setter/n02100735_7525.jpg to images_split/train_10_percent/english_setter/n02100735_7525.jpg\nimages_split/train_10_percent/soft_coated_wheaten_terrier\nMaking directory: images_split/train_10_percent/soft_coated_wheaten_terrier\nCopying: images_split/train/soft_coated_wheaten_terrier/n02098105_1066.jpg to images_split/train_10_percent/soft_coated_wheaten_terrier/n02098105_1066.jpg\nimages_split/train_10_percent/komondor\nCopying: images_split/train/komondor/n02105505_4271.jpg to images_split/train_10_percent/komondor/n02105505_4271.jpg\nimages_split/train_10_percent/german_shepherd\nCopying: images_split/train/german_shepherd/n02106662_13123.jpg to images_split/train_10_percent/german_shepherd/n02106662_13123.jpg\nimages_split/train_10_percent/wire_haired_fox_terrier\nMaking directory: images_split/train_10_percent/wire_haired_fox_terrier\nCopying: images_split/train/wire_haired_fox_terrier/n02095314_2059.jpg to images_split/train_10_percent/wire_haired_fox_terrier/n02095314_2059.jpg\nimages_split/train_10_percent/basenji\nMaking directory: images_split/train_10_percent/basenji\nCopying: images_split/train/basenji/n02110806_4853.jpg to images_split/train_10_percent/basenji/n02110806_4853.jpg\nimages_split/train_10_percent/norfolk_terrier\nCopying: images_split/train/norfolk_terrier/n02094114_1972.jpg to images_split/train_10_percent/norfolk_terrier/n02094114_1972.jpg\nimages_split/train_10_percent/basenji\nCopying: images_split/train/basenji/n02110806_2580.jpg to images_split/train_10_percent/basenji/n02110806_2580.jpg\nimages_split/train_10_percent/chihuahua\nCopying: images_split/train/chihuahua/n02085620_574.jpg to images_split/train_10_percent/chihuahua/n02085620_574.jpg\nimages_split/train_10_percent/golden_retriever\nCopying: images_split/train/golden_retriever/n02099601_846.jpg to images_split/train_10_percent/golden_retriever/n02099601_846.jpg\nimages_split/train_10_percent/ibizan_hound\nCopying: images_split/train/ibizan_hound/n02091244_5943.jpg to images_split/train_10_percent/ibizan_hound/n02091244_5943.jpg\nimages_split/train_10_percent/bouvier_des_flandres\nCopying: images_split/train/bouvier_des_flandres/n02106382_1365.jpg to images_split/train_10_percent/bouvier_des_flandres/n02106382_1365.jpg\nimages_split/train_10_percent/staffordshire_bullterrier\nMaking directory: images_split/train_10_percent/staffordshire_bullterrier\nCopying: images_split/train/staffordshire_bullterrier/n02093256_4245.jpg to images_split/train_10_percent/staffordshire_bullterrier/n02093256_4245.jpg\nimages_split/train_10_percent/greater_swiss_mountain_dog\nCopying: images_split/train/greater_swiss_mountain_dog/n02107574_1597.jpg to images_split/train_10_percent/greater_swiss_mountain_dog/n02107574_1597.jpg\nimages_split/train_10_percent/irish_setter\nCopying: images_split/train/irish_setter/n02100877_239.jpg to images_split/train_10_percent/irish_setter/n02100877_239.jpg\nimages_split/train_10_percent/miniature_pinscher\nCopying: images_split/train/miniature_pinscher/n02107312_3238.jpg to images_split/train_10_percent/miniature_pinscher/n02107312_3238.jpg\nimages_split/train_10_percent/bluetick\nMaking directory: images_split/train_10_percent/bluetick\nCopying: images_split/train/bluetick/n02088632_747.jpg to images_split/train_10_percent/bluetick/n02088632_747.jpg\nimages_split/train_10_percent/west_highland_white_terrier\nCopying: images_split/train/west_highland_white_terrier/n02098286_321.jpg to images_split/train_10_percent/west_highland_white_terrier/n02098286_321.jpg\nimages_split/train_10_percent/lhasa\nMaking directory: images_split/train_10_percent/lhasa\nCopying: images_split/train/lhasa/n02098413_2607.jpg to images_split/train_10_percent/lhasa/n02098413_2607.jpg\nimages_split/train_10_percent/boxer\nCopying: images_split/train/boxer/n02108089_7456.jpg to images_split/train_10_percent/boxer/n02108089_7456.jpg\nimages_split/train_10_percent/vizsla\nMaking directory: images_split/train_10_percent/vizsla\nCopying: images_split/train/vizsla/n02100583_2885.jpg to images_split/train_10_percent/vizsla/n02100583_2885.jpg\nimages_split/train_10_percent/cocker_spaniel\nCopying: images_split/train/cocker_spaniel/n02102318_11481.jpg to images_split/train_10_percent/cocker_spaniel/n02102318_11481.jpg\nimages_split/train_10_percent/basset\nCopying: images_split/train/basset/n02088238_9701.jpg to images_split/train_10_percent/basset/n02088238_9701.jpg\nimages_split/train_10_percent/toy_poodle\nCopying: images_split/train/toy_poodle/n02113624_166.jpg to images_split/train_10_percent/toy_poodle/n02113624_166.jpg\nimages_split/train_10_percent/bernese_mountain_dog\nCopying: images_split/train/bernese_mountain_dog/n02107683_4016.jpg to images_split/train_10_percent/bernese_mountain_dog/n02107683_4016.jpg\nimages_split/train_10_percent/kuvasz\nCopying: images_split/train/kuvasz/n02104029_1484.jpg to images_split/train_10_percent/kuvasz/n02104029_1484.jpg\nimages_split/train_10_percent/collie\nCopying: images_split/train/collie/n02106030_10675.jpg to images_split/train_10_percent/collie/n02106030_10675.jpg\nimages_split/train_10_percent/bouvier_des_flandres\nCopying: images_split/train/bouvier_des_flandres/n02106382_2676.jpg to images_split/train_10_percent/bouvier_des_flandres/n02106382_2676.jpg\nimages_split/train_10_percent/dhole\nMaking directory: images_split/train_10_percent/dhole\nCopying: images_split/train/dhole/n02115913_2412.jpg to images_split/train_10_percent/dhole/n02115913_2412.jpg\nimages_split/train_10_percent/english_springer\nCopying: images_split/train/english_springer/n02102040_841.jpg to images_split/train_10_percent/english_springer/n02102040_841.jpg\nimages_split/train_10_percent/malamute\nMaking directory: images_split/train_10_percent/malamute\nCopying: images_split/train/malamute/n02110063_12326.jpg to images_split/train_10_percent/malamute/n02110063_12326.jpg\nimages_split/train_10_percent/siberian_husky\nCopying: images_split/train/siberian_husky/n02110185_11635.jpg to images_split/train_10_percent/siberian_husky/n02110185_11635.jpg\nimages_split/train_10_percent/standard_poodle\nCopying: images_split/train/standard_poodle/n02113799_5975.jpg to images_split/train_10_percent/standard_poodle/n02113799_5975.jpg\nimages_split/train_10_percent/appenzeller\nCopying: images_split/train/appenzeller/n02107908_4891.jpg to images_split/train_10_percent/appenzeller/n02107908_4891.jpg\nimages_split/train_10_percent/brabancon_griffon\nCopying: images_split/train/brabancon_griffon/n02112706_1875.jpg to images_split/train_10_percent/brabancon_griffon/n02112706_1875.jpg\nimages_split/train_10_percent/leonberg\nCopying: images_split/train/leonberg/n02111129_74.jpg to images_split/train_10_percent/leonberg/n02111129_74.jpg\nimages_split/train_10_percent/french_bulldog\nMaking directory: images_split/train_10_percent/french_bulldog\nCopying: images_split/train/french_bulldog/n02108915_1895.jpg to images_split/train_10_percent/french_bulldog/n02108915_1895.jpg\nimages_split/train_10_percent/rottweiler\nCopying: images_split/train/rottweiler/n02106550_1388.jpg to images_split/train_10_percent/rottweiler/n02106550_1388.jpg\nimages_split/train_10_percent/keeshond\nMaking directory: images_split/train_10_percent/keeshond\nCopying: images_split/train/keeshond/n02112350_7618.jpg to images_split/train_10_percent/keeshond/n02112350_7618.jpg\nimages_split/train_10_percent/briard\nCopying: images_split/train/briard/n02105251_7434.jpg to images_split/train_10_percent/briard/n02105251_7434.jpg\nimages_split/train_10_percent/shetland_sheepdog\nCopying: images_split/train/shetland_sheepdog/n02105855_3103.jpg to images_split/train_10_percent/shetland_sheepdog/n02105855_3103.jpg\nimages_split/train_10_percent/miniature_poodle\nCopying: images_split/train/miniature_poodle/n02113712_8473.jpg to images_split/train_10_percent/miniature_poodle/n02113712_8473.jpg\nimages_split/train_10_percent/brittany_spaniel\nCopying: images_split/train/brittany_spaniel/n02101388_9859.jpg to images_split/train_10_percent/brittany_spaniel/n02101388_9859.jpg\nimages_split/train_10_percent/soft_coated_wheaten_terrier\nCopying: images_split/train/soft_coated_wheaten_terrier/n02098105_4120.jpg to images_split/train_10_percent/soft_coated_wheaten_terrier/n02098105_4120.jpg\nimages_split/train_10_percent/australian_terrier\nCopying: images_split/train/australian_terrier/n02096294_8476.jpg to images_split/train_10_percent/australian_terrier/n02096294_8476.jpg\nimages_split/train_10_percent/boxer\nCopying: images_split/train/boxer/n02108089_1956.jpg to images_split/train_10_percent/boxer/n02108089_1956.jpg\nimages_split/train_10_percent/border_terrier\nCopying: images_split/train/border_terrier/n02093754_6098.jpg to images_split/train_10_percent/border_terrier/n02093754_6098.jpg\nimages_split/train_10_percent/australian_terrier\nCopying: images_split/train/australian_terrier/n02096294_7804.jpg to images_split/train_10_percent/australian_terrier/n02096294_7804.jpg\nimages_split/train_10_percent/shetland_sheepdog\nCopying: images_split/train/shetland_sheepdog/n02105855_3255.jpg to images_split/train_10_percent/shetland_sheepdog/n02105855_3255.jpg\nimages_split/train_10_percent/saluki\nCopying: images_split/train/saluki/n02091831_10823.jpg to images_split/train_10_percent/saluki/n02091831_10823.jpg\nimages_split/train_10_percent/greater_swiss_mountain_dog\nCopying: images_split/train/greater_swiss_mountain_dog/n02107574_2033.jpg to images_split/train_10_percent/greater_swiss_mountain_dog/n02107574_2033.jpg\nimages_split/train_10_percent/gordon_setter\nCopying: images_split/train/gordon_setter/n02101006_1249.jpg to images_split/train_10_percent/gordon_setter/n02101006_1249.jpg\nimages_split/train_10_percent/beagle\nMaking directory: images_split/train_10_percent/beagle\nCopying: images_split/train/beagle/n02088364_17314.jpg to images_split/train_10_percent/beagle/n02088364_17314.jpg\nimages_split/train_10_percent/norfolk_terrier\nCopying: images_split/train/norfolk_terrier/n02094114_699.jpg to images_split/train_10_percent/norfolk_terrier/n02094114_699.jpg\nimages_split/train_10_percent/border_terrier\nCopying: images_split/train/border_terrier/n02093754_4448.jpg to images_split/train_10_percent/border_terrier/n02093754_4448.jpg\nimages_split/train_10_percent/greater_swiss_mountain_dog\nCopying: images_split/train/greater_swiss_mountain_dog/n02107574_2998.jpg to images_split/train_10_percent/greater_swiss_mountain_dog/n02107574_2998.jpg\nimages_split/train_10_percent/maltese_dog\nCopying: images_split/train/maltese_dog/n02085936_8756.jpg to images_split/train_10_percent/maltese_dog/n02085936_8756.jpg\nimages_split/train_10_percent/yorkshire_terrier\nMaking directory: images_split/train_10_percent/yorkshire_terrier\nCopying: images_split/train/yorkshire_terrier/n02094433_1770.jpg to images_split/train_10_percent/yorkshire_terrier/n02094433_1770.jpg\nimages_split/train_10_percent/greater_swiss_mountain_dog\nCopying: images_split/train/greater_swiss_mountain_dog/n02107574_2290.jpg to images_split/train_10_percent/greater_swiss_mountain_dog/n02107574_2290.jpg\nimages_split/train_10_percent/french_bulldog\nCopying: images_split/train/french_bulldog/n02108915_11653.jpg to images_split/train_10_percent/french_bulldog/n02108915_11653.jpg\nimages_split/train_10_percent/dingo\nCopying: images_split/train/dingo/n02115641_10021.jpg to images_split/train_10_percent/dingo/n02115641_10021.jpg\nimages_split/train_10_percent/afghan_hound\nCopying: images_split/train/afghan_hound/n02088094_13442.jpg to images_split/train_10_percent/afghan_hound/n02088094_13442.jpg\nimages_split/train_10_percent/lakeland_terrier\nCopying: images_split/train/lakeland_terrier/n02095570_2354.jpg to images_split/train_10_percent/lakeland_terrier/n02095570_2354.jpg\nimages_split/train_10_percent/staffordshire_bullterrier\nCopying: images_split/train/staffordshire_bullterrier/n02093256_5936.jpg to images_split/train_10_percent/staffordshire_bullterrier/n02093256_5936.jpg\nimages_split/train_10_percent/weimaraner\nCopying: images_split/train/weimaraner/n02092339_6334.jpg to images_split/train_10_percent/weimaraner/n02092339_6334.jpg\nimages_split/train_10_percent/collie\nCopying: images_split/train/collie/n02106030_17418.jpg to images_split/train_10_percent/collie/n02106030_17418.jpg\nimages_split/train_10_percent/bluetick\nCopying: images_split/train/bluetick/n02088632_3749.jpg to images_split/train_10_percent/bluetick/n02088632_3749.jpg\nimages_split/train_10_percent/otterhound\nCopying: images_split/train/otterhound/n02091635_3479.jpg to images_split/train_10_percent/otterhound/n02091635_3479.jpg\nimages_split/train_10_percent/affenpinscher\nMaking directory: images_split/train_10_percent/affenpinscher\nCopying: images_split/train/affenpinscher/n02110627_8250.jpg to images_split/train_10_percent/affenpinscher/n02110627_8250.jpg\nimages_split/train_10_percent/welsh_springer_spaniel\nCopying: images_split/train/welsh_springer_spaniel/n02102177_2628.jpg to images_split/train_10_percent/welsh_springer_spaniel/n02102177_2628.jpg\nimages_split/train_10_percent/lhasa\nCopying: images_split/train/lhasa/n02098413_8468.jpg to images_split/train_10_percent/lhasa/n02098413_8468.jpg\nimages_split/train_10_percent/basenji\nCopying: images_split/train/basenji/n02110806_4012.jpg to images_split/train_10_percent/basenji/n02110806_4012.jpg\nimages_split/train_10_percent/lhasa\nCopying: images_split/train/lhasa/n02098413_10285.jpg to images_split/train_10_percent/lhasa/n02098413_10285.jpg\nimages_split/train_10_percent/toy_terrier\nMaking directory: images_split/train_10_percent/toy_terrier\nCopying: images_split/train/toy_terrier/n02087046_7191.jpg to images_split/train_10_percent/toy_terrier/n02087046_7191.jpg\nimages_split/train_10_percent/norwich_terrier\nCopying: images_split/train/norwich_terrier/n02094258_1510.jpg to images_split/train_10_percent/norwich_terrier/n02094258_1510.jpg\nimages_split/train_10_percent/blenheim_spaniel\nCopying: images_split/train/blenheim_spaniel/n02086646_1447.jpg to images_split/train_10_percent/blenheim_spaniel/n02086646_1447.jpg\nimages_split/train_10_percent/blenheim_spaniel\nCopying: images_split/train/blenheim_spaniel/n02086646_637.jpg to images_split/train_10_percent/blenheim_spaniel/n02086646_637.jpg\nimages_split/train_10_percent/giant_schnauzer\nCopying: images_split/train/giant_schnauzer/n02097130_1197.jpg to images_split/train_10_percent/giant_schnauzer/n02097130_1197.jpg\nimages_split/train_10_percent/samoyed\nCopying: images_split/train/samoyed/n02111889_12811.jpg to images_split/train_10_percent/samoyed/n02111889_12811.jpg\nimages_split/train_10_percent/walker_hound\nCopying: images_split/train/walker_hound/n02089867_3260.jpg to images_split/train_10_percent/walker_hound/n02089867_3260.jpg\nimages_split/train_10_percent/bernese_mountain_dog\nCopying: images_split/train/bernese_mountain_dog/n02107683_5684.jpg to images_split/train_10_percent/bernese_mountain_dog/n02107683_5684.jpg\nimages_split/train_10_percent/boston_bull\nMaking directory: images_split/train_10_percent/boston_bull\nCopying: images_split/train/boston_bull/n02096585_10380.jpg to images_split/train_10_percent/boston_bull/n02096585_10380.jpg\nimages_split/train_10_percent/great_dane\nCopying: images_split/train/great_dane/n02109047_2630.jpg to images_split/train_10_percent/great_dane/n02109047_2630.jpg\nimages_split/train_10_percent/bouvier_des_flandres\nCopying: images_split/train/bouvier_des_flandres/n02106382_2322.jpg to images_split/train_10_percent/bouvier_des_flandres/n02106382_2322.jpg\nimages_split/train_10_percent/labrador_retriever\nMaking directory: images_split/train_10_percent/labrador_retriever\nCopying: images_split/train/labrador_retriever/n02099712_2228.jpg to images_split/train_10_percent/labrador_retriever/n02099712_2228.jpg\nimages_split/train_10_percent/appenzeller\nCopying: images_split/train/appenzeller/n02107908_2899.jpg to images_split/train_10_percent/appenzeller/n02107908_2899.jpg\nimages_split/train_10_percent/giant_schnauzer\nCopying: images_split/train/giant_schnauzer/n02097130_4888.jpg to images_split/train_10_percent/giant_schnauzer/n02097130_4888.jpg\nimages_split/train_10_percent/sealyham_terrier\nCopying: images_split/train/sealyham_terrier/n02095889_3497.jpg to images_split/train_10_percent/sealyham_terrier/n02095889_3497.jpg\nimages_split/train_10_percent/scottish_deerhound\nCopying: images_split/train/scottish_deerhound/n02092002_305.jpg to images_split/train_10_percent/scottish_deerhound/n02092002_305.jpg\nimages_split/train_10_percent/norfolk_terrier\nCopying: images_split/train/norfolk_terrier/n02094114_2158.jpg to images_split/train_10_percent/norfolk_terrier/n02094114_2158.jpg\nimages_split/train_10_percent/standard_poodle\nCopying: images_split/train/standard_poodle/n02113799_3098.jpg to images_split/train_10_percent/standard_poodle/n02113799_3098.jpg\nimages_split/train_10_percent/bouvier_des_flandres\nCopying: images_split/train/bouvier_des_flandres/n02106382_2083.jpg to images_split/train_10_percent/bouvier_des_flandres/n02106382_2083.jpg\nimages_split/train_10_percent/brittany_spaniel\nCopying: images_split/train/brittany_spaniel/n02101388_1517.jpg to images_split/train_10_percent/brittany_spaniel/n02101388_1517.jpg\nimages_split/train_10_percent/bedlington_terrier\nCopying: images_split/train/bedlington_terrier/n02093647_55.jpg to images_split/train_10_percent/bedlington_terrier/n02093647_55.jpg\nimages_split/train_10_percent/briard\nCopying: images_split/train/briard/n02105251_7977.jpg to images_split/train_10_percent/briard/n02105251_7977.jpg\nimages_split/train_10_percent/norwich_terrier\nCopying: images_split/train/norwich_terrier/n02094258_1918.jpg to images_split/train_10_percent/norwich_terrier/n02094258_1918.jpg\nimages_split/train_10_percent/norwich_terrier\nCopying: images_split/train/norwich_terrier/n02094258_3435.jpg to images_split/train_10_percent/norwich_terrier/n02094258_3435.jpg\nimages_split/train_10_percent/irish_setter\nCopying: images_split/train/irish_setter/n02100877_8800.jpg to images_split/train_10_percent/irish_setter/n02100877_8800.jpg\nimages_split/train_10_percent/rottweiler\nCopying: images_split/train/rottweiler/n02106550_7616.jpg to images_split/train_10_percent/rottweiler/n02106550_7616.jpg\nimages_split/train_10_percent/malinois\nCopying: images_split/train/malinois/n02105162_4056.jpg to images_split/train_10_percent/malinois/n02105162_4056.jpg\nimages_split/train_10_percent/curly_coated_retriever\nMaking directory: images_split/train_10_percent/curly_coated_retriever\nCopying: images_split/train/curly_coated_retriever/n02099429_3029.jpg to images_split/train_10_percent/curly_coated_retriever/n02099429_3029.jpg\nimages_split/train_10_percent/afghan_hound\nCopying: images_split/train/afghan_hound/n02088094_1907.jpg to images_split/train_10_percent/afghan_hound/n02088094_1907.jpg\nimages_split/train_10_percent/pomeranian\nCopying: images_split/train/pomeranian/n02112018_11105.jpg to images_split/train_10_percent/pomeranian/n02112018_11105.jpg\nimages_split/train_10_percent/briard\nCopying: images_split/train/briard/n02105251_5641.jpg to images_split/train_10_percent/briard/n02105251_5641.jpg\nimages_split/train_10_percent/golden_retriever\nCopying: images_split/train/golden_retriever/n02099601_1442.jpg to images_split/train_10_percent/golden_retriever/n02099601_1442.jpg\nimages_split/train_10_percent/entlebucher\nMaking directory: images_split/train_10_percent/entlebucher\nCopying: images_split/train/entlebucher/n02108000_1714.jpg to images_split/train_10_percent/entlebucher/n02108000_1714.jpg\nimages_split/train_10_percent/boxer\nCopying: images_split/train/boxer/n02108089_1367.jpg to images_split/train_10_percent/boxer/n02108089_1367.jpg\nimages_split/train_10_percent/scottish_deerhound\nCopying: images_split/train/scottish_deerhound/n02092002_983.jpg to images_split/train_10_percent/scottish_deerhound/n02092002_983.jpg\nimages_split/train_10_percent/labrador_retriever\nCopying: images_split/train/labrador_retriever/n02099712_8014.jpg to images_split/train_10_percent/labrador_retriever/n02099712_8014.jpg\nimages_split/train_10_percent/mexican_hairless\nCopying: images_split/train/mexican_hairless/n02113978_737.jpg to images_split/train_10_percent/mexican_hairless/n02113978_737.jpg\nimages_split/train_10_percent/collie\nCopying: images_split/train/collie/n02106030_18685.jpg to images_split/train_10_percent/collie/n02106030_18685.jpg\nimages_split/train_10_percent/saint_bernard\nMaking directory: images_split/train_10_percent/saint_bernard\nCopying: images_split/train/saint_bernard/n02109525_9869.jpg to images_split/train_10_percent/saint_bernard/n02109525_9869.jpg\nimages_split/train_10_percent/blenheim_spaniel\nCopying: images_split/train/blenheim_spaniel/n02086646_2815.jpg to images_split/train_10_percent/blenheim_spaniel/n02086646_2815.jpg\nimages_split/train_10_percent/dingo\nCopying: images_split/train/dingo/n02115641_3360.jpg to images_split/train_10_percent/dingo/n02115641_3360.jpg\nimages_split/train_10_percent/vizsla\nCopying: images_split/train/vizsla/n02100583_5792.jpg to images_split/train_10_percent/vizsla/n02100583_5792.jpg\nimages_split/train_10_percent/toy_terrier\nCopying: images_split/train/toy_terrier/n02087046_4506.jpg to images_split/train_10_percent/toy_terrier/n02087046_4506.jpg\nimages_split/train_10_percent/staffordshire_bullterrier\nCopying: images_split/train/staffordshire_bullterrier/n02093256_2405.jpg to images_split/train_10_percent/staffordshire_bullterrier/n02093256_2405.jpg\nimages_split/train_10_percent/great_pyrenees\nMaking directory: images_split/train_10_percent/great_pyrenees\nCopying: images_split/train/great_pyrenees/n02111500_337.jpg to images_split/train_10_percent/great_pyrenees/n02111500_337.jpg\nimages_split/train_10_percent/weimaraner\nCopying: images_split/train/weimaraner/n02092339_6209.jpg to images_split/train_10_percent/weimaraner/n02092339_6209.jpg\nimages_split/train_10_percent/standard_poodle\nCopying: images_split/train/standard_poodle/n02113799_6715.jpg to images_split/train_10_percent/standard_poodle/n02113799_6715.jpg\nimages_split/train_10_percent/norfolk_terrier\nCopying: images_split/train/norfolk_terrier/n02094114_4165.jpg to images_split/train_10_percent/norfolk_terrier/n02094114_4165.jpg\nimages_split/train_10_percent/papillon\nCopying: images_split/train/papillon/n02086910_3140.jpg to images_split/train_10_percent/papillon/n02086910_3140.jpg\nimages_split/train_10_percent/whippet\nCopying: images_split/train/whippet/n02091134_13957.jpg to images_split/train_10_percent/whippet/n02091134_13957.jpg\nimages_split/train_10_percent/black_and_tan_coonhound\nCopying: images_split/train/black_and_tan_coonhound/n02089078_3914.jpg to images_split/train_10_percent/black_and_tan_coonhound/n02089078_3914.jpg\nimages_split/train_10_percent/schipperke\nCopying: images_split/train/schipperke/n02104365_10139.jpg to images_split/train_10_percent/schipperke/n02104365_10139.jpg\nimages_split/train_10_percent/cocker_spaniel\nCopying: images_split/train/cocker_spaniel/n02102318_11141.jpg to images_split/train_10_percent/cocker_spaniel/n02102318_11141.jpg\nimages_split/train_10_percent/italian_greyhound\nCopying: images_split/train/italian_greyhound/n02091032_9517.jpg to images_split/train_10_percent/italian_greyhound/n02091032_9517.jpg\nimages_split/train_10_percent/chow\nMaking directory: images_split/train_10_percent/chow\nCopying: images_split/train/chow/n02112137_16520.jpg to images_split/train_10_percent/chow/n02112137_16520.jpg\nimages_split/train_10_percent/bernese_mountain_dog\nCopying: images_split/train/bernese_mountain_dog/n02107683_1747.jpg to images_split/train_10_percent/bernese_mountain_dog/n02107683_1747.jpg\nimages_split/train_10_percent/english_foxhound\nCopying: images_split/train/english_foxhound/n02089973_2497.jpg to images_split/train_10_percent/english_foxhound/n02089973_2497.jpg\nimages_split/train_10_percent/bouvier_des_flandres\nCopying: images_split/train/bouvier_des_flandres/n02106382_4034.jpg to images_split/train_10_percent/bouvier_des_flandres/n02106382_4034.jpg\nimages_split/train_10_percent/african_hunting_dog\nCopying: images_split/train/african_hunting_dog/n02116738_9769.jpg to images_split/train_10_percent/african_hunting_dog/n02116738_9769.jpg\nimages_split/train_10_percent/west_highland_white_terrier\nCopying: images_split/train/west_highland_white_terrier/n02098286_626.jpg to images_split/train_10_percent/west_highland_white_terrier/n02098286_626.jpg\nimages_split/train_10_percent/silky_terrier\nCopying: images_split/train/silky_terrier/n02097658_237.jpg to images_split/train_10_percent/silky_terrier/n02097658_237.jpg\nimages_split/train_10_percent/staffordshire_bullterrier\nCopying: images_split/train/staffordshire_bullterrier/n02093256_8541.jpg to images_split/train_10_percent/staffordshire_bullterrier/n02093256_8541.jpg\nimages_split/train_10_percent/borzoi\nCopying: images_split/train/borzoi/n02090622_8543.jpg to images_split/train_10_percent/borzoi/n02090622_8543.jpg\nimages_split/train_10_percent/norwich_terrier\nCopying: images_split/train/norwich_terrier/n02094258_2116.jpg to images_split/train_10_percent/norwich_terrier/n02094258_2116.jpg\nimages_split/train_10_percent/standard_schnauzer\nCopying: images_split/train/standard_schnauzer/n02097209_3811.jpg to images_split/train_10_percent/standard_schnauzer/n02097209_3811.jpg\nimages_split/train_10_percent/weimaraner\nCopying: images_split/train/weimaraner/n02092339_1856.jpg to images_split/train_10_percent/weimaraner/n02092339_1856.jpg\nimages_split/train_10_percent/norfolk_terrier\nCopying: images_split/train/norfolk_terrier/n02094114_1944.jpg to images_split/train_10_percent/norfolk_terrier/n02094114_1944.jpg\nimages_split/train_10_percent/welsh_springer_spaniel\nCopying: images_split/train/welsh_springer_spaniel/n02102177_1643.jpg to images_split/train_10_percent/welsh_springer_spaniel/n02102177_1643.jpg\nimages_split/train_10_percent/collie\nCopying: images_split/train/collie/n02106030_15388.jpg to images_split/train_10_percent/collie/n02106030_15388.jpg\nimages_split/train_10_percent/norfolk_terrier\nCopying: images_split/train/norfolk_terrier/n02094114_4518.jpg to images_split/train_10_percent/norfolk_terrier/n02094114_4518.jpg\nimages_split/train_10_percent/siberian_husky\nCopying: images_split/train/siberian_husky/n02110185_3039.jpg to images_split/train_10_percent/siberian_husky/n02110185_3039.jpg\nimages_split/train_10_percent/norfolk_terrier\nCopying: images_split/train/norfolk_terrier/n02094114_3458.jpg to images_split/train_10_percent/norfolk_terrier/n02094114_3458.jpg\nimages_split/train_10_percent/labrador_retriever\nCopying: images_split/train/labrador_retriever/n02099712_7968.jpg to images_split/train_10_percent/labrador_retriever/n02099712_7968.jpg\nimages_split/train_10_percent/irish_wolfhound\nCopying: images_split/train/irish_wolfhound/n02090721_1650.jpg to images_split/train_10_percent/irish_wolfhound/n02090721_1650.jpg\nimages_split/train_10_percent/great_pyrenees\nCopying: images_split/train/great_pyrenees/n02111500_6338.jpg to images_split/train_10_percent/great_pyrenees/n02111500_6338.jpg\nimages_split/train_10_percent/border_collie\nCopying: images_split/train/border_collie/n02106166_243.jpg to images_split/train_10_percent/border_collie/n02106166_243.jpg\nimages_split/train_10_percent/irish_setter\nCopying: images_split/train/irish_setter/n02100877_3056.jpg to images_split/train_10_percent/irish_setter/n02100877_3056.jpg\nimages_split/train_10_percent/borzoi\nCopying: images_split/train/borzoi/n02090622_7307.jpg to images_split/train_10_percent/borzoi/n02090622_7307.jpg\nimages_split/train_10_percent/norfolk_terrier\nCopying: images_split/train/norfolk_terrier/n02094114_99.jpg to images_split/train_10_percent/norfolk_terrier/n02094114_99.jpg\nimages_split/train_10_percent/sussex_spaniel\nCopying: images_split/train/sussex_spaniel/n02102480_4217.jpg to images_split/train_10_percent/sussex_spaniel/n02102480_4217.jpg\nimages_split/train_10_percent/japanese_spaniel\nCopying: images_split/train/japanese_spaniel/n02085782_230.jpg to images_split/train_10_percent/japanese_spaniel/n02085782_230.jpg\nimages_split/train_10_percent/curly_coated_retriever\nCopying: images_split/train/curly_coated_retriever/n02099429_3396.jpg to images_split/train_10_percent/curly_coated_retriever/n02099429_3396.jpg\nimages_split/train_10_percent/greater_swiss_mountain_dog\nCopying: images_split/train/greater_swiss_mountain_dog/n02107574_1132.jpg to images_split/train_10_percent/greater_swiss_mountain_dog/n02107574_1132.jpg\nimages_split/train_10_percent/redbone\nCopying: images_split/train/redbone/n02090379_5005.jpg to images_split/train_10_percent/redbone/n02090379_5005.jpg\nimages_split/train_10_percent/greater_swiss_mountain_dog\nCopying: images_split/train/greater_swiss_mountain_dog/n02107574_1817.jpg to images_split/train_10_percent/greater_swiss_mountain_dog/n02107574_1817.jpg\nimages_split/train_10_percent/appenzeller\nCopying: images_split/train/appenzeller/n02107908_2723.jpg to images_split/train_10_percent/appenzeller/n02107908_2723.jpg\nimages_split/train_10_percent/dingo\nCopying: images_split/train/dingo/n02115641_9977.jpg to images_split/train_10_percent/dingo/n02115641_9977.jpg\nimages_split/train_10_percent/toy_terrier\nCopying: images_split/train/toy_terrier/n02087046_2551.jpg to images_split/train_10_percent/toy_terrier/n02087046_2551.jpg\nimages_split/train_10_percent/keeshond\nCopying: images_split/train/keeshond/n02112350_8391.jpg to images_split/train_10_percent/keeshond/n02112350_8391.jpg\nimages_split/train_10_percent/affenpinscher\nCopying: images_split/train/affenpinscher/n02110627_10986.jpg to images_split/train_10_percent/affenpinscher/n02110627_10986.jpg\nimages_split/train_10_percent/briard\nCopying: images_split/train/briard/n02105251_6376.jpg to images_split/train_10_percent/briard/n02105251_6376.jpg\nimages_split/train_10_percent/welsh_springer_spaniel\nCopying: images_split/train/welsh_springer_spaniel/n02102177_3947.jpg to images_split/train_10_percent/welsh_springer_spaniel/n02102177_3947.jpg\nimages_split/train_10_percent/redbone\nCopying: images_split/train/redbone/n02090379_3109.jpg to images_split/train_10_percent/redbone/n02090379_3109.jpg\nimages_split/train_10_percent/irish_terrier\nMaking directory: images_split/train_10_percent/irish_terrier\nCopying: images_split/train/irish_terrier/n02093991_4589.jpg to images_split/train_10_percent/irish_terrier/n02093991_4589.jpg\nimages_split/train_10_percent/airedale\nCopying: images_split/train/airedale/n02096051_9391.jpg to images_split/train_10_percent/airedale/n02096051_9391.jpg\nimages_split/train_10_percent/japanese_spaniel\nCopying: images_split/train/japanese_spaniel/n02085782_4772.jpg to images_split/train_10_percent/japanese_spaniel/n02085782_4772.jpg\nimages_split/train_10_percent/bull_mastiff\nCopying: images_split/train/bull_mastiff/n02108422_4898.jpg to images_split/train_10_percent/bull_mastiff/n02108422_4898.jpg\nimages_split/train_10_percent/giant_schnauzer\nCopying: images_split/train/giant_schnauzer/n02097130_4740.jpg to images_split/train_10_percent/giant_schnauzer/n02097130_4740.jpg\nimages_split/train_10_percent/irish_wolfhound\nCopying: images_split/train/irish_wolfhound/n02090721_1222.jpg to images_split/train_10_percent/irish_wolfhound/n02090721_1222.jpg\nimages_split/train_10_percent/chihuahua\nCopying: images_split/train/chihuahua/n02085620_7.jpg to images_split/train_10_percent/chihuahua/n02085620_7.jpg\nimages_split/train_10_percent/chesapeake_bay_retriever\nMaking directory: images_split/train_10_percent/chesapeake_bay_retriever\nCopying: images_split/train/chesapeake_bay_retriever/n02099849_4457.jpg to images_split/train_10_percent/chesapeake_bay_retriever/n02099849_4457.jpg\nimages_split/train_10_percent/siberian_husky\nCopying: images_split/train/siberian_husky/n02110185_8216.jpg to images_split/train_10_percent/siberian_husky/n02110185_8216.jpg\nimages_split/train_10_percent/rhodesian_ridgeback\nMaking directory: images_split/train_10_percent/rhodesian_ridgeback\nCopying: images_split/train/rhodesian_ridgeback/n02087394_760.jpg to images_split/train_10_percent/rhodesian_ridgeback/n02087394_760.jpg\nimages_split/train_10_percent/doberman\nMaking directory: images_split/train_10_percent/doberman\nCopying: images_split/train/doberman/n02107142_8834.jpg to images_split/train_10_percent/doberman/n02107142_8834.jpg\nimages_split/train_10_percent/lakeland_terrier\nCopying: images_split/train/lakeland_terrier/n02095570_1752.jpg to images_split/train_10_percent/lakeland_terrier/n02095570_1752.jpg\nimages_split/train_10_percent/silky_terrier\nCopying: images_split/train/silky_terrier/n02097658_2182.jpg to images_split/train_10_percent/silky_terrier/n02097658_2182.jpg\nimages_split/train_10_percent/siberian_husky\nCopying: images_split/train/siberian_husky/n02110185_10902.jpg to images_split/train_10_percent/siberian_husky/n02110185_10902.jpg\nimages_split/train_10_percent/basenji\nCopying: images_split/train/basenji/n02110806_1396.jpg to images_split/train_10_percent/basenji/n02110806_1396.jpg\nimages_split/train_10_percent/cocker_spaniel\nCopying: images_split/train/cocker_spaniel/n02102318_10020.jpg to images_split/train_10_percent/cocker_spaniel/n02102318_10020.jpg\nimages_split/train_10_percent/old_english_sheepdog\nCopying: images_split/train/old_english_sheepdog/n02105641_5998.jpg to images_split/train_10_percent/old_english_sheepdog/n02105641_5998.jpg\nimages_split/train_10_percent/bluetick\nCopying: images_split/train/bluetick/n02088632_2965.jpg to images_split/train_10_percent/bluetick/n02088632_2965.jpg\nimages_split/train_10_percent/sealyham_terrier\nCopying: images_split/train/sealyham_terrier/n02095889_5528.jpg to images_split/train_10_percent/sealyham_terrier/n02095889_5528.jpg\nimages_split/train_10_percent/kerry_blue_terrier\nMaking directory: images_split/train_10_percent/kerry_blue_terrier\nCopying: images_split/train/kerry_blue_terrier/n02093859_2055.jpg to images_split/train_10_percent/kerry_blue_terrier/n02093859_2055.jpg\nimages_split/train_10_percent/italian_greyhound\nCopying: images_split/train/italian_greyhound/n02091032_5.jpg to images_split/train_10_percent/italian_greyhound/n02091032_5.jpg\nimages_split/train_10_percent/siberian_husky\nCopying: images_split/train/siberian_husky/n02110185_4694.jpg to images_split/train_10_percent/siberian_husky/n02110185_4694.jpg\nimages_split/train_10_percent/greater_swiss_mountain_dog\nCopying: images_split/train/greater_swiss_mountain_dog/n02107574_3113.jpg to images_split/train_10_percent/greater_swiss_mountain_dog/n02107574_3113.jpg\nimages_split/train_10_percent/saluki\nCopying: images_split/train/saluki/n02091831_1730.jpg to images_split/train_10_percent/saluki/n02091831_1730.jpg\nimages_split/train_10_percent/pomeranian\nCopying: images_split/train/pomeranian/n02112018_14394.jpg to images_split/train_10_percent/pomeranian/n02112018_14394.jpg\nimages_split/train_10_percent/airedale\nCopying: images_split/train/airedale/n02096051_2432.jpg to images_split/train_10_percent/airedale/n02096051_2432.jpg\nimages_split/train_10_percent/chihuahua\nCopying: images_split/train/chihuahua/n02085620_1916.jpg to images_split/train_10_percent/chihuahua/n02085620_1916.jpg\nimages_split/train_10_percent/scottish_deerhound\nCopying: images_split/train/scottish_deerhound/n02092002_4230.jpg to images_split/train_10_percent/scottish_deerhound/n02092002_4230.jpg\nimages_split/train_10_percent/golden_retriever\nCopying: images_split/train/golden_retriever/n02099601_176.jpg to images_split/train_10_percent/golden_retriever/n02099601_176.jpg\nimages_split/train_10_percent/collie\nCopying: images_split/train/collie/n02106030_16095.jpg to images_split/train_10_percent/collie/n02106030_16095.jpg\nimages_split/train_10_percent/kuvasz\nCopying: images_split/train/kuvasz/n02104029_3900.jpg to images_split/train_10_percent/kuvasz/n02104029_3900.jpg\nimages_split/train_10_percent/border_collie\nCopying: images_split/train/border_collie/n02106166_3416.jpg to images_split/train_10_percent/border_collie/n02106166_3416.jpg\nimages_split/train_10_percent/bluetick\nCopying: images_split/train/bluetick/n02088632_4613.jpg to images_split/train_10_percent/bluetick/n02088632_4613.jpg\nimages_split/train_10_percent/groenendael\nMaking directory: images_split/train_10_percent/groenendael\nCopying: images_split/train/groenendael/n02105056_2194.jpg to images_split/train_10_percent/groenendael/n02105056_2194.jpg\nimages_split/train_10_percent/greater_swiss_mountain_dog\nCopying: images_split/train/greater_swiss_mountain_dog/n02107574_2283.jpg to images_split/train_10_percent/greater_swiss_mountain_dog/n02107574_2283.jpg\nimages_split/train_10_percent/briard\nCopying: images_split/train/briard/n02105251_8560.jpg to images_split/train_10_percent/briard/n02105251_8560.jpg\nimages_split/train_10_percent/norwegian_elkhound\nCopying: images_split/train/norwegian_elkhound/n02091467_7351.jpg to images_split/train_10_percent/norwegian_elkhound/n02091467_7351.jpg\nimages_split/train_10_percent/chow\nCopying: images_split/train/chow/n02112137_9591.jpg to images_split/train_10_percent/chow/n02112137_9591.jpg\nimages_split/train_10_percent/borzoi\nCopying: images_split/train/borzoi/n02090622_6713.jpg to images_split/train_10_percent/borzoi/n02090622_6713.jpg\nimages_split/train_10_percent/leonberg\nCopying: images_split/train/leonberg/n02111129_306.jpg to images_split/train_10_percent/leonberg/n02111129_306.jpg\nimages_split/train_10_percent/miniature_schnauzer\nCopying: images_split/train/miniature_schnauzer/n02097047_1418.jpg to images_split/train_10_percent/miniature_schnauzer/n02097047_1418.jpg\nimages_split/train_10_percent/gordon_setter\nCopying: images_split/train/gordon_setter/n02101006_1506.jpg to images_split/train_10_percent/gordon_setter/n02101006_1506.jpg\nimages_split/train_10_percent/kerry_blue_terrier\nCopying: images_split/train/kerry_blue_terrier/n02093859_276.jpg to images_split/train_10_percent/kerry_blue_terrier/n02093859_276.jpg\nimages_split/train_10_percent/scotch_terrier\nMaking directory: images_split/train_10_percent/scotch_terrier\nCopying: images_split/train/scotch_terrier/n02097298_6383.jpg to images_split/train_10_percent/scotch_terrier/n02097298_6383.jpg\nimages_split/train_10_percent/giant_schnauzer\nCopying: images_split/train/giant_schnauzer/n02097130_5708.jpg to images_split/train_10_percent/giant_schnauzer/n02097130_5708.jpg\nimages_split/train_10_percent/basset\nCopying: images_split/train/basset/n02088238_13608.jpg to images_split/train_10_percent/basset/n02088238_13608.jpg\nimages_split/train_10_percent/saluki\nCopying: images_split/train/saluki/n02091831_1594.jpg to images_split/train_10_percent/saluki/n02091831_1594.jpg\nimages_split/train_10_percent/lakeland_terrier\nCopying: images_split/train/lakeland_terrier/n02095570_4650.jpg to images_split/train_10_percent/lakeland_terrier/n02095570_4650.jpg\nimages_split/train_10_percent/boxer\nCopying: images_split/train/boxer/n02108089_836.jpg to images_split/train_10_percent/boxer/n02108089_836.jpg\nimages_split/train_10_percent/kerry_blue_terrier\nCopying: images_split/train/kerry_blue_terrier/n02093859_1598.jpg to images_split/train_10_percent/kerry_blue_terrier/n02093859_1598.jpg\nimages_split/train_10_percent/west_highland_white_terrier\nCopying: images_split/train/west_highland_white_terrier/n02098286_5829.jpg to images_split/train_10_percent/west_highland_white_terrier/n02098286_5829.jpg\nimages_split/train_10_percent/scottish_deerhound\nCopying: images_split/train/scottish_deerhound/n02092002_6735.jpg to images_split/train_10_percent/scottish_deerhound/n02092002_6735.jpg\nimages_split/train_10_percent/malinois\nCopying: images_split/train/malinois/n02105162_5330.jpg to images_split/train_10_percent/malinois/n02105162_5330.jpg\nimages_split/train_10_percent/komondor\nCopying: images_split/train/komondor/n02105505_3558.jpg to images_split/train_10_percent/komondor/n02105505_3558.jpg\nimages_split/train_10_percent/bedlington_terrier\nCopying: images_split/train/bedlington_terrier/n02093647_1980.jpg to images_split/train_10_percent/bedlington_terrier/n02093647_1980.jpg\nimages_split/train_10_percent/borzoi\nCopying: images_split/train/borzoi/n02090622_7677.jpg to images_split/train_10_percent/borzoi/n02090622_7677.jpg\nimages_split/train_10_percent/pekinese\nCopying: images_split/train/pekinese/n02086079_5125.jpg to images_split/train_10_percent/pekinese/n02086079_5125.jpg\nimages_split/train_10_percent/chow\nCopying: images_split/train/chow/n02112137_13499.jpg to images_split/train_10_percent/chow/n02112137_13499.jpg\nimages_split/train_10_percent/toy_poodle\nCopying: images_split/train/toy_poodle/n02113624_1293.jpg to images_split/train_10_percent/toy_poodle/n02113624_1293.jpg\nimages_split/train_10_percent/kuvasz\nCopying: images_split/train/kuvasz/n02104029_371.jpg to images_split/train_10_percent/kuvasz/n02104029_371.jpg\nimages_split/train_10_percent/silky_terrier\nCopying: images_split/train/silky_terrier/n02097658_329.jpg to images_split/train_10_percent/silky_terrier/n02097658_329.jpg\nimages_split/train_10_percent/german_short_haired_pointer\nCopying: images_split/train/german_short_haired_pointer/n02100236_396.jpg to images_split/train_10_percent/german_short_haired_pointer/n02100236_396.jpg\nimages_split/train_10_percent/doberman\nCopying: images_split/train/doberman/n02107142_4255.jpg to images_split/train_10_percent/doberman/n02107142_4255.jpg\nimages_split/train_10_percent/tibetan_terrier\nCopying: images_split/train/tibetan_terrier/n02097474_2553.jpg to images_split/train_10_percent/tibetan_terrier/n02097474_2553.jpg\nimages_split/train_10_percent/beagle\nCopying: images_split/train/beagle/n02088364_11930.jpg to images_split/train_10_percent/beagle/n02088364_11930.jpg\nimages_split/train_10_percent/cocker_spaniel\nCopying: images_split/train/cocker_spaniel/n02102318_8518.jpg to images_split/train_10_percent/cocker_spaniel/n02102318_8518.jpg\nimages_split/train_10_percent/border_collie\nCopying: images_split/train/border_collie/n02106166_4966.jpg to images_split/train_10_percent/border_collie/n02106166_4966.jpg\nimages_split/train_10_percent/english_setter\nCopying: images_split/train/english_setter/n02100735_4870.jpg to images_split/train_10_percent/english_setter/n02100735_4870.jpg\nimages_split/train_10_percent/kelpie\nCopying: images_split/train/kelpie/n02105412_2333.jpg to images_split/train_10_percent/kelpie/n02105412_2333.jpg\nimages_split/train_10_percent/labrador_retriever\nCopying: images_split/train/labrador_retriever/n02099712_1436.jpg to images_split/train_10_percent/labrador_retriever/n02099712_1436.jpg\nimages_split/train_10_percent/blenheim_spaniel\nCopying: images_split/train/blenheim_spaniel/n02086646_3629.jpg to images_split/train_10_percent/blenheim_spaniel/n02086646_3629.jpg\nimages_split/train_10_percent/beagle\nCopying: images_split/train/beagle/n02088364_14095.jpg to images_split/train_10_percent/beagle/n02088364_14095.jpg\nimages_split/train_10_percent/afghan_hound\nCopying: images_split/train/afghan_hound/n02088094_1335.jpg to images_split/train_10_percent/afghan_hound/n02088094_1335.jpg\nimages_split/train_10_percent/bloodhound\nCopying: images_split/train/bloodhound/n02088466_10724.jpg to images_split/train_10_percent/bloodhound/n02088466_10724.jpg\nimages_split/train_10_percent/black_and_tan_coonhound\nCopying: images_split/train/black_and_tan_coonhound/n02089078_1842.jpg to images_split/train_10_percent/black_and_tan_coonhound/n02089078_1842.jpg\nimages_split/train_10_percent/yorkshire_terrier\nCopying: images_split/train/yorkshire_terrier/n02094433_795.jpg to images_split/train_10_percent/yorkshire_terrier/n02094433_795.jpg\nimages_split/train_10_percent/bluetick\nCopying: images_split/train/bluetick/n02088632_87.jpg to images_split/train_10_percent/bluetick/n02088632_87.jpg\nimages_split/train_10_percent/clumber\nCopying: images_split/train/clumber/n02101556_2003.jpg to images_split/train_10_percent/clumber/n02101556_2003.jpg\nimages_split/train_10_percent/rhodesian_ridgeback\nCopying: images_split/train/rhodesian_ridgeback/n02087394_8260.jpg to images_split/train_10_percent/rhodesian_ridgeback/n02087394_8260.jpg\nimages_split/train_10_percent/papillon\nCopying: images_split/train/papillon/n02086910_881.jpg to images_split/train_10_percent/papillon/n02086910_881.jpg\nimages_split/train_10_percent/entlebucher\nCopying: images_split/train/entlebucher/n02108000_3104.jpg to images_split/train_10_percent/entlebucher/n02108000_3104.jpg\nimages_split/train_10_percent/boxer\nCopying: images_split/train/boxer/n02108089_2106.jpg to images_split/train_10_percent/boxer/n02108089_2106.jpg\nimages_split/train_10_percent/gordon_setter\nCopying: images_split/train/gordon_setter/n02101006_1160.jpg to images_split/train_10_percent/gordon_setter/n02101006_1160.jpg\nimages_split/train_10_percent/samoyed\nCopying: images_split/train/samoyed/n02111889_16414.jpg to images_split/train_10_percent/samoyed/n02111889_16414.jpg\nimages_split/train_10_percent/african_hunting_dog\nCopying: images_split/train/african_hunting_dog/n02116738_4019.jpg to images_split/train_10_percent/african_hunting_dog/n02116738_4019.jpg\nimages_split/train_10_percent/saluki\nCopying: images_split/train/saluki/n02091831_13774.jpg to images_split/train_10_percent/saluki/n02091831_13774.jpg\nimages_split/train_10_percent/golden_retriever\nCopying: images_split/train/golden_retriever/n02099601_816.jpg to images_split/train_10_percent/golden_retriever/n02099601_816.jpg\nimages_split/train_10_percent/dhole\nCopying: images_split/train/dhole/n02115913_1578.jpg to images_split/train_10_percent/dhole/n02115913_1578.jpg\nimages_split/train_10_percent/pug\nMaking directory: images_split/train_10_percent/pug\nCopying: images_split/train/pug/n02110958_16217.jpg to images_split/train_10_percent/pug/n02110958_16217.jpg\nimages_split/train_10_percent/miniature_schnauzer\nCopying: images_split/train/miniature_schnauzer/n02097047_6701.jpg to images_split/train_10_percent/miniature_schnauzer/n02097047_6701.jpg\nimages_split/train_10_percent/maltese_dog\nCopying: images_split/train/maltese_dog/n02085936_7394.jpg to images_split/train_10_percent/maltese_dog/n02085936_7394.jpg\nimages_split/train_10_percent/norwegian_elkhound\nCopying: images_split/train/norwegian_elkhound/n02091467_4827.jpg to images_split/train_10_percent/norwegian_elkhound/n02091467_4827.jpg\nimages_split/train_10_percent/irish_wolfhound\nCopying: images_split/train/irish_wolfhound/n02090721_1131.jpg to images_split/train_10_percent/irish_wolfhound/n02090721_1131.jpg\nimages_split/train_10_percent/boxer\nCopying: images_split/train/boxer/n02108089_4730.jpg to images_split/train_10_percent/boxer/n02108089_4730.jpg\nimages_split/train_10_percent/pug\nCopying: images_split/train/pug/n02110958_13930.jpg to images_split/train_10_percent/pug/n02110958_13930.jpg\nimages_split/train_10_percent/samoyed\nCopying: images_split/train/samoyed/n02111889_373.jpg to images_split/train_10_percent/samoyed/n02111889_373.jpg\nimages_split/train_10_percent/miniature_pinscher\nCopying: images_split/train/miniature_pinscher/n02107312_5077.jpg to images_split/train_10_percent/miniature_pinscher/n02107312_5077.jpg\nimages_split/train_10_percent/irish_terrier\nCopying: images_split/train/irish_terrier/n02093991_4872.jpg to images_split/train_10_percent/irish_terrier/n02093991_4872.jpg\nimages_split/train_10_percent/wire_haired_fox_terrier\nCopying: images_split/train/wire_haired_fox_terrier/n02095314_2964.jpg to images_split/train_10_percent/wire_haired_fox_terrier/n02095314_2964.jpg\nimages_split/train_10_percent/german_shepherd\nCopying: images_split/train/german_shepherd/n02106662_6931.jpg to images_split/train_10_percent/german_shepherd/n02106662_6931.jpg\nimages_split/train_10_percent/irish_terrier\nCopying: images_split/train/irish_terrier/n02093991_1142.jpg to images_split/train_10_percent/irish_terrier/n02093991_1142.jpg\nimages_split/train_10_percent/walker_hound\nCopying: images_split/train/walker_hound/n02089867_1368.jpg to images_split/train_10_percent/walker_hound/n02089867_1368.jpg\nimages_split/train_10_percent/kuvasz\nCopying: images_split/train/kuvasz/n02104029_1816.jpg to images_split/train_10_percent/kuvasz/n02104029_1816.jpg\nimages_split/train_10_percent/tibetan_mastiff\nCopying: images_split/train/tibetan_mastiff/n02108551_340.jpg to images_split/train_10_percent/tibetan_mastiff/n02108551_340.jpg\nimages_split/train_10_percent/briard\nCopying: images_split/train/briard/n02105251_6300.jpg to images_split/train_10_percent/briard/n02105251_6300.jpg\nimages_split/train_10_percent/lakeland_terrier\nCopying: images_split/train/lakeland_terrier/n02095570_3777.jpg to images_split/train_10_percent/lakeland_terrier/n02095570_3777.jpg\nimages_split/train_10_percent/scottish_deerhound\nCopying: images_split/train/scottish_deerhound/n02092002_4218.jpg to images_split/train_10_percent/scottish_deerhound/n02092002_4218.jpg\nimages_split/train_10_percent/groenendael\nCopying: images_split/train/groenendael/n02105056_3107.jpg to images_split/train_10_percent/groenendael/n02105056_3107.jpg\nimages_split/train_10_percent/african_hunting_dog\nCopying: images_split/train/african_hunting_dog/n02116738_5953.jpg to images_split/train_10_percent/african_hunting_dog/n02116738_5953.jpg\nimages_split/train_10_percent/airedale\nCopying: images_split/train/airedale/n02096051_183.jpg to images_split/train_10_percent/airedale/n02096051_183.jpg\nimages_split/train_10_percent/borzoi\nCopying: images_split/train/borzoi/n02090622_2337.jpg to images_split/train_10_percent/borzoi/n02090622_2337.jpg\nimages_split/train_10_percent/dingo\nCopying: images_split/train/dingo/n02115641_925.jpg to images_split/train_10_percent/dingo/n02115641_925.jpg\nimages_split/train_10_percent/papillon\nCopying: images_split/train/papillon/n02086910_3455.jpg to images_split/train_10_percent/papillon/n02086910_3455.jpg\nimages_split/train_10_percent/cardigan\nCopying: images_split/train/cardigan/n02113186_7676.jpg to images_split/train_10_percent/cardigan/n02113186_7676.jpg\nimages_split/train_10_percent/boston_bull\nCopying: images_split/train/boston_bull/n02096585_1753.jpg to images_split/train_10_percent/boston_bull/n02096585_1753.jpg\nimages_split/train_10_percent/newfoundland\nCopying: images_split/train/newfoundland/n02111277_1470.jpg to images_split/train_10_percent/newfoundland/n02111277_1470.jpg\nimages_split/train_10_percent/cardigan\nCopying: images_split/train/cardigan/n02113186_7215.jpg to images_split/train_10_percent/cardigan/n02113186_7215.jpg\nimages_split/train_10_percent/yorkshire_terrier\nCopying: images_split/train/yorkshire_terrier/n02094433_745.jpg to images_split/train_10_percent/yorkshire_terrier/n02094433_745.jpg\nimages_split/train_10_percent/boston_bull\nCopying: images_split/train/boston_bull/n02096585_942.jpg to images_split/train_10_percent/boston_bull/n02096585_942.jpg\nimages_split/train_10_percent/lakeland_terrier\nCopying: images_split/train/lakeland_terrier/n02095570_4436.jpg to images_split/train_10_percent/lakeland_terrier/n02095570_4436.jpg\nimages_split/train_10_percent/briard\nCopying: images_split/train/briard/n02105251_8854.jpg to images_split/train_10_percent/briard/n02105251_8854.jpg\nimages_split/train_10_percent/papillon\nCopying: images_split/train/papillon/n02086910_1449.jpg to images_split/train_10_percent/papillon/n02086910_1449.jpg\nimages_split/train_10_percent/english_foxhound\nCopying: images_split/train/english_foxhound/n02089973_382.jpg to images_split/train_10_percent/english_foxhound/n02089973_382.jpg\nimages_split/train_10_percent/bedlington_terrier\nCopying: images_split/train/bedlington_terrier/n02093647_680.jpg to images_split/train_10_percent/bedlington_terrier/n02093647_680.jpg\nimages_split/train_10_percent/whippet\nCopying: images_split/train/whippet/n02091134_2339.jpg to images_split/train_10_percent/whippet/n02091134_2339.jpg\nimages_split/train_10_percent/irish_terrier\nCopying: images_split/train/irish_terrier/n02093991_3403.jpg to images_split/train_10_percent/irish_terrier/n02093991_3403.jpg\nimages_split/train_10_percent/lhasa\nCopying: images_split/train/lhasa/n02098413_20774.jpg to images_split/train_10_percent/lhasa/n02098413_20774.jpg\nimages_split/train_10_percent/shetland_sheepdog\nCopying: images_split/train/shetland_sheepdog/n02105855_2039.jpg to images_split/train_10_percent/shetland_sheepdog/n02105855_2039.jpg\nimages_split/train_10_percent/english_springer\nCopying: images_split/train/english_springer/n02102040_7596.jpg to images_split/train_10_percent/english_springer/n02102040_7596.jpg\nimages_split/train_10_percent/pomeranian\nCopying: images_split/train/pomeranian/n02112018_3504.jpg to images_split/train_10_percent/pomeranian/n02112018_3504.jpg\nimages_split/train_10_percent/groenendael\nCopying: images_split/train/groenendael/n02105056_6474.jpg to images_split/train_10_percent/groenendael/n02105056_6474.jpg\nimages_split/train_10_percent/wire_haired_fox_terrier\nCopying: images_split/train/wire_haired_fox_terrier/n02095314_663.jpg to images_split/train_10_percent/wire_haired_fox_terrier/n02095314_663.jpg\nimages_split/train_10_percent/border_terrier\nCopying: images_split/train/border_terrier/n02093754_1722.jpg to images_split/train_10_percent/border_terrier/n02093754_1722.jpg\nimages_split/train_10_percent/shetland_sheepdog\nCopying: images_split/train/shetland_sheepdog/n02105855_1963.jpg to images_split/train_10_percent/shetland_sheepdog/n02105855_1963.jpg\nimages_split/train_10_percent/soft_coated_wheaten_terrier\nCopying: images_split/train/soft_coated_wheaten_terrier/n02098105_2897.jpg to images_split/train_10_percent/soft_coated_wheaten_terrier/n02098105_2897.jpg\nimages_split/train_10_percent/west_highland_white_terrier\nCopying: images_split/train/west_highland_white_terrier/n02098286_437.jpg to images_split/train_10_percent/west_highland_white_terrier/n02098286_437.jpg\nimages_split/train_10_percent/norwich_terrier\nCopying: images_split/train/norwich_terrier/n02094258_3036.jpg to images_split/train_10_percent/norwich_terrier/n02094258_3036.jpg\nimages_split/train_10_percent/standard_poodle\nCopying: images_split/train/standard_poodle/n02113799_6634.jpg to images_split/train_10_percent/standard_poodle/n02113799_6634.jpg\nimages_split/train_10_percent/irish_water_spaniel\nCopying: images_split/train/irish_water_spaniel/n02102973_3326.jpg to images_split/train_10_percent/irish_water_spaniel/n02102973_3326.jpg\nimages_split/train_10_percent/afghan_hound\nCopying: images_split/train/afghan_hound/n02088094_7683.jpg to images_split/train_10_percent/afghan_hound/n02088094_7683.jpg\nimages_split/train_10_percent/greater_swiss_mountain_dog\nCopying: images_split/train/greater_swiss_mountain_dog/n02107574_835.jpg to images_split/train_10_percent/greater_swiss_mountain_dog/n02107574_835.jpg\nimages_split/train_10_percent/afghan_hound\nCopying: images_split/train/afghan_hound/n02088094_4598.jpg to images_split/train_10_percent/afghan_hound/n02088094_4598.jpg\nimages_split/train_10_percent/groenendael\nCopying: images_split/train/groenendael/n02105056_4640.jpg to images_split/train_10_percent/groenendael/n02105056_4640.jpg\nimages_split/train_10_percent/sealyham_terrier\nCopying: images_split/train/sealyham_terrier/n02095889_2927.jpg to images_split/train_10_percent/sealyham_terrier/n02095889_2927.jpg\nimages_split/train_10_percent/sussex_spaniel\nCopying: images_split/train/sussex_spaniel/n02102480_3762.jpg to images_split/train_10_percent/sussex_spaniel/n02102480_3762.jpg\nimages_split/train_10_percent/standard_poodle\nCopying: images_split/train/standard_poodle/n02113799_253.jpg to images_split/train_10_percent/standard_poodle/n02113799_253.jpg\nimages_split/train_10_percent/tibetan_mastiff\nCopying: images_split/train/tibetan_mastiff/n02108551_4379.jpg to images_split/train_10_percent/tibetan_mastiff/n02108551_4379.jpg\nimages_split/train_10_percent/yorkshire_terrier\nCopying: images_split/train/yorkshire_terrier/n02094433_1765.jpg to images_split/train_10_percent/yorkshire_terrier/n02094433_1765.jpg\nimages_split/train_10_percent/newfoundland\nCopying: images_split/train/newfoundland/n02111277_3990.jpg to images_split/train_10_percent/newfoundland/n02111277_3990.jpg\nimages_split/train_10_percent/komondor\nCopying: images_split/train/komondor/n02105505_4069.jpg to images_split/train_10_percent/komondor/n02105505_4069.jpg\nimages_split/train_10_percent/rhodesian_ridgeback\nCopying: images_split/train/rhodesian_ridgeback/n02087394_3458.jpg to images_split/train_10_percent/rhodesian_ridgeback/n02087394_3458.jpg\nimages_split/train_10_percent/cardigan\nCopying: images_split/train/cardigan/n02113186_9809.jpg to images_split/train_10_percent/cardigan/n02113186_9809.jpg\nimages_split/train_10_percent/irish_wolfhound\nCopying: images_split/train/irish_wolfhound/n02090721_2319.jpg to images_split/train_10_percent/irish_wolfhound/n02090721_2319.jpg\nimages_split/train_10_percent/cocker_spaniel\nCopying: images_split/train/cocker_spaniel/n02102318_10179.jpg to images_split/train_10_percent/cocker_spaniel/n02102318_10179.jpg\nimages_split/train_10_percent/irish_terrier\nCopying: images_split/train/irish_terrier/n02093991_3807.jpg to images_split/train_10_percent/irish_terrier/n02093991_3807.jpg\nimages_split/train_10_percent/bedlington_terrier\nCopying: images_split/train/bedlington_terrier/n02093647_518.jpg to images_split/train_10_percent/bedlington_terrier/n02093647_518.jpg\nimages_split/train_10_percent/redbone\nCopying: images_split/train/redbone/n02090379_3300.jpg to images_split/train_10_percent/redbone/n02090379_3300.jpg\nimages_split/train_10_percent/leonberg\nCopying: images_split/train/leonberg/n02111129_1583.jpg to images_split/train_10_percent/leonberg/n02111129_1583.jpg\nimages_split/train_10_percent/german_short_haired_pointer\nCopying: images_split/train/german_short_haired_pointer/n02100236_3877.jpg to images_split/train_10_percent/german_short_haired_pointer/n02100236_3877.jpg\nimages_split/train_10_percent/bouvier_des_flandres\nCopying: images_split/train/bouvier_des_flandres/n02106382_4504.jpg to images_split/train_10_percent/bouvier_des_flandres/n02106382_4504.jpg\nimages_split/train_10_percent/siberian_husky\nCopying: images_split/train/siberian_husky/n02110185_10171.jpg to images_split/train_10_percent/siberian_husky/n02110185_10171.jpg\nimages_split/train_10_percent/leonberg\nCopying: images_split/train/leonberg/n02111129_513.jpg to images_split/train_10_percent/leonberg/n02111129_513.jpg\nimages_split/train_10_percent/toy_poodle\nCopying: images_split/train/toy_poodle/n02113624_2308.jpg to images_split/train_10_percent/toy_poodle/n02113624_2308.jpg\nimages_split/train_10_percent/sussex_spaniel\nCopying: images_split/train/sussex_spaniel/n02102480_5805.jpg to images_split/train_10_percent/sussex_spaniel/n02102480_5805.jpg\nimages_split/train_10_percent/komondor\nCopying: images_split/train/komondor/n02105505_3898.jpg to images_split/train_10_percent/komondor/n02105505_3898.jpg\nimages_split/train_10_percent/schipperke\nCopying: images_split/train/schipperke/n02104365_5740.jpg to images_split/train_10_percent/schipperke/n02104365_5740.jpg\nimages_split/train_10_percent/welsh_springer_spaniel\nCopying: images_split/train/welsh_springer_spaniel/n02102177_1128.jpg to images_split/train_10_percent/welsh_springer_spaniel/n02102177_1128.jpg\nimages_split/train_10_percent/norwich_terrier\nCopying: images_split/train/norwich_terrier/n02094258_969.jpg to images_split/train_10_percent/norwich_terrier/n02094258_969.jpg\nimages_split/train_10_percent/kelpie\nCopying: images_split/train/kelpie/n02105412_5395.jpg to images_split/train_10_percent/kelpie/n02105412_5395.jpg\nimages_split/train_10_percent/bloodhound\nCopying: images_split/train/bloodhound/n02088466_1015.jpg to images_split/train_10_percent/bloodhound/n02088466_1015.jpg\nimages_split/train_10_percent/malamute\nCopying: images_split/train/malamute/n02110063_12269.jpg to images_split/train_10_percent/malamute/n02110063_12269.jpg\nimages_split/train_10_percent/leonberg\nCopying: images_split/train/leonberg/n02111129_2301.jpg to images_split/train_10_percent/leonberg/n02111129_2301.jpg\nimages_split/train_10_percent/black_and_tan_coonhound\nCopying: images_split/train/black_and_tan_coonhound/n02089078_965.jpg to images_split/train_10_percent/black_and_tan_coonhound/n02089078_965.jpg\nimages_split/train_10_percent/chesapeake_bay_retriever\nCopying: images_split/train/chesapeake_bay_retriever/n02099849_1645.jpg to images_split/train_10_percent/chesapeake_bay_retriever/n02099849_1645.jpg\nimages_split/train_10_percent/collie\nCopying: images_split/train/collie/n02106030_1005.jpg to images_split/train_10_percent/collie/n02106030_1005.jpg\nimages_split/train_10_percent/tibetan_terrier\nCopying: images_split/train/tibetan_terrier/n02097474_4907.jpg to images_split/train_10_percent/tibetan_terrier/n02097474_4907.jpg\nimages_split/train_10_percent/silky_terrier\nCopying: images_split/train/silky_terrier/n02097658_6202.jpg to images_split/train_10_percent/silky_terrier/n02097658_6202.jpg\nimages_split/train_10_percent/lakeland_terrier\nCopying: images_split/train/lakeland_terrier/n02095570_4511.jpg to images_split/train_10_percent/lakeland_terrier/n02095570_4511.jpg\nimages_split/train_10_percent/english_springer\nCopying: images_split/train/english_springer/n02102040_988.jpg to images_split/train_10_percent/english_springer/n02102040_988.jpg\nimages_split/train_10_percent/german_shepherd\nCopying: images_split/train/german_shepherd/n02106662_5169.jpg to images_split/train_10_percent/german_shepherd/n02106662_5169.jpg\nimages_split/train_10_percent/samoyed\nCopying: images_split/train/samoyed/n02111889_2476.jpg to images_split/train_10_percent/samoyed/n02111889_2476.jpg\nimages_split/train_10_percent/greater_swiss_mountain_dog\nCopying: images_split/train/greater_swiss_mountain_dog/n02107574_1669.jpg to images_split/train_10_percent/greater_swiss_mountain_dog/n02107574_1669.jpg\nimages_split/train_10_percent/west_highland_white_terrier\nCopying: images_split/train/west_highland_white_terrier/n02098286_3480.jpg to images_split/train_10_percent/west_highland_white_terrier/n02098286_3480.jpg\nimages_split/train_10_percent/yorkshire_terrier\nCopying: images_split/train/yorkshire_terrier/n02094433_4990.jpg to images_split/train_10_percent/yorkshire_terrier/n02094433_4990.jpg\nimages_split/train_10_percent/shih_tzu\nCopying: images_split/train/shih_tzu/n02086240_30.jpg to images_split/train_10_percent/shih_tzu/n02086240_30.jpg\nimages_split/train_10_percent/siberian_husky\nCopying: images_split/train/siberian_husky/n02110185_11626.jpg to images_split/train_10_percent/siberian_husky/n02110185_11626.jpg\nimages_split/train_10_percent/miniature_pinscher\nCopying: images_split/train/miniature_pinscher/n02107312_612.jpg to images_split/train_10_percent/miniature_pinscher/n02107312_612.jpg\nimages_split/train_10_percent/silky_terrier\nCopying: images_split/train/silky_terrier/n02097658_2329.jpg to images_split/train_10_percent/silky_terrier/n02097658_2329.jpg\nimages_split/train_10_percent/kerry_blue_terrier\nCopying: images_split/train/kerry_blue_terrier/n02093859_739.jpg to images_split/train_10_percent/kerry_blue_terrier/n02093859_739.jpg\nimages_split/train_10_percent/sussex_spaniel\nCopying: images_split/train/sussex_spaniel/n02102480_4769.jpg to images_split/train_10_percent/sussex_spaniel/n02102480_4769.jpg\nimages_split/train_10_percent/wire_haired_fox_terrier\nCopying: images_split/train/wire_haired_fox_terrier/n02095314_3512.jpg to images_split/train_10_percent/wire_haired_fox_terrier/n02095314_3512.jpg\nimages_split/train_10_percent/lakeland_terrier\nCopying: images_split/train/lakeland_terrier/n02095570_5024.jpg to images_split/train_10_percent/lakeland_terrier/n02095570_5024.jpg\nimages_split/train_10_percent/toy_terrier\nCopying: images_split/train/toy_terrier/n02087046_7469.jpg to images_split/train_10_percent/toy_terrier/n02087046_7469.jpg\nimages_split/train_10_percent/irish_water_spaniel\nCopying: images_split/train/irish_water_spaniel/n02102973_2535.jpg to images_split/train_10_percent/irish_water_spaniel/n02102973_2535.jpg\nimages_split/train_10_percent/standard_poodle\nCopying: images_split/train/standard_poodle/n02113799_6727.jpg to images_split/train_10_percent/standard_poodle/n02113799_6727.jpg\nimages_split/train_10_percent/english_foxhound\nCopying: images_split/train/english_foxhound/n02089973_2404.jpg to images_split/train_10_percent/english_foxhound/n02089973_2404.jpg\nimages_split/train_10_percent/standard_schnauzer\nCopying: images_split/train/standard_schnauzer/n02097209_1653.jpg to images_split/train_10_percent/standard_schnauzer/n02097209_1653.jpg\nimages_split/train_10_percent/boxer\nCopying: images_split/train/boxer/n02108089_770.jpg to images_split/train_10_percent/boxer/n02108089_770.jpg\nimages_split/train_10_percent/miniature_pinscher\nCopying: images_split/train/miniature_pinscher/n02107312_811.jpg to images_split/train_10_percent/miniature_pinscher/n02107312_811.jpg\nimages_split/train_10_percent/norwich_terrier\nCopying: images_split/train/norwich_terrier/n02094258_792.jpg to images_split/train_10_percent/norwich_terrier/n02094258_792.jpg\nimages_split/train_10_percent/bull_mastiff\nCopying: images_split/train/bull_mastiff/n02108422_2297.jpg to images_split/train_10_percent/bull_mastiff/n02108422_2297.jpg\nimages_split/train_10_percent/west_highland_white_terrier\nCopying: images_split/train/west_highland_white_terrier/n02098286_2257.jpg to images_split/train_10_percent/west_highland_white_terrier/n02098286_2257.jpg\nimages_split/train_10_percent/whippet\nCopying: images_split/train/whippet/n02091134_19204.jpg to images_split/train_10_percent/whippet/n02091134_19204.jpg\nimages_split/train_10_percent/appenzeller\nCopying: images_split/train/appenzeller/n02107908_4079.jpg to images_split/train_10_percent/appenzeller/n02107908_4079.jpg\nimages_split/train_10_percent/american_staffordshire_terrier\nCopying: images_split/train/american_staffordshire_terrier/n02093428_15280.jpg to images_split/train_10_percent/american_staffordshire_terrier/n02093428_15280.jpg\nimages_split/train_10_percent/scotch_terrier\nCopying: images_split/train/scotch_terrier/n02097298_7631.jpg to images_split/train_10_percent/scotch_terrier/n02097298_7631.jpg\nimages_split/train_10_percent/affenpinscher\nCopying: images_split/train/affenpinscher/n02110627_3032.jpg to images_split/train_10_percent/affenpinscher/n02110627_3032.jpg\nimages_split/train_10_percent/kerry_blue_terrier\nCopying: images_split/train/kerry_blue_terrier/n02093859_2075.jpg to images_split/train_10_percent/kerry_blue_terrier/n02093859_2075.jpg\nimages_split/train_10_percent/toy_terrier\nCopying: images_split/train/toy_terrier/n02087046_1520.jpg to images_split/train_10_percent/toy_terrier/n02087046_1520.jpg\nimages_split/train_10_percent/miniature_schnauzer\nCopying: images_split/train/miniature_schnauzer/n02097047_6434.jpg to images_split/train_10_percent/miniature_schnauzer/n02097047_6434.jpg\nimages_split/train_10_percent/black_and_tan_coonhound\nCopying: images_split/train/black_and_tan_coonhound/n02089078_3078.jpg to images_split/train_10_percent/black_and_tan_coonhound/n02089078_3078.jpg\nimages_split/train_10_percent/kerry_blue_terrier\nCopying: images_split/train/kerry_blue_terrier/n02093859_1218.jpg to images_split/train_10_percent/kerry_blue_terrier/n02093859_1218.jpg\nimages_split/train_10_percent/eskimo_dog\nCopying: images_split/train/eskimo_dog/n02109961_12993.jpg to images_split/train_10_percent/eskimo_dog/n02109961_12993.jpg\nimages_split/train_10_percent/kuvasz\nCopying: images_split/train/kuvasz/n02104029_4581.jpg to images_split/train_10_percent/kuvasz/n02104029_4581.jpg\nimages_split/train_10_percent/samoyed\nCopying: images_split/train/samoyed/n02111889_6950.jpg to images_split/train_10_percent/samoyed/n02111889_6950.jpg\nimages_split/train_10_percent/irish_wolfhound\nCopying: images_split/train/irish_wolfhound/n02090721_1644.jpg to images_split/train_10_percent/irish_wolfhound/n02090721_1644.jpg\nimages_split/train_10_percent/toy_poodle\nCopying: images_split/train/toy_poodle/n02113624_8951.jpg to images_split/train_10_percent/toy_poodle/n02113624_8951.jpg\nimages_split/train_10_percent/giant_schnauzer\nCopying: images_split/train/giant_schnauzer/n02097130_2164.jpg to images_split/train_10_percent/giant_schnauzer/n02097130_2164.jpg\nimages_split/train_10_percent/dandie_dinmont\nMaking directory: images_split/train_10_percent/dandie_dinmont\nCopying: images_split/train/dandie_dinmont/n02096437_3349.jpg to images_split/train_10_percent/dandie_dinmont/n02096437_3349.jpg\nimages_split/train_10_percent/great_dane\nCopying: images_split/train/great_dane/n02109047_13016.jpg to images_split/train_10_percent/great_dane/n02109047_13016.jpg\nimages_split/train_10_percent/entlebucher\nCopying: images_split/train/entlebucher/n02108000_316.jpg to images_split/train_10_percent/entlebucher/n02108000_316.jpg\nimages_split/train_10_percent/entlebucher\nCopying: images_split/train/entlebucher/n02108000_2878.jpg to images_split/train_10_percent/entlebucher/n02108000_2878.jpg\nimages_split/train_10_percent/appenzeller\nCopying: images_split/train/appenzeller/n02107908_1575.jpg to images_split/train_10_percent/appenzeller/n02107908_1575.jpg\nimages_split/train_10_percent/weimaraner\nCopying: images_split/train/weimaraner/n02092339_5137.jpg to images_split/train_10_percent/weimaraner/n02092339_5137.jpg\nimages_split/train_10_percent/sussex_spaniel\nCopying: images_split/train/sussex_spaniel/n02102480_3717.jpg to images_split/train_10_percent/sussex_spaniel/n02102480_3717.jpg\nimages_split/train_10_percent/lhasa\nCopying: images_split/train/lhasa/n02098413_6348.jpg to images_split/train_10_percent/lhasa/n02098413_6348.jpg\nimages_split/train_10_percent/vizsla\nCopying: images_split/train/vizsla/n02100583_473.jpg to images_split/train_10_percent/vizsla/n02100583_473.jpg\nimages_split/train_10_percent/giant_schnauzer\nCopying: images_split/train/giant_schnauzer/n02097130_2942.jpg to images_split/train_10_percent/giant_schnauzer/n02097130_2942.jpg\nimages_split/train_10_percent/japanese_spaniel\nCopying: images_split/train/japanese_spaniel/n02085782_172.jpg to images_split/train_10_percent/japanese_spaniel/n02085782_172.jpg\nimages_split/train_10_percent/chow\nCopying: images_split/train/chow/n02112137_6314.jpg to images_split/train_10_percent/chow/n02112137_6314.jpg\nimages_split/train_10_percent/french_bulldog\nCopying: images_split/train/french_bulldog/n02108915_9399.jpg to images_split/train_10_percent/french_bulldog/n02108915_9399.jpg\nimages_split/train_10_percent/west_highland_white_terrier\nCopying: images_split/train/west_highland_white_terrier/n02098286_3573.jpg to images_split/train_10_percent/west_highland_white_terrier/n02098286_3573.jpg\nimages_split/train_10_percent/beagle\nCopying: images_split/train/beagle/n02088364_14369.jpg to images_split/train_10_percent/beagle/n02088364_14369.jpg\nimages_split/train_10_percent/airedale\nCopying: images_split/train/airedale/n02096051_1761.jpg to images_split/train_10_percent/airedale/n02096051_1761.jpg\nimages_split/train_10_percent/yorkshire_terrier\nCopying: images_split/train/yorkshire_terrier/n02094433_6328.jpg to images_split/train_10_percent/yorkshire_terrier/n02094433_6328.jpg\nimages_split/train_10_percent/blenheim_spaniel\nCopying: images_split/train/blenheim_spaniel/n02086646_1182.jpg to images_split/train_10_percent/blenheim_spaniel/n02086646_1182.jpg\nimages_split/train_10_percent/dingo\nCopying: images_split/train/dingo/n02115641_8168.jpg to images_split/train_10_percent/dingo/n02115641_8168.jpg\nimages_split/train_10_percent/appenzeller\nCopying: images_split/train/appenzeller/n02107908_1235.jpg to images_split/train_10_percent/appenzeller/n02107908_1235.jpg\nimages_split/train_10_percent/wire_haired_fox_terrier\nCopying: images_split/train/wire_haired_fox_terrier/n02095314_481.jpg to images_split/train_10_percent/wire_haired_fox_terrier/n02095314_481.jpg\nimages_split/train_10_percent/shetland_sheepdog\nCopying: images_split/train/shetland_sheepdog/n02105855_3150.jpg to images_split/train_10_percent/shetland_sheepdog/n02105855_3150.jpg\nimages_split/train_10_percent/english_springer\nCopying: images_split/train/english_springer/n02102040_1259.jpg to images_split/train_10_percent/english_springer/n02102040_1259.jpg\nimages_split/train_10_percent/pug\nCopying: images_split/train/pug/n02110958_13263.jpg to images_split/train_10_percent/pug/n02110958_13263.jpg\nimages_split/train_10_percent/newfoundland\nCopying: images_split/train/newfoundland/n02111277_182.jpg to images_split/train_10_percent/newfoundland/n02111277_182.jpg\nimages_split/train_10_percent/bloodhound\nCopying: images_split/train/bloodhound/n02088466_9579.jpg to images_split/train_10_percent/bloodhound/n02088466_9579.jpg\nimages_split/train_10_percent/briard\nCopying: images_split/train/briard/n02105251_6840.jpg to images_split/train_10_percent/briard/n02105251_6840.jpg\nimages_split/train_10_percent/lhasa\nCopying: images_split/train/lhasa/n02098413_15855.jpg to images_split/train_10_percent/lhasa/n02098413_15855.jpg\nimages_split/train_10_percent/scotch_terrier\nCopying: images_split/train/scotch_terrier/n02097298_3589.jpg to images_split/train_10_percent/scotch_terrier/n02097298_3589.jpg\nimages_split/train_10_percent/greater_swiss_mountain_dog\nCopying: images_split/train/greater_swiss_mountain_dog/n02107574_3099.jpg to images_split/train_10_percent/greater_swiss_mountain_dog/n02107574_3099.jpg\nimages_split/train_10_percent/italian_greyhound\nCopying: images_split/train/italian_greyhound/n02091032_12013.jpg to images_split/train_10_percent/italian_greyhound/n02091032_12013.jpg\nimages_split/train_10_percent/boston_bull\nCopying: images_split/train/boston_bull/n02096585_9122.jpg to images_split/train_10_percent/boston_bull/n02096585_9122.jpg\nimages_split/train_10_percent/beagle\nCopying: images_split/train/beagle/n02088364_12710.jpg to images_split/train_10_percent/beagle/n02088364_12710.jpg\nimages_split/train_10_percent/welsh_springer_spaniel\nCopying: images_split/train/welsh_springer_spaniel/n02102177_2405.jpg to images_split/train_10_percent/welsh_springer_spaniel/n02102177_2405.jpg\nimages_split/train_10_percent/norwich_terrier\nCopying: images_split/train/norwich_terrier/n02094258_1345.jpg to images_split/train_10_percent/norwich_terrier/n02094258_1345.jpg\nimages_split/train_10_percent/silky_terrier\nCopying: images_split/train/silky_terrier/n02097658_2143.jpg to images_split/train_10_percent/silky_terrier/n02097658_2143.jpg\nimages_split/train_10_percent/english_foxhound\nCopying: images_split/train/english_foxhound/n02089973_846.jpg to images_split/train_10_percent/english_foxhound/n02089973_846.jpg\nimages_split/train_10_percent/whippet\nCopying: images_split/train/whippet/n02091134_7661.jpg to images_split/train_10_percent/whippet/n02091134_7661.jpg\nimages_split/train_10_percent/english_setter\nCopying: images_split/train/english_setter/n02100735_216.jpg to images_split/train_10_percent/english_setter/n02100735_216.jpg\nimages_split/train_10_percent/standard_poodle\nCopying: images_split/train/standard_poodle/n02113799_2426.jpg to images_split/train_10_percent/standard_poodle/n02113799_2426.jpg\nimages_split/train_10_percent/basenji\nCopying: images_split/train/basenji/n02110806_6035.jpg to images_split/train_10_percent/basenji/n02110806_6035.jpg\nimages_split/train_10_percent/pug\nCopying: images_split/train/pug/n02110958_12224.jpg to images_split/train_10_percent/pug/n02110958_12224.jpg\nimages_split/train_10_percent/giant_schnauzer\nCopying: images_split/train/giant_schnauzer/n02097130_1287.jpg to images_split/train_10_percent/giant_schnauzer/n02097130_1287.jpg\nimages_split/train_10_percent/tibetan_terrier\nCopying: images_split/train/tibetan_terrier/n02097474_3226.jpg to images_split/train_10_percent/tibetan_terrier/n02097474_3226.jpg\nimages_split/train_10_percent/black_and_tan_coonhound\nCopying: images_split/train/black_and_tan_coonhound/n02089078_393.jpg to images_split/train_10_percent/black_and_tan_coonhound/n02089078_393.jpg\nimages_split/train_10_percent/bedlington_terrier\nCopying: images_split/train/bedlington_terrier/n02093647_2108.jpg to images_split/train_10_percent/bedlington_terrier/n02093647_2108.jpg\nimages_split/train_10_percent/bernese_mountain_dog\nCopying: images_split/train/bernese_mountain_dog/n02107683_6660.jpg to images_split/train_10_percent/bernese_mountain_dog/n02107683_6660.jpg\nimages_split/train_10_percent/weimaraner\nCopying: images_split/train/weimaraner/n02092339_2766.jpg to images_split/train_10_percent/weimaraner/n02092339_2766.jpg\nimages_split/train_10_percent/american_staffordshire_terrier\nCopying: images_split/train/american_staffordshire_terrier/n02093428_5331.jpg to images_split/train_10_percent/american_staffordshire_terrier/n02093428_5331.jpg\nimages_split/train_10_percent/english_foxhound\nCopying: images_split/train/english_foxhound/n02089973_569.jpg to images_split/train_10_percent/english_foxhound/n02089973_569.jpg\nimages_split/train_10_percent/french_bulldog\nCopying: images_split/train/french_bulldog/n02108915_2118.jpg to images_split/train_10_percent/french_bulldog/n02108915_2118.jpg\nimages_split/train_10_percent/great_dane\nCopying: images_split/train/great_dane/n02109047_1449.jpg to images_split/train_10_percent/great_dane/n02109047_1449.jpg\nimages_split/train_10_percent/labrador_retriever\nCopying: images_split/train/labrador_retriever/n02099712_1828.jpg to images_split/train_10_percent/labrador_retriever/n02099712_1828.jpg\nimages_split/train_10_percent/african_hunting_dog\nCopying: images_split/train/african_hunting_dog/n02116738_6117.jpg to images_split/train_10_percent/african_hunting_dog/n02116738_6117.jpg\nimages_split/train_10_percent/briard\nCopying: images_split/train/briard/n02105251_8345.jpg to images_split/train_10_percent/briard/n02105251_8345.jpg\nimages_split/train_10_percent/keeshond\nCopying: images_split/train/keeshond/n02112350_8469.jpg to images_split/train_10_percent/keeshond/n02112350_8469.jpg\nimages_split/train_10_percent/italian_greyhound\nCopying: images_split/train/italian_greyhound/n02091032_4971.jpg to images_split/train_10_percent/italian_greyhound/n02091032_4971.jpg\nimages_split/train_10_percent/bluetick\nCopying: images_split/train/bluetick/n02088632_1592.jpg to images_split/train_10_percent/bluetick/n02088632_1592.jpg\nimages_split/train_10_percent/great_dane\nCopying: images_split/train/great_dane/n02109047_6265.jpg to images_split/train_10_percent/great_dane/n02109047_6265.jpg\nimages_split/train_10_percent/lakeland_terrier\nCopying: images_split/train/lakeland_terrier/n02095570_2972.jpg to images_split/train_10_percent/lakeland_terrier/n02095570_2972.jpg\nimages_split/train_10_percent/irish_setter\nCopying: images_split/train/irish_setter/n02100877_1787.jpg to images_split/train_10_percent/irish_setter/n02100877_1787.jpg\nimages_split/train_10_percent/african_hunting_dog\nCopying: images_split/train/african_hunting_dog/n02116738_8489.jpg to images_split/train_10_percent/african_hunting_dog/n02116738_8489.jpg\nimages_split/train_10_percent/redbone\nCopying: images_split/train/redbone/n02090379_2149.jpg to images_split/train_10_percent/redbone/n02090379_2149.jpg\nimages_split/train_10_percent/siberian_husky\nCopying: images_split/train/siberian_husky/n02110185_7762.jpg to images_split/train_10_percent/siberian_husky/n02110185_7762.jpg\nimages_split/train_10_percent/entlebucher\nCopying: images_split/train/entlebucher/n02108000_1671.jpg to images_split/train_10_percent/entlebucher/n02108000_1671.jpg\nimages_split/train_10_percent/scotch_terrier\nCopying: images_split/train/scotch_terrier/n02097298_1007.jpg to images_split/train_10_percent/scotch_terrier/n02097298_1007.jpg\nimages_split/train_10_percent/silky_terrier\nCopying: images_split/train/silky_terrier/n02097658_6867.jpg to images_split/train_10_percent/silky_terrier/n02097658_6867.jpg\nimages_split/train_10_percent/border_collie\nCopying: images_split/train/border_collie/n02106166_18.jpg to images_split/train_10_percent/border_collie/n02106166_18.jpg\nimages_split/train_10_percent/bernese_mountain_dog\nCopying: images_split/train/bernese_mountain_dog/n02107683_3908.jpg to images_split/train_10_percent/bernese_mountain_dog/n02107683_3908.jpg\nimages_split/train_10_percent/miniature_schnauzer\nCopying: images_split/train/miniature_schnauzer/n02097047_6553.jpg to images_split/train_10_percent/miniature_schnauzer/n02097047_6553.jpg\nimages_split/train_10_percent/rottweiler\nCopying: images_split/train/rottweiler/n02106550_7003.jpg to images_split/train_10_percent/rottweiler/n02106550_7003.jpg\nimages_split/train_10_percent/papillon\nCopying: images_split/train/papillon/n02086910_9128.jpg to images_split/train_10_percent/papillon/n02086910_9128.jpg\nimages_split/train_10_percent/lhasa\nCopying: images_split/train/lhasa/n02098413_4100.jpg to images_split/train_10_percent/lhasa/n02098413_4100.jpg\nimages_split/train_10_percent/rottweiler\nCopying: images_split/train/rottweiler/n02106550_9432.jpg to images_split/train_10_percent/rottweiler/n02106550_9432.jpg\nimages_split/train_10_percent/rhodesian_ridgeback\nCopying: images_split/train/rhodesian_ridgeback/n02087394_3477.jpg to images_split/train_10_percent/rhodesian_ridgeback/n02087394_3477.jpg\nimages_split/train_10_percent/briard\nCopying: images_split/train/briard/n02105251_8743.jpg to images_split/train_10_percent/briard/n02105251_8743.jpg\nimages_split/train_10_percent/clumber\nCopying: images_split/train/clumber/n02101556_5405.jpg to images_split/train_10_percent/clumber/n02101556_5405.jpg\nimages_split/train_10_percent/afghan_hound\nCopying: images_split/train/afghan_hound/n02088094_13879.jpg to images_split/train_10_percent/afghan_hound/n02088094_13879.jpg\nimages_split/train_10_percent/chesapeake_bay_retriever\nCopying: images_split/train/chesapeake_bay_retriever/n02099849_513.jpg to images_split/train_10_percent/chesapeake_bay_retriever/n02099849_513.jpg\nimages_split/train_10_percent/staffordshire_bullterrier\nCopying: images_split/train/staffordshire_bullterrier/n02093256_5110.jpg to images_split/train_10_percent/staffordshire_bullterrier/n02093256_5110.jpg\nimages_split/train_10_percent/shih_tzu\nCopying: images_split/train/shih_tzu/n02086240_10785.jpg to images_split/train_10_percent/shih_tzu/n02086240_10785.jpg\nimages_split/train_10_percent/basenji\nCopying: images_split/train/basenji/n02110806_6247.jpg to images_split/train_10_percent/basenji/n02110806_6247.jpg\nimages_split/train_10_percent/pekinese\nCopying: images_split/train/pekinese/n02086079_15871.jpg to images_split/train_10_percent/pekinese/n02086079_15871.jpg\nimages_split/train_10_percent/entlebucher\nCopying: images_split/train/entlebucher/n02108000_522.jpg to images_split/train_10_percent/entlebucher/n02108000_522.jpg\nimages_split/train_10_percent/golden_retriever\nCopying: images_split/train/golden_retriever/n02099601_1768.jpg to images_split/train_10_percent/golden_retriever/n02099601_1768.jpg\nimages_split/train_10_percent/malamute\nCopying: images_split/train/malamute/n02110063_17572.jpg to images_split/train_10_percent/malamute/n02110063_17572.jpg\nimages_split/train_10_percent/chow\nCopying: images_split/train/chow/n02112137_8504.jpg to images_split/train_10_percent/chow/n02112137_8504.jpg\nimages_split/train_10_percent/dingo\nCopying: images_split/train/dingo/n02115641_7071.jpg to images_split/train_10_percent/dingo/n02115641_7071.jpg\nimages_split/train_10_percent/pekinese\nCopying: images_split/train/pekinese/n02086079_3174.jpg to images_split/train_10_percent/pekinese/n02086079_3174.jpg\nimages_split/train_10_percent/brittany_spaniel\nCopying: images_split/train/brittany_spaniel/n02101388_4113.jpg to images_split/train_10_percent/brittany_spaniel/n02101388_4113.jpg\nimages_split/train_10_percent/dandie_dinmont\nCopying: images_split/train/dandie_dinmont/n02096437_599.jpg to images_split/train_10_percent/dandie_dinmont/n02096437_599.jpg\nimages_split/train_10_percent/collie\nCopying: images_split/train/collie/n02106030_7572.jpg to images_split/train_10_percent/collie/n02106030_7572.jpg\nimages_split/train_10_percent/japanese_spaniel\nCopying: images_split/train/japanese_spaniel/n02085782_1267.jpg to images_split/train_10_percent/japanese_spaniel/n02085782_1267.jpg\nimages_split/train_10_percent/malinois\nCopying: images_split/train/malinois/n02105162_2896.jpg to images_split/train_10_percent/malinois/n02105162_2896.jpg\nimages_split/train_10_percent/pug\nCopying: images_split/train/pug/n02110958_15015.jpg to images_split/train_10_percent/pug/n02110958_15015.jpg\nimages_split/train_10_percent/border_terrier\nCopying: images_split/train/border_terrier/n02093754_4564.jpg to images_split/train_10_percent/border_terrier/n02093754_4564.jpg\nimages_split/train_10_percent/whippet\nCopying: images_split/train/whippet/n02091134_12142.jpg to images_split/train_10_percent/whippet/n02091134_12142.jpg\nimages_split/train_10_percent/dhole\nCopying: images_split/train/dhole/n02115913_3740.jpg to images_split/train_10_percent/dhole/n02115913_3740.jpg\nimages_split/train_10_percent/borzoi\nCopying: images_split/train/borzoi/n02090622_7232.jpg to images_split/train_10_percent/borzoi/n02090622_7232.jpg\nimages_split/train_10_percent/gordon_setter\nCopying: images_split/train/gordon_setter/n02101006_4510.jpg to images_split/train_10_percent/gordon_setter/n02101006_4510.jpg\nimages_split/train_10_percent/weimaraner\nCopying: images_split/train/weimaraner/n02092339_6077.jpg to images_split/train_10_percent/weimaraner/n02092339_6077.jpg\nimages_split/train_10_percent/miniature_poodle\nCopying: images_split/train/miniature_poodle/n02113712_3207.jpg to images_split/train_10_percent/miniature_poodle/n02113712_3207.jpg\nimages_split/train_10_percent/boston_bull\nCopying: images_split/train/boston_bull/n02096585_1532.jpg to images_split/train_10_percent/boston_bull/n02096585_1532.jpg\nimages_split/train_10_percent/toy_terrier\nCopying: images_split/train/toy_terrier/n02087046_7245.jpg to images_split/train_10_percent/toy_terrier/n02087046_7245.jpg\nimages_split/train_10_percent/yorkshire_terrier\nCopying: images_split/train/yorkshire_terrier/n02094433_3642.jpg to images_split/train_10_percent/yorkshire_terrier/n02094433_3642.jpg\nimages_split/train_10_percent/collie\nCopying: images_split/train/collie/n02106030_16509.jpg to images_split/train_10_percent/collie/n02106030_16509.jpg\nimages_split/train_10_percent/great_dane\nCopying: images_split/train/great_dane/n02109047_875.jpg to images_split/train_10_percent/great_dane/n02109047_875.jpg\nimages_split/train_10_percent/bull_mastiff\nCopying: images_split/train/bull_mastiff/n02108422_4793.jpg to images_split/train_10_percent/bull_mastiff/n02108422_4793.jpg\nimages_split/train_10_percent/miniature_poodle\nCopying: images_split/train/miniature_poodle/n02113712_2596.jpg to images_split/train_10_percent/miniature_poodle/n02113712_2596.jpg\nimages_split/train_10_percent/doberman\nCopying: images_split/train/doberman/n02107142_17899.jpg to images_split/train_10_percent/doberman/n02107142_17899.jpg\nimages_split/train_10_percent/bull_mastiff\nCopying: images_split/train/bull_mastiff/n02108422_1701.jpg to images_split/train_10_percent/bull_mastiff/n02108422_1701.jpg\nimages_split/train_10_percent/schipperke\nCopying: images_split/train/schipperke/n02104365_8998.jpg to images_split/train_10_percent/schipperke/n02104365_8998.jpg\nimages_split/train_10_percent/irish_wolfhound\nCopying: images_split/train/irish_wolfhound/n02090721_7155.jpg to images_split/train_10_percent/irish_wolfhound/n02090721_7155.jpg\nimages_split/train_10_percent/dhole\nCopying: images_split/train/dhole/n02115913_4266.jpg to images_split/train_10_percent/dhole/n02115913_4266.jpg\nimages_split/train_10_percent/soft_coated_wheaten_terrier\nCopying: images_split/train/soft_coated_wheaten_terrier/n02098105_1386.jpg to images_split/train_10_percent/soft_coated_wheaten_terrier/n02098105_1386.jpg\nimages_split/train_10_percent/welsh_springer_spaniel\nCopying: images_split/train/welsh_springer_spaniel/n02102177_2755.jpg to images_split/train_10_percent/welsh_springer_spaniel/n02102177_2755.jpg\nimages_split/train_10_percent/whippet\nCopying: images_split/train/whippet/n02091134_15784.jpg to images_split/train_10_percent/whippet/n02091134_15784.jpg\nimages_split/train_10_percent/great_pyrenees\nCopying: images_split/train/great_pyrenees/n02111500_3597.jpg to images_split/train_10_percent/great_pyrenees/n02111500_3597.jpg\nimages_split/train_10_percent/black_and_tan_coonhound\nCopying: images_split/train/black_and_tan_coonhound/n02089078_111.jpg to images_split/train_10_percent/black_and_tan_coonhound/n02089078_111.jpg\nimages_split/train_10_percent/groenendael\nCopying: images_split/train/groenendael/n02105056_6596.jpg to images_split/train_10_percent/groenendael/n02105056_6596.jpg\nimages_split/train_10_percent/standard_schnauzer\nCopying: images_split/train/standard_schnauzer/n02097209_1536.jpg to images_split/train_10_percent/standard_schnauzer/n02097209_1536.jpg\nimages_split/train_10_percent/schipperke\nCopying: images_split/train/schipperke/n02104365_2435.jpg to images_split/train_10_percent/schipperke/n02104365_2435.jpg\nimages_split/train_10_percent/bull_mastiff\nCopying: images_split/train/bull_mastiff/n02108422_2123.jpg to images_split/train_10_percent/bull_mastiff/n02108422_2123.jpg\nimages_split/train_10_percent/collie\nCopying: images_split/train/collie/n02106030_16173.jpg to images_split/train_10_percent/collie/n02106030_16173.jpg\nimages_split/train_10_percent/miniature_schnauzer\nCopying: images_split/train/miniature_schnauzer/n02097047_1589.jpg to images_split/train_10_percent/miniature_schnauzer/n02097047_1589.jpg\nimages_split/train_10_percent/flat_coated_retriever\nCopying: images_split/train/flat_coated_retriever/n02099267_1205.jpg to images_split/train_10_percent/flat_coated_retriever/n02099267_1205.jpg\nimages_split/train_10_percent/shetland_sheepdog\nCopying: images_split/train/shetland_sheepdog/n02105855_6802.jpg to images_split/train_10_percent/shetland_sheepdog/n02105855_6802.jpg\nimages_split/train_10_percent/staffordshire_bullterrier\nCopying: images_split/train/staffordshire_bullterrier/n02093256_4452.jpg to images_split/train_10_percent/staffordshire_bullterrier/n02093256_4452.jpg\nimages_split/train_10_percent/weimaraner\nCopying: images_split/train/weimaraner/n02092339_258.jpg to images_split/train_10_percent/weimaraner/n02092339_258.jpg\nimages_split/train_10_percent/pekinese\nCopying: images_split/train/pekinese/n02086079_1820.jpg to images_split/train_10_percent/pekinese/n02086079_1820.jpg\nimages_split/train_10_percent/leonberg\nCopying: images_split/train/leonberg/n02111129_1115.jpg to images_split/train_10_percent/leonberg/n02111129_1115.jpg\nimages_split/train_10_percent/newfoundland\nCopying: images_split/train/newfoundland/n02111277_3165.jpg to images_split/train_10_percent/newfoundland/n02111277_3165.jpg\nimages_split/train_10_percent/schipperke\nCopying: images_split/train/schipperke/n02104365_7629.jpg to images_split/train_10_percent/schipperke/n02104365_7629.jpg\nimages_split/train_10_percent/brittany_spaniel\nCopying: images_split/train/brittany_spaniel/n02101388_5541.jpg to images_split/train_10_percent/brittany_spaniel/n02101388_5541.jpg\nimages_split/train_10_percent/lakeland_terrier\nCopying: images_split/train/lakeland_terrier/n02095570_4854.jpg to images_split/train_10_percent/lakeland_terrier/n02095570_4854.jpg\nimages_split/train_10_percent/tibetan_mastiff\nCopying: images_split/train/tibetan_mastiff/n02108551_2596.jpg to images_split/train_10_percent/tibetan_mastiff/n02108551_2596.jpg\nimages_split/train_10_percent/toy_terrier\nCopying: images_split/train/toy_terrier/n02087046_2819.jpg to images_split/train_10_percent/toy_terrier/n02087046_2819.jpg\nimages_split/train_10_percent/otterhound\nCopying: images_split/train/otterhound/n02091635_1043.jpg to images_split/train_10_percent/otterhound/n02091635_1043.jpg\nimages_split/train_10_percent/vizsla\nCopying: images_split/train/vizsla/n02100583_10960.jpg to images_split/train_10_percent/vizsla/n02100583_10960.jpg\nimages_split/train_10_percent/great_dane\nCopying: images_split/train/great_dane/n02109047_2009.jpg to images_split/train_10_percent/great_dane/n02109047_2009.jpg\nimages_split/train_10_percent/miniature_pinscher\nCopying: images_split/train/miniature_pinscher/n02107312_4996.jpg to images_split/train_10_percent/miniature_pinscher/n02107312_4996.jpg\nimages_split/train_10_percent/airedale\nCopying: images_split/train/airedale/n02096051_8019.jpg to images_split/train_10_percent/airedale/n02096051_8019.jpg\nimages_split/train_10_percent/newfoundland\nCopying: images_split/train/newfoundland/n02111277_13728.jpg to images_split/train_10_percent/newfoundland/n02111277_13728.jpg\nimages_split/train_10_percent/old_english_sheepdog\nCopying: images_split/train/old_english_sheepdog/n02105641_3499.jpg to images_split/train_10_percent/old_english_sheepdog/n02105641_3499.jpg\nimages_split/train_10_percent/australian_terrier\nCopying: images_split/train/australian_terrier/n02096294_7711.jpg to images_split/train_10_percent/australian_terrier/n02096294_7711.jpg\nimages_split/train_10_percent/chihuahua\nCopying: images_split/train/chihuahua/n02085620_5771.jpg to images_split/train_10_percent/chihuahua/n02085620_5771.jpg\nimages_split/train_10_percent/west_highland_white_terrier\nCopying: images_split/train/west_highland_white_terrier/n02098286_5814.jpg to images_split/train_10_percent/west_highland_white_terrier/n02098286_5814.jpg\nimages_split/train_10_percent/newfoundland\nCopying: images_split/train/newfoundland/n02111277_3454.jpg to images_split/train_10_percent/newfoundland/n02111277_3454.jpg\nimages_split/train_10_percent/japanese_spaniel\nCopying: images_split/train/japanese_spaniel/n02085782_3781.jpg to images_split/train_10_percent/japanese_spaniel/n02085782_3781.jpg\nimages_split/train_10_percent/sealyham_terrier\nCopying: images_split/train/sealyham_terrier/n02095889_2977.jpg to images_split/train_10_percent/sealyham_terrier/n02095889_2977.jpg\nimages_split/train_10_percent/samoyed\nCopying: images_split/train/samoyed/n02111889_5778.jpg to images_split/train_10_percent/samoyed/n02111889_5778.jpg\nimages_split/train_10_percent/malinois\nCopying: images_split/train/malinois/n02105162_10375.jpg to images_split/train_10_percent/malinois/n02105162_10375.jpg\nimages_split/train_10_percent/pomeranian\nCopying: images_split/train/pomeranian/n02112018_6159.jpg to images_split/train_10_percent/pomeranian/n02112018_6159.jpg\nimages_split/train_10_percent/cardigan\nCopying: images_split/train/cardigan/n02113186_13169.jpg to images_split/train_10_percent/cardigan/n02113186_13169.jpg\nimages_split/train_10_percent/labrador_retriever\nCopying: images_split/train/labrador_retriever/n02099712_3947.jpg to images_split/train_10_percent/labrador_retriever/n02099712_3947.jpg\nimages_split/train_10_percent/rhodesian_ridgeback\nCopying: images_split/train/rhodesian_ridgeback/n02087394_7544.jpg to images_split/train_10_percent/rhodesian_ridgeback/n02087394_7544.jpg\nimages_split/train_10_percent/irish_water_spaniel\nCopying: images_split/train/irish_water_spaniel/n02102973_273.jpg to images_split/train_10_percent/irish_water_spaniel/n02102973_273.jpg\nimages_split/train_10_percent/kuvasz\nCopying: images_split/train/kuvasz/n02104029_525.jpg to images_split/train_10_percent/kuvasz/n02104029_525.jpg\nimages_split/train_10_percent/italian_greyhound\nCopying: images_split/train/italian_greyhound/n02091032_5093.jpg to images_split/train_10_percent/italian_greyhound/n02091032_5093.jpg\nimages_split/train_10_percent/irish_water_spaniel\nCopying: images_split/train/irish_water_spaniel/n02102973_4287.jpg to images_split/train_10_percent/irish_water_spaniel/n02102973_4287.jpg\nimages_split/train_10_percent/brittany_spaniel\nCopying: images_split/train/brittany_spaniel/n02101388_746.jpg to images_split/train_10_percent/brittany_spaniel/n02101388_746.jpg\nimages_split/train_10_percent/afghan_hound\nCopying: images_split/train/afghan_hound/n02088094_6690.jpg to images_split/train_10_percent/afghan_hound/n02088094_6690.jpg\nimages_split/train_10_percent/bernese_mountain_dog\nCopying: images_split/train/bernese_mountain_dog/n02107683_3278.jpg to images_split/train_10_percent/bernese_mountain_dog/n02107683_3278.jpg\nimages_split/train_10_percent/kelpie\nCopying: images_split/train/kelpie/n02105412_2212.jpg to images_split/train_10_percent/kelpie/n02105412_2212.jpg\nimages_split/train_10_percent/japanese_spaniel\nCopying: images_split/train/japanese_spaniel/n02085782_1528.jpg to images_split/train_10_percent/japanese_spaniel/n02085782_1528.jpg\nimages_split/train_10_percent/lhasa\nCopying: images_split/train/lhasa/n02098413_1655.jpg to images_split/train_10_percent/lhasa/n02098413_1655.jpg\nimages_split/train_10_percent/soft_coated_wheaten_terrier\nCopying: images_split/train/soft_coated_wheaten_terrier/n02098105_257.jpg to images_split/train_10_percent/soft_coated_wheaten_terrier/n02098105_257.jpg\nimages_split/train_10_percent/labrador_retriever\nCopying: images_split/train/labrador_retriever/n02099712_5008.jpg to images_split/train_10_percent/labrador_retriever/n02099712_5008.jpg\nimages_split/train_10_percent/groenendael\nCopying: images_split/train/groenendael/n02105056_5145.jpg to images_split/train_10_percent/groenendael/n02105056_5145.jpg\nimages_split/train_10_percent/shetland_sheepdog\nCopying: images_split/train/shetland_sheepdog/n02105855_14509.jpg to images_split/train_10_percent/shetland_sheepdog/n02105855_14509.jpg\nimages_split/train_10_percent/mexican_hairless\nCopying: images_split/train/mexican_hairless/n02113978_1773.jpg to images_split/train_10_percent/mexican_hairless/n02113978_1773.jpg\nimages_split/train_10_percent/greater_swiss_mountain_dog\nCopying: images_split/train/greater_swiss_mountain_dog/n02107574_2795.jpg to images_split/train_10_percent/greater_swiss_mountain_dog/n02107574_2795.jpg\nimages_split/train_10_percent/mexican_hairless\nCopying: images_split/train/mexican_hairless/n02113978_1480.jpg to images_split/train_10_percent/mexican_hairless/n02113978_1480.jpg\nimages_split/train_10_percent/irish_terrier\nCopying: images_split/train/irish_terrier/n02093991_3935.jpg to images_split/train_10_percent/irish_terrier/n02093991_3935.jpg\nimages_split/train_10_percent/maltese_dog\nCopying: images_split/train/maltese_dog/n02085936_6927.jpg to images_split/train_10_percent/maltese_dog/n02085936_6927.jpg\nimages_split/train_10_percent/briard\nCopying: images_split/train/briard/n02105251_5649.jpg to images_split/train_10_percent/briard/n02105251_5649.jpg\nimages_split/train_10_percent/english_springer\nCopying: images_split/train/english_springer/n02102040_742.jpg to images_split/train_10_percent/english_springer/n02102040_742.jpg\nimages_split/train_10_percent/soft_coated_wheaten_terrier\nCopying: images_split/train/soft_coated_wheaten_terrier/n02098105_3855.jpg to images_split/train_10_percent/soft_coated_wheaten_terrier/n02098105_3855.jpg\nimages_split/train_10_percent/airedale\nCopying: images_split/train/airedale/n02096051_1121.jpg to images_split/train_10_percent/airedale/n02096051_1121.jpg\nimages_split/train_10_percent/pug\nCopying: images_split/train/pug/n02110958_14647.jpg to images_split/train_10_percent/pug/n02110958_14647.jpg\nimages_split/train_10_percent/basenji\nCopying: images_split/train/basenji/n02110806_2249.jpg to images_split/train_10_percent/basenji/n02110806_2249.jpg\nimages_split/train_10_percent/miniature_pinscher\nCopying: images_split/train/miniature_pinscher/n02107312_1590.jpg to images_split/train_10_percent/miniature_pinscher/n02107312_1590.jpg\nimages_split/train_10_percent/lhasa\nCopying: images_split/train/lhasa/n02098413_11467.jpg to images_split/train_10_percent/lhasa/n02098413_11467.jpg\nimages_split/train_10_percent/lakeland_terrier\nCopying: images_split/train/lakeland_terrier/n02095570_4161.jpg to images_split/train_10_percent/lakeland_terrier/n02095570_4161.jpg\nimages_split/train_10_percent/komondor\nCopying: images_split/train/komondor/n02105505_3705.jpg to images_split/train_10_percent/komondor/n02105505_3705.jpg\nimages_split/train_10_percent/chihuahua\nCopying: images_split/train/chihuahua/n02085620_8611.jpg to images_split/train_10_percent/chihuahua/n02085620_8611.jpg\nimages_split/train_10_percent/weimaraner\nCopying: images_split/train/weimaraner/n02092339_2653.jpg to images_split/train_10_percent/weimaraner/n02092339_2653.jpg\nimages_split/train_10_percent/great_dane\nCopying: images_split/train/great_dane/n02109047_22413.jpg to images_split/train_10_percent/great_dane/n02109047_22413.jpg\nimages_split/train_10_percent/english_foxhound\nCopying: images_split/train/english_foxhound/n02089973_3799.jpg to images_split/train_10_percent/english_foxhound/n02089973_3799.jpg\nimages_split/train_10_percent/kelpie\nCopying: images_split/train/kelpie/n02105412_3065.jpg to images_split/train_10_percent/kelpie/n02105412_3065.jpg\nimages_split/train_10_percent/norwegian_elkhound\nCopying: images_split/train/norwegian_elkhound/n02091467_2950.jpg to images_split/train_10_percent/norwegian_elkhound/n02091467_2950.jpg\nimages_split/train_10_percent/vizsla\nCopying: images_split/train/vizsla/n02100583_9794.jpg to images_split/train_10_percent/vizsla/n02100583_9794.jpg\nimages_split/train_10_percent/toy_poodle\nCopying: images_split/train/toy_poodle/n02113624_4087.jpg to images_split/train_10_percent/toy_poodle/n02113624_4087.jpg\nimages_split/train_10_percent/toy_poodle\nCopying: images_split/train/toy_poodle/n02113624_2816.jpg to images_split/train_10_percent/toy_poodle/n02113624_2816.jpg\nimages_split/train_10_percent/papillon\nCopying: images_split/train/papillon/n02086910_3117.jpg to images_split/train_10_percent/papillon/n02086910_3117.jpg\nimages_split/train_10_percent/wire_haired_fox_terrier\nCopying: images_split/train/wire_haired_fox_terrier/n02095314_3153.jpg to images_split/train_10_percent/wire_haired_fox_terrier/n02095314_3153.jpg\nimages_split/train_10_percent/welsh_springer_spaniel\nCopying: images_split/train/welsh_springer_spaniel/n02102177_3710.jpg to images_split/train_10_percent/welsh_springer_spaniel/n02102177_3710.jpg\nimages_split/train_10_percent/newfoundland\nCopying: images_split/train/newfoundland/n02111277_1572.jpg to images_split/train_10_percent/newfoundland/n02111277_1572.jpg\nimages_split/train_10_percent/siberian_husky\nCopying: images_split/train/siberian_husky/n02110185_14479.jpg to images_split/train_10_percent/siberian_husky/n02110185_14479.jpg\nimages_split/train_10_percent/welsh_springer_spaniel\nCopying: images_split/train/welsh_springer_spaniel/n02102177_3377.jpg to images_split/train_10_percent/welsh_springer_spaniel/n02102177_3377.jpg\nimages_split/train_10_percent/brabancon_griffon\nCopying: images_split/train/brabancon_griffon/n02112706_360.jpg to images_split/train_10_percent/brabancon_griffon/n02112706_360.jpg\nimages_split/train_10_percent/old_english_sheepdog\nCopying: images_split/train/old_english_sheepdog/n02105641_5577.jpg to images_split/train_10_percent/old_english_sheepdog/n02105641_5577.jpg\nimages_split/train_10_percent/silky_terrier\nCopying: images_split/train/silky_terrier/n02097658_980.jpg to images_split/train_10_percent/silky_terrier/n02097658_980.jpg\nimages_split/train_10_percent/english_springer\nCopying: images_split/train/english_springer/n02102040_6843.jpg to images_split/train_10_percent/english_springer/n02102040_6843.jpg\nimages_split/train_10_percent/golden_retriever\nCopying: images_split/train/golden_retriever/n02099601_2495.jpg to images_split/train_10_percent/golden_retriever/n02099601_2495.jpg\nimages_split/train_10_percent/redbone\nCopying: images_split/train/redbone/n02090379_4388.jpg to images_split/train_10_percent/redbone/n02090379_4388.jpg\nimages_split/train_10_percent/dhole\nCopying: images_split/train/dhole/n02115913_92.jpg to images_split/train_10_percent/dhole/n02115913_92.jpg\nimages_split/train_10_percent/shih_tzu\nCopying: images_split/train/shih_tzu/n02086240_6924.jpg to images_split/train_10_percent/shih_tzu/n02086240_6924.jpg\nimages_split/train_10_percent/basenji\nCopying: images_split/train/basenji/n02110806_4472.jpg to images_split/train_10_percent/basenji/n02110806_4472.jpg\nimages_split/train_10_percent/scottish_deerhound\nCopying: images_split/train/scottish_deerhound/n02092002_7099.jpg to images_split/train_10_percent/scottish_deerhound/n02092002_7099.jpg\nimages_split/train_10_percent/english_foxhound\nCopying: images_split/train/english_foxhound/n02089973_4359.jpg to images_split/train_10_percent/english_foxhound/n02089973_4359.jpg\nimages_split/train_10_percent/kerry_blue_terrier\nCopying: images_split/train/kerry_blue_terrier/n02093859_1120.jpg to images_split/train_10_percent/kerry_blue_terrier/n02093859_1120.jpg\nimages_split/train_10_percent/tibetan_terrier\nCopying: images_split/train/tibetan_terrier/n02097474_5280.jpg to images_split/train_10_percent/tibetan_terrier/n02097474_5280.jpg\nimages_split/train_10_percent/flat_coated_retriever\nCopying: images_split/train/flat_coated_retriever/n02099267_4048.jpg to images_split/train_10_percent/flat_coated_retriever/n02099267_4048.jpg\nimages_split/train_10_percent/keeshond\nCopying: images_split/train/keeshond/n02112350_7087.jpg to images_split/train_10_percent/keeshond/n02112350_7087.jpg\nimages_split/train_10_percent/chihuahua\nCopying: images_split/train/chihuahua/n02085620_8558.jpg to images_split/train_10_percent/chihuahua/n02085620_8558.jpg\nimages_split/train_10_percent/irish_terrier\nCopying: images_split/train/irish_terrier/n02093991_1978.jpg to images_split/train_10_percent/irish_terrier/n02093991_1978.jpg\nimages_split/train_10_percent/german_short_haired_pointer\nCopying: images_split/train/german_short_haired_pointer/n02100236_4900.jpg to images_split/train_10_percent/german_short_haired_pointer/n02100236_4900.jpg\nimages_split/train_10_percent/irish_setter\nCopying: images_split/train/irish_setter/n02100877_5231.jpg to images_split/train_10_percent/irish_setter/n02100877_5231.jpg\nimages_split/train_10_percent/samoyed\nCopying: images_split/train/samoyed/n02111889_1314.jpg to images_split/train_10_percent/samoyed/n02111889_1314.jpg\nimages_split/train_10_percent/west_highland_white_terrier\nCopying: images_split/train/west_highland_white_terrier/n02098286_2958.jpg to images_split/train_10_percent/west_highland_white_terrier/n02098286_2958.jpg\nimages_split/train_10_percent/norwegian_elkhound\nCopying: images_split/train/norwegian_elkhound/n02091467_3849.jpg to images_split/train_10_percent/norwegian_elkhound/n02091467_3849.jpg\nimages_split/train_10_percent/greater_swiss_mountain_dog\nCopying: images_split/train/greater_swiss_mountain_dog/n02107574_492.jpg to images_split/train_10_percent/greater_swiss_mountain_dog/n02107574_492.jpg\nimages_split/train_10_percent/border_collie\nCopying: images_split/train/border_collie/n02106166_1460.jpg to images_split/train_10_percent/border_collie/n02106166_1460.jpg\nimages_split/train_10_percent/mexican_hairless\nCopying: images_split/train/mexican_hairless/n02113978_2306.jpg to images_split/train_10_percent/mexican_hairless/n02113978_2306.jpg\nimages_split/train_10_percent/brittany_spaniel\nCopying: images_split/train/brittany_spaniel/n02101388_6564.jpg to images_split/train_10_percent/brittany_spaniel/n02101388_6564.jpg\nimages_split/train_10_percent/saint_bernard\nCopying: images_split/train/saint_bernard/n02109525_368.jpg to images_split/train_10_percent/saint_bernard/n02109525_368.jpg\nimages_split/train_10_percent/bull_mastiff\nCopying: images_split/train/bull_mastiff/n02108422_2990.jpg to images_split/train_10_percent/bull_mastiff/n02108422_2990.jpg\nimages_split/train_10_percent/english_setter\nCopying: images_split/train/english_setter/n02100735_6665.jpg to images_split/train_10_percent/english_setter/n02100735_6665.jpg\nimages_split/train_10_percent/bedlington_terrier\nCopying: images_split/train/bedlington_terrier/n02093647_1108.jpg to images_split/train_10_percent/bedlington_terrier/n02093647_1108.jpg\nimages_split/train_10_percent/leonberg\nCopying: images_split/train/leonberg/n02111129_2700.jpg to images_split/train_10_percent/leonberg/n02111129_2700.jpg\nimages_split/train_10_percent/german_shepherd\nCopying: images_split/train/german_shepherd/n02106662_7545.jpg to images_split/train_10_percent/german_shepherd/n02106662_7545.jpg\nimages_split/train_10_percent/clumber\nCopying: images_split/train/clumber/n02101556_6228.jpg to images_split/train_10_percent/clumber/n02101556_6228.jpg\nimages_split/train_10_percent/pomeranian\nCopying: images_split/train/pomeranian/n02112018_7880.jpg to images_split/train_10_percent/pomeranian/n02112018_7880.jpg\nimages_split/train_10_percent/gordon_setter\nCopying: images_split/train/gordon_setter/n02101006_3807.jpg to images_split/train_10_percent/gordon_setter/n02101006_3807.jpg\nimages_split/train_10_percent/borzoi\nCopying: images_split/train/borzoi/n02090622_6131.jpg to images_split/train_10_percent/borzoi/n02090622_6131.jpg\nimages_split/train_10_percent/leonberg\nCopying: images_split/train/leonberg/n02111129_2751.jpg to images_split/train_10_percent/leonberg/n02111129_2751.jpg\nimages_split/train_10_percent/american_staffordshire_terrier\nCopying: images_split/train/american_staffordshire_terrier/n02093428_10381.jpg to images_split/train_10_percent/american_staffordshire_terrier/n02093428_10381.jpg\nimages_split/train_10_percent/chesapeake_bay_retriever\nCopying: images_split/train/chesapeake_bay_retriever/n02099849_435.jpg to images_split/train_10_percent/chesapeake_bay_retriever/n02099849_435.jpg\nimages_split/train_10_percent/mexican_hairless\nCopying: images_split/train/mexican_hairless/n02113978_3184.jpg to images_split/train_10_percent/mexican_hairless/n02113978_3184.jpg\nimages_split/train_10_percent/pekinese\nCopying: images_split/train/pekinese/n02086079_7235.jpg to images_split/train_10_percent/pekinese/n02086079_7235.jpg\nimages_split/train_10_percent/scotch_terrier\nCopying: images_split/train/scotch_terrier/n02097298_8392.jpg to images_split/train_10_percent/scotch_terrier/n02097298_8392.jpg\nimages_split/train_10_percent/collie\nCopying: images_split/train/collie/n02106030_14677.jpg to images_split/train_10_percent/collie/n02106030_14677.jpg\nimages_split/train_10_percent/staffordshire_bullterrier\nCopying: images_split/train/staffordshire_bullterrier/n02093256_5654.jpg to images_split/train_10_percent/staffordshire_bullterrier/n02093256_5654.jpg\nimages_split/train_10_percent/great_dane\nCopying: images_split/train/great_dane/n02109047_27131.jpg to images_split/train_10_percent/great_dane/n02109047_27131.jpg\nimages_split/train_10_percent/leonberg\nCopying: images_split/train/leonberg/n02111129_1206.jpg to images_split/train_10_percent/leonberg/n02111129_1206.jpg\nimages_split/train_10_percent/bouvier_des_flandres\nCopying: images_split/train/bouvier_des_flandres/n02106382_241.jpg to images_split/train_10_percent/bouvier_des_flandres/n02106382_241.jpg\nimages_split/train_10_percent/redbone\nCopying: images_split/train/redbone/n02090379_3721.jpg to images_split/train_10_percent/redbone/n02090379_3721.jpg\nimages_split/train_10_percent/cocker_spaniel\nCopying: images_split/train/cocker_spaniel/n02102318_12717.jpg to images_split/train_10_percent/cocker_spaniel/n02102318_12717.jpg\nimages_split/train_10_percent/dandie_dinmont\nCopying: images_split/train/dandie_dinmont/n02096437_335.jpg to images_split/train_10_percent/dandie_dinmont/n02096437_335.jpg\nimages_split/train_10_percent/briard\nCopying: images_split/train/briard/n02105251_7170.jpg to images_split/train_10_percent/briard/n02105251_7170.jpg\nimages_split/train_10_percent/ibizan_hound\nCopying: images_split/train/ibizan_hound/n02091244_5601.jpg to images_split/train_10_percent/ibizan_hound/n02091244_5601.jpg\nimages_split/train_10_percent/afghan_hound\nCopying: images_split/train/afghan_hound/n02088094_7260.jpg to images_split/train_10_percent/afghan_hound/n02088094_7260.jpg\nimages_split/train_10_percent/rottweiler\nCopying: images_split/train/rottweiler/n02106550_4987.jpg to images_split/train_10_percent/rottweiler/n02106550_4987.jpg\nimages_split/train_10_percent/basenji\nCopying: images_split/train/basenji/n02110806_2157.jpg to images_split/train_10_percent/basenji/n02110806_2157.jpg\nimages_split/train_10_percent/great_dane\nCopying: images_split/train/great_dane/n02109047_5894.jpg to images_split/train_10_percent/great_dane/n02109047_5894.jpg\nimages_split/train_10_percent/affenpinscher\nCopying: images_split/train/affenpinscher/n02110627_12556.jpg to images_split/train_10_percent/affenpinscher/n02110627_12556.jpg\nimages_split/train_10_percent/toy_poodle\nCopying: images_split/train/toy_poodle/n02113624_1559.jpg to images_split/train_10_percent/toy_poodle/n02113624_1559.jpg\nimages_split/train_10_percent/lakeland_terrier\nCopying: images_split/train/lakeland_terrier/n02095570_1590.jpg to images_split/train_10_percent/lakeland_terrier/n02095570_1590.jpg\nimages_split/train_10_percent/brittany_spaniel\nCopying: images_split/train/brittany_spaniel/n02101388_9320.jpg to images_split/train_10_percent/brittany_spaniel/n02101388_9320.jpg\nimages_split/train_10_percent/newfoundland\nCopying: images_split/train/newfoundland/n02111277_399.jpg to images_split/train_10_percent/newfoundland/n02111277_399.jpg\nimages_split/train_10_percent/english_setter\nCopying: images_split/train/english_setter/n02100735_10175.jpg to images_split/train_10_percent/english_setter/n02100735_10175.jpg\nimages_split/train_10_percent/irish_water_spaniel\nCopying: images_split/train/irish_water_spaniel/n02102973_1846.jpg to images_split/train_10_percent/irish_water_spaniel/n02102973_1846.jpg\nimages_split/train_10_percent/cardigan\nCopying: images_split/train/cardigan/n02113186_11741.jpg to images_split/train_10_percent/cardigan/n02113186_11741.jpg\nimages_split/train_10_percent/staffordshire_bullterrier\nCopying: images_split/train/staffordshire_bullterrier/n02093256_1638.jpg to images_split/train_10_percent/staffordshire_bullterrier/n02093256_1638.jpg\nimages_split/train_10_percent/kuvasz\nCopying: images_split/train/kuvasz/n02104029_2467.jpg to images_split/train_10_percent/kuvasz/n02104029_2467.jpg\nimages_split/train_10_percent/walker_hound\nCopying: images_split/train/walker_hound/n02089867_2039.jpg to images_split/train_10_percent/walker_hound/n02089867_2039.jpg\nimages_split/train_10_percent/rhodesian_ridgeback\nCopying: images_split/train/rhodesian_ridgeback/n02087394_1099.jpg to images_split/train_10_percent/rhodesian_ridgeback/n02087394_1099.jpg\nimages_split/train_10_percent/brittany_spaniel\nCopying: images_split/train/brittany_spaniel/n02101388_3697.jpg to images_split/train_10_percent/brittany_spaniel/n02101388_3697.jpg\nimages_split/train_10_percent/afghan_hound\nCopying: images_split/train/afghan_hound/n02088094_6485.jpg to images_split/train_10_percent/afghan_hound/n02088094_6485.jpg\nimages_split/train_10_percent/redbone\nCopying: images_split/train/redbone/n02090379_5182.jpg to images_split/train_10_percent/redbone/n02090379_5182.jpg\nimages_split/train_10_percent/english_foxhound\nCopying: images_split/train/english_foxhound/n02089973_2300.jpg to images_split/train_10_percent/english_foxhound/n02089973_2300.jpg\nimages_split/train_10_percent/afghan_hound\nCopying: images_split/train/afghan_hound/n02088094_5812.jpg to images_split/train_10_percent/afghan_hound/n02088094_5812.jpg\nimages_split/train_10_percent/african_hunting_dog\nCopying: images_split/train/african_hunting_dog/n02116738_5936.jpg to images_split/train_10_percent/african_hunting_dog/n02116738_5936.jpg\nimages_split/train_10_percent/saluki\nCopying: images_split/train/saluki/n02091831_3038.jpg to images_split/train_10_percent/saluki/n02091831_3038.jpg\nimages_split/train_10_percent/french_bulldog\nCopying: images_split/train/french_bulldog/n02108915_644.jpg to images_split/train_10_percent/french_bulldog/n02108915_644.jpg\nimages_split/train_10_percent/norwich_terrier\nCopying: images_split/train/norwich_terrier/n02094258_1220.jpg to images_split/train_10_percent/norwich_terrier/n02094258_1220.jpg\nimages_split/train_10_percent/ibizan_hound\nCopying: images_split/train/ibizan_hound/n02091244_4914.jpg to images_split/train_10_percent/ibizan_hound/n02091244_4914.jpg\nimages_split/train_10_percent/basenji\nCopying: images_split/train/basenji/n02110806_5238.jpg to images_split/train_10_percent/basenji/n02110806_5238.jpg\nimages_split/train_10_percent/silky_terrier\nCopying: images_split/train/silky_terrier/n02097658_816.jpg to images_split/train_10_percent/silky_terrier/n02097658_816.jpg\nimages_split/train_10_percent/boxer\nCopying: images_split/train/boxer/n02108089_2917.jpg to images_split/train_10_percent/boxer/n02108089_2917.jpg\nimages_split/train_10_percent/basenji\nCopying: images_split/train/basenji/n02110806_4244.jpg to images_split/train_10_percent/basenji/n02110806_4244.jpg\nimages_split/train_10_percent/dhole\nCopying: images_split/train/dhole/n02115913_4110.jpg to images_split/train_10_percent/dhole/n02115913_4110.jpg\nimages_split/train_10_percent/great_pyrenees\nCopying: images_split/train/great_pyrenees/n02111500_4590.jpg to images_split/train_10_percent/great_pyrenees/n02111500_4590.jpg\nimages_split/train_10_percent/sealyham_terrier\nCopying: images_split/train/sealyham_terrier/n02095889_6232.jpg to images_split/train_10_percent/sealyham_terrier/n02095889_6232.jpg\nimages_split/train_10_percent/malamute\nCopying: images_split/train/malamute/n02110063_13541.jpg to images_split/train_10_percent/malamute/n02110063_13541.jpg\nimages_split/train_10_percent/english_setter\nCopying: images_split/train/english_setter/n02100735_9865.jpg to images_split/train_10_percent/english_setter/n02100735_9865.jpg\nimages_split/train_10_percent/rottweiler\nCopying: images_split/train/rottweiler/n02106550_13226.jpg to images_split/train_10_percent/rottweiler/n02106550_13226.jpg\nimages_split/train_10_percent/saint_bernard\nCopying: images_split/train/saint_bernard/n02109525_16284.jpg to images_split/train_10_percent/saint_bernard/n02109525_16284.jpg\nimages_split/train_10_percent/blenheim_spaniel\nCopying: images_split/train/blenheim_spaniel/n02086646_20.jpg to images_split/train_10_percent/blenheim_spaniel/n02086646_20.jpg\nimages_split/train_10_percent/standard_schnauzer\nCopying: images_split/train/standard_schnauzer/n02097209_221.jpg to images_split/train_10_percent/standard_schnauzer/n02097209_221.jpg\nimages_split/train_10_percent/shetland_sheepdog\nCopying: images_split/train/shetland_sheepdog/n02105855_16145.jpg to images_split/train_10_percent/shetland_sheepdog/n02105855_16145.jpg\nimages_split/train_10_percent/toy_terrier\nCopying: images_split/train/toy_terrier/n02087046_4808.jpg to images_split/train_10_percent/toy_terrier/n02087046_4808.jpg\nimages_split/train_10_percent/dhole\nCopying: images_split/train/dhole/n02115913_1352.jpg to images_split/train_10_percent/dhole/n02115913_1352.jpg\nimages_split/train_10_percent/basenji\nCopying: images_split/train/basenji/n02110806_1359.jpg to images_split/train_10_percent/basenji/n02110806_1359.jpg\nimages_split/train_10_percent/wire_haired_fox_terrier\nCopying: images_split/train/wire_haired_fox_terrier/n02095314_1772.jpg to images_split/train_10_percent/wire_haired_fox_terrier/n02095314_1772.jpg\nimages_split/train_10_percent/west_highland_white_terrier\nCopying: images_split/train/west_highland_white_terrier/n02098286_763.jpg to images_split/train_10_percent/west_highland_white_terrier/n02098286_763.jpg\nimages_split/train_10_percent/irish_wolfhound\nCopying: images_split/train/irish_wolfhound/n02090721_1105.jpg to images_split/train_10_percent/irish_wolfhound/n02090721_1105.jpg\nimages_split/train_10_percent/norwegian_elkhound\nCopying: images_split/train/norwegian_elkhound/n02091467_1760.jpg to images_split/train_10_percent/norwegian_elkhound/n02091467_1760.jpg\nimages_split/train_10_percent/lhasa\nCopying: images_split/train/lhasa/n02098413_6162.jpg to images_split/train_10_percent/lhasa/n02098413_6162.jpg\nimages_split/train_10_percent/boxer\nCopying: images_split/train/boxer/n02108089_13898.jpg to images_split/train_10_percent/boxer/n02108089_13898.jpg\nimages_split/train_10_percent/basset\nCopying: images_split/train/basset/n02088238_9994.jpg to images_split/train_10_percent/basset/n02088238_9994.jpg\nimages_split/train_10_percent/german_short_haired_pointer\nCopying: images_split/train/german_short_haired_pointer/n02100236_5647.jpg to images_split/train_10_percent/german_short_haired_pointer/n02100236_5647.jpg\nimages_split/train_10_percent/bernese_mountain_dog\nCopying: images_split/train/bernese_mountain_dog/n02107683_4695.jpg to images_split/train_10_percent/bernese_mountain_dog/n02107683_4695.jpg\nimages_split/train_10_percent/boxer\nCopying: images_split/train/boxer/n02108089_5753.jpg to images_split/train_10_percent/boxer/n02108089_5753.jpg\nimages_split/train_10_percent/yorkshire_terrier\nCopying: images_split/train/yorkshire_terrier/n02094433_2223.jpg to images_split/train_10_percent/yorkshire_terrier/n02094433_2223.jpg\nimages_split/train_10_percent/groenendael\nCopying: images_split/train/groenendael/n02105056_6614.jpg to images_split/train_10_percent/groenendael/n02105056_6614.jpg\nimages_split/train_10_percent/gordon_setter\nCopying: images_split/train/gordon_setter/n02101006_635.jpg to images_split/train_10_percent/gordon_setter/n02101006_635.jpg\nimages_split/train_10_percent/italian_greyhound\nCopying: images_split/train/italian_greyhound/n02091032_4323.jpg to images_split/train_10_percent/italian_greyhound/n02091032_4323.jpg\nimages_split/train_10_percent/pekinese\nCopying: images_split/train/pekinese/n02086079_17291.jpg to images_split/train_10_percent/pekinese/n02086079_17291.jpg\nimages_split/train_10_percent/wire_haired_fox_terrier\nCopying: images_split/train/wire_haired_fox_terrier/n02095314_1782.jpg to images_split/train_10_percent/wire_haired_fox_terrier/n02095314_1782.jpg\nimages_split/train_10_percent/kelpie\nCopying: images_split/train/kelpie/n02105412_1031.jpg to images_split/train_10_percent/kelpie/n02105412_1031.jpg\nimages_split/train_10_percent/border_terrier\nCopying: images_split/train/border_terrier/n02093754_5261.jpg to images_split/train_10_percent/border_terrier/n02093754_5261.jpg\nimages_split/train_10_percent/keeshond\nCopying: images_split/train/keeshond/n02112350_7952.jpg to images_split/train_10_percent/keeshond/n02112350_7952.jpg\nimages_split/train_10_percent/otterhound\nCopying: images_split/train/otterhound/n02091635_470.jpg to images_split/train_10_percent/otterhound/n02091635_470.jpg\nimages_split/train_10_percent/norwich_terrier\nCopying: images_split/train/norwich_terrier/n02094258_2732.jpg to images_split/train_10_percent/norwich_terrier/n02094258_2732.jpg\nimages_split/train_10_percent/siberian_husky\nCopying: images_split/train/siberian_husky/n02110185_3328.jpg to images_split/train_10_percent/siberian_husky/n02110185_3328.jpg\nimages_split/train_10_percent/keeshond\nCopying: images_split/train/keeshond/n02112350_6007.jpg to images_split/train_10_percent/keeshond/n02112350_6007.jpg\nimages_split/train_10_percent/irish_setter\nCopying: images_split/train/irish_setter/n02100877_2952.jpg to images_split/train_10_percent/irish_setter/n02100877_2952.jpg\nimages_split/train_10_percent/dhole\nCopying: images_split/train/dhole/n02115913_3842.jpg to images_split/train_10_percent/dhole/n02115913_3842.jpg\nimages_split/train_10_percent/curly_coated_retriever\nCopying: images_split/train/curly_coated_retriever/n02099429_1701.jpg to images_split/train_10_percent/curly_coated_retriever/n02099429_1701.jpg\nimages_split/train_10_percent/norwich_terrier\nCopying: images_split/train/norwich_terrier/n02094258_1219.jpg to images_split/train_10_percent/norwich_terrier/n02094258_1219.jpg\nimages_split/train_10_percent/weimaraner\nCopying: images_split/train/weimaraner/n02092339_1013.jpg to images_split/train_10_percent/weimaraner/n02092339_1013.jpg\nimages_split/train_10_percent/staffordshire_bullterrier\nCopying: images_split/train/staffordshire_bullterrier/n02093256_5439.jpg to images_split/train_10_percent/staffordshire_bullterrier/n02093256_5439.jpg\nimages_split/train_10_percent/afghan_hound\nCopying: images_split/train/afghan_hound/n02088094_3613.jpg to images_split/train_10_percent/afghan_hound/n02088094_3613.jpg\nimages_split/train_10_percent/otterhound\nCopying: images_split/train/otterhound/n02091635_1492.jpg to images_split/train_10_percent/otterhound/n02091635_1492.jpg\nimages_split/train_10_percent/australian_terrier\nCopying: images_split/train/australian_terrier/n02096294_167.jpg to images_split/train_10_percent/australian_terrier/n02096294_167.jpg\nimages_split/train_10_percent/chesapeake_bay_retriever\nCopying: images_split/train/chesapeake_bay_retriever/n02099849_3403.jpg to images_split/train_10_percent/chesapeake_bay_retriever/n02099849_3403.jpg\nimages_split/train_10_percent/redbone\nCopying: images_split/train/redbone/n02090379_855.jpg to images_split/train_10_percent/redbone/n02090379_855.jpg\nimages_split/train_10_percent/chow\nCopying: images_split/train/chow/n02112137_10654.jpg to images_split/train_10_percent/chow/n02112137_10654.jpg\nimages_split/train_10_percent/scottish_deerhound\nCopying: images_split/train/scottish_deerhound/n02092002_7751.jpg to images_split/train_10_percent/scottish_deerhound/n02092002_7751.jpg\nimages_split/train_10_percent/norwich_terrier\nCopying: images_split/train/norwich_terrier/n02094258_90.jpg to images_split/train_10_percent/norwich_terrier/n02094258_90.jpg\nimages_split/train_10_percent/standard_schnauzer\nCopying: images_split/train/standard_schnauzer/n02097209_991.jpg to images_split/train_10_percent/standard_schnauzer/n02097209_991.jpg\nimages_split/train_10_percent/border_terrier\nCopying: images_split/train/border_terrier/n02093754_6992.jpg to images_split/train_10_percent/border_terrier/n02093754_6992.jpg\nimages_split/train_10_percent/german_shepherd\nCopying: images_split/train/german_shepherd/n02106662_23360.jpg to images_split/train_10_percent/german_shepherd/n02106662_23360.jpg\nimages_split/train_10_percent/cardigan\nCopying: images_split/train/cardigan/n02113186_6415.jpg to images_split/train_10_percent/cardigan/n02113186_6415.jpg\nimages_split/train_10_percent/kerry_blue_terrier\nCopying: images_split/train/kerry_blue_terrier/n02093859_1279.jpg to images_split/train_10_percent/kerry_blue_terrier/n02093859_1279.jpg\nimages_split/train_10_percent/saint_bernard\nCopying: images_split/train/saint_bernard/n02109525_2770.jpg to images_split/train_10_percent/saint_bernard/n02109525_2770.jpg\nimages_split/train_10_percent/malamute\nCopying: images_split/train/malamute/n02110063_11227.jpg to images_split/train_10_percent/malamute/n02110063_11227.jpg\nimages_split/train_10_percent/miniature_pinscher\nCopying: images_split/train/miniature_pinscher/n02107312_6380.jpg to images_split/train_10_percent/miniature_pinscher/n02107312_6380.jpg\nimages_split/train_10_percent/pekinese\nCopying: images_split/train/pekinese/n02086079_5053.jpg to images_split/train_10_percent/pekinese/n02086079_5053.jpg\nimages_split/train_10_percent/bloodhound\nCopying: images_split/train/bloodhound/n02088466_4731.jpg to images_split/train_10_percent/bloodhound/n02088466_4731.jpg\nimages_split/train_10_percent/labrador_retriever\nCopying: images_split/train/labrador_retriever/n02099712_5679.jpg to images_split/train_10_percent/labrador_retriever/n02099712_5679.jpg\nimages_split/train_10_percent/boxer\nCopying: images_split/train/boxer/n02108089_11616.jpg to images_split/train_10_percent/boxer/n02108089_11616.jpg\nimages_split/train_10_percent/eskimo_dog\nCopying: images_split/train/eskimo_dog/n02109961_3217.jpg to images_split/train_10_percent/eskimo_dog/n02109961_3217.jpg\nimages_split/train_10_percent/english_foxhound\nCopying: images_split/train/english_foxhound/n02089973_2599.jpg to images_split/train_10_percent/english_foxhound/n02089973_2599.jpg\nimages_split/train_10_percent/clumber\nCopying: images_split/train/clumber/n02101556_3100.jpg to images_split/train_10_percent/clumber/n02101556_3100.jpg\nimages_split/train_10_percent/english_setter\nCopying: images_split/train/english_setter/n02100735_9111.jpg to images_split/train_10_percent/english_setter/n02100735_9111.jpg\nimages_split/train_10_percent/scottish_deerhound\nCopying: images_split/train/scottish_deerhound/n02092002_49.jpg to images_split/train_10_percent/scottish_deerhound/n02092002_49.jpg\nimages_split/train_10_percent/saint_bernard\nCopying: images_split/train/saint_bernard/n02109525_1843.jpg to images_split/train_10_percent/saint_bernard/n02109525_1843.jpg\nimages_split/train_10_percent/welsh_springer_spaniel\nCopying: images_split/train/welsh_springer_spaniel/n02102177_501.jpg to images_split/train_10_percent/welsh_springer_spaniel/n02102177_501.jpg\nimages_split/train_10_percent/chow\nCopying: images_split/train/chow/n02112137_12685.jpg to images_split/train_10_percent/chow/n02112137_12685.jpg\nimages_split/train_10_percent/great_dane\nCopying: images_split/train/great_dane/n02109047_5936.jpg to images_split/train_10_percent/great_dane/n02109047_5936.jpg\nimages_split/train_10_percent/great_pyrenees\nCopying: images_split/train/great_pyrenees/n02111500_870.jpg to images_split/train_10_percent/great_pyrenees/n02111500_870.jpg\nimages_split/train_10_percent/entlebucher\nCopying: images_split/train/entlebucher/n02108000_326.jpg to images_split/train_10_percent/entlebucher/n02108000_326.jpg\nimages_split/train_10_percent/brittany_spaniel\nCopying: images_split/train/brittany_spaniel/n02101388_6377.jpg to images_split/train_10_percent/brittany_spaniel/n02101388_6377.jpg\nimages_split/train_10_percent/curly_coated_retriever\nCopying: images_split/train/curly_coated_retriever/n02099429_3103.jpg to images_split/train_10_percent/curly_coated_retriever/n02099429_3103.jpg\nimages_split/train_10_percent/labrador_retriever\nCopying: images_split/train/labrador_retriever/n02099712_357.jpg to images_split/train_10_percent/labrador_retriever/n02099712_357.jpg\nimages_split/train_10_percent/kelpie\nCopying: images_split/train/kelpie/n02105412_1211.jpg to images_split/train_10_percent/kelpie/n02105412_1211.jpg\nimages_split/train_10_percent/great_pyrenees\nCopying: images_split/train/great_pyrenees/n02111500_2746.jpg to images_split/train_10_percent/great_pyrenees/n02111500_2746.jpg\nimages_split/train_10_percent/komondor\nCopying: images_split/train/komondor/n02105505_4282.jpg to images_split/train_10_percent/komondor/n02105505_4282.jpg\nimages_split/train_10_percent/miniature_poodle\nCopying: images_split/train/miniature_poodle/n02113712_8756.jpg to images_split/train_10_percent/miniature_poodle/n02113712_8756.jpg\nimages_split/train_10_percent/standard_poodle\nCopying: images_split/train/standard_poodle/n02113799_936.jpg to images_split/train_10_percent/standard_poodle/n02113799_936.jpg\nimages_split/train_10_percent/english_setter\nCopying: images_split/train/english_setter/n02100735_7838.jpg to images_split/train_10_percent/english_setter/n02100735_7838.jpg\nimages_split/train_10_percent/golden_retriever\nCopying: images_split/train/golden_retriever/n02099601_704.jpg to images_split/train_10_percent/golden_retriever/n02099601_704.jpg\nimages_split/train_10_percent/welsh_springer_spaniel\nCopying: images_split/train/welsh_springer_spaniel/n02102177_716.jpg to images_split/train_10_percent/welsh_springer_spaniel/n02102177_716.jpg\nimages_split/train_10_percent/beagle\nCopying: images_split/train/beagle/n02088364_4070.jpg to images_split/train_10_percent/beagle/n02088364_4070.jpg\nimages_split/train_10_percent/tibetan_terrier\nCopying: images_split/train/tibetan_terrier/n02097474_1328.jpg to images_split/train_10_percent/tibetan_terrier/n02097474_1328.jpg\nimages_split/train_10_percent/labrador_retriever\nCopying: images_split/train/labrador_retriever/n02099712_7418.jpg to images_split/train_10_percent/labrador_retriever/n02099712_7418.jpg\nimages_split/train_10_percent/pug\nCopying: images_split/train/pug/n02110958_13439.jpg to images_split/train_10_percent/pug/n02110958_13439.jpg\nimages_split/train_10_percent/pug\nCopying: images_split/train/pug/n02110958_12120.jpg to images_split/train_10_percent/pug/n02110958_12120.jpg\nimages_split/train_10_percent/chihuahua\nCopying: images_split/train/chihuahua/n02085620_2706.jpg to images_split/train_10_percent/chihuahua/n02085620_2706.jpg\nimages_split/train_10_percent/japanese_spaniel\nCopying: images_split/train/japanese_spaniel/n02085782_4616.jpg to images_split/train_10_percent/japanese_spaniel/n02085782_4616.jpg\nimages_split/train_10_percent/briard\nCopying: images_split/train/briard/n02105251_7772.jpg to images_split/train_10_percent/briard/n02105251_7772.jpg\nimages_split/train_10_percent/keeshond\nCopying: images_split/train/keeshond/n02112350_7157.jpg to images_split/train_10_percent/keeshond/n02112350_7157.jpg\nimages_split/train_10_percent/african_hunting_dog\nCopying: images_split/train/african_hunting_dog/n02116738_6790.jpg to images_split/train_10_percent/african_hunting_dog/n02116738_6790.jpg\nimages_split/train_10_percent/bull_mastiff\nCopying: images_split/train/bull_mastiff/n02108422_3277.jpg to images_split/train_10_percent/bull_mastiff/n02108422_3277.jpg\nimages_split/train_10_percent/toy_poodle\nCopying: images_split/train/toy_poodle/n02113624_7997.jpg to images_split/train_10_percent/toy_poodle/n02113624_7997.jpg\nimages_split/train_10_percent/redbone\nCopying: images_split/train/redbone/n02090379_2280.jpg to images_split/train_10_percent/redbone/n02090379_2280.jpg\nimages_split/train_10_percent/airedale\nCopying: images_split/train/airedale/n02096051_6873.jpg to images_split/train_10_percent/airedale/n02096051_6873.jpg\nimages_split/train_10_percent/shetland_sheepdog\nCopying: images_split/train/shetland_sheepdog/n02105855_13407.jpg to images_split/train_10_percent/shetland_sheepdog/n02105855_13407.jpg\nimages_split/train_10_percent/chesapeake_bay_retriever\nCopying: images_split/train/chesapeake_bay_retriever/n02099849_43.jpg to images_split/train_10_percent/chesapeake_bay_retriever/n02099849_43.jpg\nimages_split/train_10_percent/rottweiler\nCopying: images_split/train/rottweiler/n02106550_11444.jpg to images_split/train_10_percent/rottweiler/n02106550_11444.jpg\nimages_split/train_10_percent/entlebucher\nCopying: images_split/train/entlebucher/n02108000_841.jpg to images_split/train_10_percent/entlebucher/n02108000_841.jpg\nimages_split/train_10_percent/standard_schnauzer\nCopying: images_split/train/standard_schnauzer/n02097209_169.jpg to images_split/train_10_percent/standard_schnauzer/n02097209_169.jpg\nimages_split/train_10_percent/australian_terrier\nCopying: images_split/train/australian_terrier/n02096294_3800.jpg to images_split/train_10_percent/australian_terrier/n02096294_3800.jpg\nimages_split/train_10_percent/samoyed\nCopying: images_split/train/samoyed/n02111889_13281.jpg to images_split/train_10_percent/samoyed/n02111889_13281.jpg\nimages_split/train_10_percent/wire_haired_fox_terrier\nCopying: images_split/train/wire_haired_fox_terrier/n02095314_2196.jpg to images_split/train_10_percent/wire_haired_fox_terrier/n02095314_2196.jpg\nimages_split/train_10_percent/affenpinscher\nCopying: images_split/train/affenpinscher/n02110627_13211.jpg to images_split/train_10_percent/affenpinscher/n02110627_13211.jpg\nimages_split/train_10_percent/airedale\nCopying: images_split/train/airedale/n02096051_7847.jpg to images_split/train_10_percent/airedale/n02096051_7847.jpg\nimages_split/train_10_percent/norfolk_terrier\nCopying: images_split/train/norfolk_terrier/n02094114_315.jpg to images_split/train_10_percent/norfolk_terrier/n02094114_315.jpg\nimages_split/train_10_percent/toy_poodle\nCopying: images_split/train/toy_poodle/n02113624_1606.jpg to images_split/train_10_percent/toy_poodle/n02113624_1606.jpg\nimages_split/train_10_percent/scottish_deerhound\nCopying: images_split/train/scottish_deerhound/n02092002_4131.jpg to images_split/train_10_percent/scottish_deerhound/n02092002_4131.jpg\nimages_split/train_10_percent/kuvasz\nCopying: images_split/train/kuvasz/n02104029_2246.jpg to images_split/train_10_percent/kuvasz/n02104029_2246.jpg\nimages_split/train_10_percent/shih_tzu\nCopying: images_split/train/shih_tzu/n02086240_1142.jpg to images_split/train_10_percent/shih_tzu/n02086240_1142.jpg\nimages_split/train_10_percent/kuvasz\nCopying: images_split/train/kuvasz/n02104029_133.jpg to images_split/train_10_percent/kuvasz/n02104029_133.jpg\nimages_split/train_10_percent/cocker_spaniel\nCopying: images_split/train/cocker_spaniel/n02102318_11573.jpg to images_split/train_10_percent/cocker_spaniel/n02102318_11573.jpg\nimages_split/train_10_percent/west_highland_white_terrier\nCopying: images_split/train/west_highland_white_terrier/n02098286_2481.jpg to images_split/train_10_percent/west_highland_white_terrier/n02098286_2481.jpg\nimages_split/train_10_percent/japanese_spaniel\nCopying: images_split/train/japanese_spaniel/n02085782_3649.jpg to images_split/train_10_percent/japanese_spaniel/n02085782_3649.jpg\nimages_split/train_10_percent/borzoi\nCopying: images_split/train/borzoi/n02090622_6408.jpg to images_split/train_10_percent/borzoi/n02090622_6408.jpg\nimages_split/train_10_percent/basenji\nCopying: images_split/train/basenji/n02110806_3937.jpg to images_split/train_10_percent/basenji/n02110806_3937.jpg\nimages_split/train_10_percent/american_staffordshire_terrier\nCopying: images_split/train/american_staffordshire_terrier/n02093428_19443.jpg to images_split/train_10_percent/american_staffordshire_terrier/n02093428_19443.jpg\nimages_split/train_10_percent/boxer\nCopying: images_split/train/boxer/n02108089_2056.jpg to images_split/train_10_percent/boxer/n02108089_2056.jpg\nimages_split/train_10_percent/golden_retriever\nCopying: images_split/train/golden_retriever/n02099601_14.jpg to images_split/train_10_percent/golden_retriever/n02099601_14.jpg\nimages_split/train_10_percent/borzoi\nCopying: images_split/train/borzoi/n02090622_7799.jpg to images_split/train_10_percent/borzoi/n02090622_7799.jpg\nimages_split/train_10_percent/black_and_tan_coonhound\nCopying: images_split/train/black_and_tan_coonhound/n02089078_324.jpg to images_split/train_10_percent/black_and_tan_coonhound/n02089078_324.jpg\nimages_split/train_10_percent/toy_terrier\nCopying: images_split/train/toy_terrier/n02087046_5756.jpg to images_split/train_10_percent/toy_terrier/n02087046_5756.jpg\nimages_split/train_10_percent/tibetan_terrier\nCopying: images_split/train/tibetan_terrier/n02097474_2763.jpg to images_split/train_10_percent/tibetan_terrier/n02097474_2763.jpg\nimages_split/train_10_percent/redbone\nCopying: images_split/train/redbone/n02090379_1401.jpg to images_split/train_10_percent/redbone/n02090379_1401.jpg\nimages_split/train_10_percent/great_dane\nCopying: images_split/train/great_dane/n02109047_5573.jpg to images_split/train_10_percent/great_dane/n02109047_5573.jpg\nimages_split/train_10_percent/eskimo_dog\nCopying: images_split/train/eskimo_dog/n02109961_16953.jpg to images_split/train_10_percent/eskimo_dog/n02109961_16953.jpg\nimages_split/train_10_percent/maltese_dog\nCopying: images_split/train/maltese_dog/n02085936_2905.jpg to images_split/train_10_percent/maltese_dog/n02085936_2905.jpg\nimages_split/train_10_percent/border_terrier\nCopying: images_split/train/border_terrier/n02093754_4697.jpg to images_split/train_10_percent/border_terrier/n02093754_4697.jpg\nimages_split/train_10_percent/eskimo_dog\nCopying: images_split/train/eskimo_dog/n02109961_5924.jpg to images_split/train_10_percent/eskimo_dog/n02109961_5924.jpg\nimages_split/train_10_percent/gordon_setter\nCopying: images_split/train/gordon_setter/n02101006_1208.jpg to images_split/train_10_percent/gordon_setter/n02101006_1208.jpg\nimages_split/train_10_percent/chow\nCopying: images_split/train/chow/n02112137_2411.jpg to images_split/train_10_percent/chow/n02112137_2411.jpg\nimages_split/train_10_percent/gordon_setter\nCopying: images_split/train/gordon_setter/n02101006_922.jpg to images_split/train_10_percent/gordon_setter/n02101006_922.jpg\nimages_split/train_10_percent/italian_greyhound\nCopying: images_split/train/italian_greyhound/n02091032_5975.jpg to images_split/train_10_percent/italian_greyhound/n02091032_5975.jpg\nimages_split/train_10_percent/greater_swiss_mountain_dog\nCopying: images_split/train/greater_swiss_mountain_dog/n02107574_1390.jpg to images_split/train_10_percent/greater_swiss_mountain_dog/n02107574_1390.jpg\nimages_split/train_10_percent/tibetan_terrier\nCopying: images_split/train/tibetan_terrier/n02097474_3914.jpg to images_split/train_10_percent/tibetan_terrier/n02097474_3914.jpg\nimages_split/train_10_percent/dingo\nCopying: images_split/train/dingo/n02115641_8463.jpg to images_split/train_10_percent/dingo/n02115641_8463.jpg\nimages_split/train_10_percent/cocker_spaniel\nCopying: images_split/train/cocker_spaniel/n02102318_12846.jpg to images_split/train_10_percent/cocker_spaniel/n02102318_12846.jpg\nimages_split/train_10_percent/shih_tzu\nCopying: images_split/train/shih_tzu/n02086240_7142.jpg to images_split/train_10_percent/shih_tzu/n02086240_7142.jpg\nimages_split/train_10_percent/australian_terrier\nCopying: images_split/train/australian_terrier/n02096294_8067.jpg to images_split/train_10_percent/australian_terrier/n02096294_8067.jpg\nimages_split/train_10_percent/west_highland_white_terrier\nCopying: images_split/train/west_highland_white_terrier/n02098286_758.jpg to images_split/train_10_percent/west_highland_white_terrier/n02098286_758.jpg\nimages_split/train_10_percent/english_springer\nCopying: images_split/train/english_springer/n02102040_350.jpg to images_split/train_10_percent/english_springer/n02102040_350.jpg\nimages_split/train_10_percent/komondor\nCopying: images_split/train/komondor/n02105505_1443.jpg to images_split/train_10_percent/komondor/n02105505_1443.jpg\nimages_split/train_10_percent/miniature_schnauzer\nCopying: images_split/train/miniature_schnauzer/n02097047_1553.jpg to images_split/train_10_percent/miniature_schnauzer/n02097047_1553.jpg\nimages_split/train_10_percent/shetland_sheepdog\nCopying: images_split/train/shetland_sheepdog/n02105855_7410.jpg to images_split/train_10_percent/shetland_sheepdog/n02105855_7410.jpg\nimages_split/train_10_percent/mexican_hairless\nCopying: images_split/train/mexican_hairless/n02113978_1535.jpg to images_split/train_10_percent/mexican_hairless/n02113978_1535.jpg\nimages_split/train_10_percent/pug\nCopying: images_split/train/pug/n02110958_15449.jpg to images_split/train_10_percent/pug/n02110958_15449.jpg\nimages_split/train_10_percent/cocker_spaniel\nCopying: images_split/train/cocker_spaniel/n02102318_9250.jpg to images_split/train_10_percent/cocker_spaniel/n02102318_9250.jpg\nimages_split/train_10_percent/pembroke\nCopying: images_split/train/pembroke/n02113023_4052.jpg to images_split/train_10_percent/pembroke/n02113023_4052.jpg\nimages_split/train_10_percent/norfolk_terrier\nCopying: images_split/train/norfolk_terrier/n02094114_3177.jpg to images_split/train_10_percent/norfolk_terrier/n02094114_3177.jpg\nimages_split/train_10_percent/vizsla\nCopying: images_split/train/vizsla/n02100583_1969.jpg to images_split/train_10_percent/vizsla/n02100583_1969.jpg\nimages_split/train_10_percent/wire_haired_fox_terrier\nCopying: images_split/train/wire_haired_fox_terrier/n02095314_2308.jpg to images_split/train_10_percent/wire_haired_fox_terrier/n02095314_2308.jpg\nimages_split/train_10_percent/affenpinscher\nCopying: images_split/train/affenpinscher/n02110627_13782.jpg to images_split/train_10_percent/affenpinscher/n02110627_13782.jpg\nimages_split/train_10_percent/eskimo_dog\nCopying: images_split/train/eskimo_dog/n02109961_10774.jpg to images_split/train_10_percent/eskimo_dog/n02109961_10774.jpg\nimages_split/train_10_percent/leonberg\nCopying: images_split/train/leonberg/n02111129_856.jpg to images_split/train_10_percent/leonberg/n02111129_856.jpg\nimages_split/train_10_percent/miniature_pinscher\nCopying: images_split/train/miniature_pinscher/n02107312_67.jpg to images_split/train_10_percent/miniature_pinscher/n02107312_67.jpg\nimages_split/train_10_percent/great_dane\nCopying: images_split/train/great_dane/n02109047_32692.jpg to images_split/train_10_percent/great_dane/n02109047_32692.jpg\nimages_split/train_10_percent/tibetan_terrier\nCopying: images_split/train/tibetan_terrier/n02097474_889.jpg to images_split/train_10_percent/tibetan_terrier/n02097474_889.jpg\nimages_split/train_10_percent/kerry_blue_terrier\nCopying: images_split/train/kerry_blue_terrier/n02093859_184.jpg to images_split/train_10_percent/kerry_blue_terrier/n02093859_184.jpg\nimages_split/train_10_percent/english_foxhound\nCopying: images_split/train/english_foxhound/n02089973_1799.jpg to images_split/train_10_percent/english_foxhound/n02089973_1799.jpg\nimages_split/train_10_percent/standard_poodle\nCopying: images_split/train/standard_poodle/n02113799_1328.jpg to images_split/train_10_percent/standard_poodle/n02113799_1328.jpg\nimages_split/train_10_percent/pekinese\nCopying: images_split/train/pekinese/n02086079_14929.jpg to images_split/train_10_percent/pekinese/n02086079_14929.jpg\nimages_split/train_10_percent/cairn\nCopying: images_split/train/cairn/n02096177_5117.jpg to images_split/train_10_percent/cairn/n02096177_5117.jpg\nimages_split/train_10_percent/collie\nCopying: images_split/train/collie/n02106030_15172.jpg to images_split/train_10_percent/collie/n02106030_15172.jpg\nimages_split/train_10_percent/english_setter\nCopying: images_split/train/english_setter/n02100735_6469.jpg to images_split/train_10_percent/english_setter/n02100735_6469.jpg\nimages_split/train_10_percent/leonberg\nCopying: images_split/train/leonberg/n02111129_2781.jpg to images_split/train_10_percent/leonberg/n02111129_2781.jpg\nimages_split/train_10_percent/japanese_spaniel\nCopying: images_split/train/japanese_spaniel/n02085782_4590.jpg to images_split/train_10_percent/japanese_spaniel/n02085782_4590.jpg\nimages_split/train_10_percent/redbone\nCopying: images_split/train/redbone/n02090379_1515.jpg to images_split/train_10_percent/redbone/n02090379_1515.jpg\nimages_split/train_10_percent/samoyed\nCopying: images_split/train/samoyed/n02111889_4368.jpg to images_split/train_10_percent/samoyed/n02111889_4368.jpg\nimages_split/train_10_percent/kelpie\nCopying: images_split/train/kelpie/n02105412_5135.jpg to images_split/train_10_percent/kelpie/n02105412_5135.jpg\nimages_split/train_10_percent/newfoundland\nCopying: images_split/train/newfoundland/n02111277_5369.jpg to images_split/train_10_percent/newfoundland/n02111277_5369.jpg\nimages_split/train_10_percent/ibizan_hound\nCopying: images_split/train/ibizan_hound/n02091244_589.jpg to images_split/train_10_percent/ibizan_hound/n02091244_589.jpg\nimages_split/train_10_percent/appenzeller\nCopying: images_split/train/appenzeller/n02107908_5662.jpg to images_split/train_10_percent/appenzeller/n02107908_5662.jpg\nimages_split/train_10_percent/welsh_springer_spaniel\nCopying: images_split/train/welsh_springer_spaniel/n02102177_818.jpg to images_split/train_10_percent/welsh_springer_spaniel/n02102177_818.jpg\nimages_split/train_10_percent/australian_terrier\nCopying: images_split/train/australian_terrier/n02096294_1536.jpg to images_split/train_10_percent/australian_terrier/n02096294_1536.jpg\nimages_split/train_10_percent/soft_coated_wheaten_terrier\nCopying: images_split/train/soft_coated_wheaten_terrier/n02098105_2744.jpg to images_split/train_10_percent/soft_coated_wheaten_terrier/n02098105_2744.jpg\nimages_split/train_10_percent/appenzeller\nCopying: images_split/train/appenzeller/n02107908_3531.jpg to images_split/train_10_percent/appenzeller/n02107908_3531.jpg\nimages_split/train_10_percent/pekinese\nCopying: images_split/train/pekinese/n02086079_14307.jpg to images_split/train_10_percent/pekinese/n02086079_14307.jpg\nimages_split/train_10_percent/welsh_springer_spaniel\nCopying: images_split/train/welsh_springer_spaniel/n02102177_1055.jpg to images_split/train_10_percent/welsh_springer_spaniel/n02102177_1055.jpg\nimages_split/train_10_percent/american_staffordshire_terrier\nCopying: images_split/train/american_staffordshire_terrier/n02093428_10908.jpg to images_split/train_10_percent/american_staffordshire_terrier/n02093428_10908.jpg\nimages_split/train_10_percent/lhasa\nCopying: images_split/train/lhasa/n02098413_18548.jpg to images_split/train_10_percent/lhasa/n02098413_18548.jpg\nimages_split/train_10_percent/chesapeake_bay_retriever\nCopying: images_split/train/chesapeake_bay_retriever/n02099849_562.jpg to images_split/train_10_percent/chesapeake_bay_retriever/n02099849_562.jpg\nimages_split/train_10_percent/mexican_hairless\nCopying: images_split/train/mexican_hairless/n02113978_1088.jpg to images_split/train_10_percent/mexican_hairless/n02113978_1088.jpg\nimages_split/train_10_percent/entlebucher\nCopying: images_split/train/entlebucher/n02108000_918.jpg to images_split/train_10_percent/entlebucher/n02108000_918.jpg\nimages_split/train_10_percent/cardigan\nCopying: images_split/train/cardigan/n02113186_3049.jpg to images_split/train_10_percent/cardigan/n02113186_3049.jpg\nimages_split/train_10_percent/irish_wolfhound\nCopying: images_split/train/irish_wolfhound/n02090721_1002.jpg to images_split/train_10_percent/irish_wolfhound/n02090721_1002.jpg\nimages_split/train_10_percent/leonberg\nCopying: images_split/train/leonberg/n02111129_4001.jpg to images_split/train_10_percent/leonberg/n02111129_4001.jpg\nimages_split/train_10_percent/clumber\nCopying: images_split/train/clumber/n02101556_2918.jpg to images_split/train_10_percent/clumber/n02101556_2918.jpg\nimages_split/train_10_percent/ibizan_hound\nCopying: images_split/train/ibizan_hound/n02091244_430.jpg to images_split/train_10_percent/ibizan_hound/n02091244_430.jpg\nimages_split/train_10_percent/lhasa\nCopying: images_split/train/lhasa/n02098413_7163.jpg to images_split/train_10_percent/lhasa/n02098413_7163.jpg\nimages_split/train_10_percent/groenendael\nCopying: images_split/train/groenendael/n02105056_4770.jpg to images_split/train_10_percent/groenendael/n02105056_4770.jpg\nimages_split/train_10_percent/toy_terrier\nCopying: images_split/train/toy_terrier/n02087046_6166.jpg to images_split/train_10_percent/toy_terrier/n02087046_6166.jpg\nimages_split/train_10_percent/yorkshire_terrier\nCopying: images_split/train/yorkshire_terrier/n02094433_1219.jpg to images_split/train_10_percent/yorkshire_terrier/n02094433_1219.jpg\nimages_split/train_10_percent/dandie_dinmont\nCopying: images_split/train/dandie_dinmont/n02096437_708.jpg to images_split/train_10_percent/dandie_dinmont/n02096437_708.jpg\nimages_split/train_10_percent/lakeland_terrier\nCopying: images_split/train/lakeland_terrier/n02095570_889.jpg to images_split/train_10_percent/lakeland_terrier/n02095570_889.jpg\nimages_split/train_10_percent/bedlington_terrier\nCopying: images_split/train/bedlington_terrier/n02093647_3215.jpg to images_split/train_10_percent/bedlington_terrier/n02093647_3215.jpg\nimages_split/train_10_percent/papillon\nCopying: images_split/train/papillon/n02086910_2579.jpg to images_split/train_10_percent/papillon/n02086910_2579.jpg\nimages_split/train_10_percent/yorkshire_terrier\nCopying: images_split/train/yorkshire_terrier/n02094433_2375.jpg to images_split/train_10_percent/yorkshire_terrier/n02094433_2375.jpg\nimages_split/train_10_percent/boston_bull\nCopying: images_split/train/boston_bull/n02096585_342.jpg to images_split/train_10_percent/boston_bull/n02096585_342.jpg\nimages_split/train_10_percent/dhole\nCopying: images_split/train/dhole/n02115913_4188.jpg to images_split/train_10_percent/dhole/n02115913_4188.jpg\nimages_split/train_10_percent/cairn\nCopying: images_split/train/cairn/n02096177_1344.jpg to images_split/train_10_percent/cairn/n02096177_1344.jpg\nimages_split/train_10_percent/eskimo_dog\nCopying: images_split/train/eskimo_dog/n02109961_1076.jpg to images_split/train_10_percent/eskimo_dog/n02109961_1076.jpg\nimages_split/train_10_percent/west_highland_white_terrier\nCopying: images_split/train/west_highland_white_terrier/n02098286_6158.jpg to images_split/train_10_percent/west_highland_white_terrier/n02098286_6158.jpg\nimages_split/train_10_percent/standard_poodle\nCopying: images_split/train/standard_poodle/n02113799_6704.jpg to images_split/train_10_percent/standard_poodle/n02113799_6704.jpg\nimages_split/train_10_percent/boxer\nCopying: images_split/train/boxer/n02108089_1353.jpg to images_split/train_10_percent/boxer/n02108089_1353.jpg\nimages_split/train_10_percent/irish_terrier\nCopying: images_split/train/irish_terrier/n02093991_278.jpg to images_split/train_10_percent/irish_terrier/n02093991_278.jpg\nimages_split/train_10_percent/italian_greyhound\nCopying: images_split/train/italian_greyhound/n02091032_10688.jpg to images_split/train_10_percent/italian_greyhound/n02091032_10688.jpg\nimages_split/train_10_percent/ibizan_hound\nCopying: images_split/train/ibizan_hound/n02091244_2782.jpg to images_split/train_10_percent/ibizan_hound/n02091244_2782.jpg\nimages_split/train_10_percent/siberian_husky\nCopying: images_split/train/siberian_husky/n02110185_11396.jpg to images_split/train_10_percent/siberian_husky/n02110185_11396.jpg\nimages_split/train_10_percent/kelpie\nCopying: images_split/train/kelpie/n02105412_3979.jpg to images_split/train_10_percent/kelpie/n02105412_3979.jpg\nimages_split/train_10_percent/dandie_dinmont\nCopying: images_split/train/dandie_dinmont/n02096437_3969.jpg to images_split/train_10_percent/dandie_dinmont/n02096437_3969.jpg\nimages_split/train_10_percent/komondor\nCopying: images_split/train/komondor/n02105505_2322.jpg to images_split/train_10_percent/komondor/n02105505_2322.jpg\nimages_split/train_10_percent/toy_terrier\nCopying: images_split/train/toy_terrier/n02087046_4315.jpg to images_split/train_10_percent/toy_terrier/n02087046_4315.jpg\nimages_split/train_10_percent/french_bulldog\nCopying: images_split/train/french_bulldog/n02108915_5306.jpg to images_split/train_10_percent/french_bulldog/n02108915_5306.jpg\nimages_split/train_10_percent/black_and_tan_coonhound\nCopying: images_split/train/black_and_tan_coonhound/n02089078_1245.jpg to images_split/train_10_percent/black_and_tan_coonhound/n02089078_1245.jpg\nimages_split/train_10_percent/giant_schnauzer\nCopying: images_split/train/giant_schnauzer/n02097130_3396.jpg to images_split/train_10_percent/giant_schnauzer/n02097130_3396.jpg\nimages_split/train_10_percent/collie\nCopying: images_split/train/collie/n02106030_16029.jpg to images_split/train_10_percent/collie/n02106030_16029.jpg\nimages_split/train_10_percent/pomeranian\nCopying: images_split/train/pomeranian/n02112018_7953.jpg to images_split/train_10_percent/pomeranian/n02112018_7953.jpg\nimages_split/train_10_percent/boston_bull\nCopying: images_split/train/boston_bull/n02096585_11808.jpg to images_split/train_10_percent/boston_bull/n02096585_11808.jpg\nimages_split/train_10_percent/labrador_retriever\nCopying: images_split/train/labrador_retriever/n02099712_3776.jpg to images_split/train_10_percent/labrador_retriever/n02099712_3776.jpg\nimages_split/train_10_percent/curly_coated_retriever\nCopying: images_split/train/curly_coated_retriever/n02099429_3024.jpg to images_split/train_10_percent/curly_coated_retriever/n02099429_3024.jpg\nimages_split/train_10_percent/french_bulldog\nCopying: images_split/train/french_bulldog/n02108915_759.jpg to images_split/train_10_percent/french_bulldog/n02108915_759.jpg\nimages_split/train_10_percent/german_shepherd\nCopying: images_split/train/german_shepherd/n02106662_19720.jpg to images_split/train_10_percent/german_shepherd/n02106662_19720.jpg\nimages_split/train_10_percent/shih_tzu\nCopying: images_split/train/shih_tzu/n02086240_6116.jpg to images_split/train_10_percent/shih_tzu/n02086240_6116.jpg\nimages_split/train_10_percent/cairn\nCopying: images_split/train/cairn/n02096177_1207.jpg to images_split/train_10_percent/cairn/n02096177_1207.jpg\nimages_split/train_10_percent/shih_tzu\nCopying: images_split/train/shih_tzu/n02086240_2710.jpg to images_split/train_10_percent/shih_tzu/n02086240_2710.jpg\nimages_split/train_10_percent/entlebucher\nCopying: images_split/train/entlebucher/n02108000_2962.jpg to images_split/train_10_percent/entlebucher/n02108000_2962.jpg\nimages_split/train_10_percent/labrador_retriever\nCopying: images_split/train/labrador_retriever/n02099712_7815.jpg to images_split/train_10_percent/labrador_retriever/n02099712_7815.jpg\nimages_split/train_10_percent/cocker_spaniel\nCopying: images_split/train/cocker_spaniel/n02102318_11769.jpg to images_split/train_10_percent/cocker_spaniel/n02102318_11769.jpg\nimages_split/train_10_percent/miniature_schnauzer\nCopying: images_split/train/miniature_schnauzer/n02097047_271.jpg to images_split/train_10_percent/miniature_schnauzer/n02097047_271.jpg\nimages_split/train_10_percent/bluetick\nCopying: images_split/train/bluetick/n02088632_4210.jpg to images_split/train_10_percent/bluetick/n02088632_4210.jpg\nimages_split/train_10_percent/eskimo_dog\nCopying: images_split/train/eskimo_dog/n02109961_3810.jpg to images_split/train_10_percent/eskimo_dog/n02109961_3810.jpg\nimages_split/train_10_percent/leonberg\nCopying: images_split/train/leonberg/n02111129_2617.jpg to images_split/train_10_percent/leonberg/n02111129_2617.jpg\nimages_split/train_10_percent/english_foxhound\nCopying: images_split/train/english_foxhound/n02089973_4185.jpg to images_split/train_10_percent/english_foxhound/n02089973_4185.jpg\nimages_split/train_10_percent/staffordshire_bullterrier\nCopying: images_split/train/staffordshire_bullterrier/n02093256_2073.jpg to images_split/train_10_percent/staffordshire_bullterrier/n02093256_2073.jpg\nimages_split/train_10_percent/keeshond\nCopying: images_split/train/keeshond/n02112350_2709.jpg to images_split/train_10_percent/keeshond/n02112350_2709.jpg\nimages_split/train_10_percent/newfoundland\nCopying: images_split/train/newfoundland/n02111277_5964.jpg to images_split/train_10_percent/newfoundland/n02111277_5964.jpg\nimages_split/train_10_percent/beagle\nCopying: images_split/train/beagle/n02088364_5716.jpg to images_split/train_10_percent/beagle/n02088364_5716.jpg\nimages_split/train_10_percent/toy_poodle\nCopying: images_split/train/toy_poodle/n02113624_3796.jpg to images_split/train_10_percent/toy_poodle/n02113624_3796.jpg\nimages_split/train_10_percent/basenji\nCopying: images_split/train/basenji/n02110806_4122.jpg to images_split/train_10_percent/basenji/n02110806_4122.jpg\nimages_split/train_10_percent/malamute\nCopying: images_split/train/malamute/n02110063_15110.jpg to images_split/train_10_percent/malamute/n02110063_15110.jpg\nimages_split/train_10_percent/borzoi\nCopying: images_split/train/borzoi/n02090622_693.jpg to images_split/train_10_percent/borzoi/n02090622_693.jpg\nimages_split/train_10_percent/wire_haired_fox_terrier\nCopying: images_split/train/wire_haired_fox_terrier/n02095314_956.jpg to images_split/train_10_percent/wire_haired_fox_terrier/n02095314_956.jpg\nimages_split/train_10_percent/boxer\nCopying: images_split/train/boxer/n02108089_5301.jpg to images_split/train_10_percent/boxer/n02108089_5301.jpg\nimages_split/train_10_percent/saint_bernard\nCopying: images_split/train/saint_bernard/n02109525_11444.jpg to images_split/train_10_percent/saint_bernard/n02109525_11444.jpg\nimages_split/train_10_percent/boxer\nCopying: images_split/train/boxer/n02108089_125.jpg to images_split/train_10_percent/boxer/n02108089_125.jpg\nimages_split/train_10_percent/toy_terrier\nCopying: images_split/train/toy_terrier/n02087046_3211.jpg to images_split/train_10_percent/toy_terrier/n02087046_3211.jpg\nimages_split/train_10_percent/blenheim_spaniel\nCopying: images_split/train/blenheim_spaniel/n02086646_421.jpg to images_split/train_10_percent/blenheim_spaniel/n02086646_421.jpg\nimages_split/train_10_percent/komondor\nCopying: images_split/train/komondor/n02105505_3792.jpg to images_split/train_10_percent/komondor/n02105505_3792.jpg\nimages_split/train_10_percent/pomeranian\nCopying: images_split/train/pomeranian/n02112018_2821.jpg to images_split/train_10_percent/pomeranian/n02112018_2821.jpg\nimages_split/train_10_percent/keeshond\nCopying: images_split/train/keeshond/n02112350_9545.jpg to images_split/train_10_percent/keeshond/n02112350_9545.jpg\nimages_split/train_10_percent/mexican_hairless\nCopying: images_split/train/mexican_hairless/n02113978_3743.jpg to images_split/train_10_percent/mexican_hairless/n02113978_3743.jpg\nimages_split/train_10_percent/borzoi\nCopying: images_split/train/borzoi/n02090622_2688.jpg to images_split/train_10_percent/borzoi/n02090622_2688.jpg\nimages_split/train_10_percent/cairn\nCopying: images_split/train/cairn/n02096177_4902.jpg to images_split/train_10_percent/cairn/n02096177_4902.jpg\nimages_split/train_10_percent/silky_terrier\nCopying: images_split/train/silky_terrier/n02097658_595.jpg to images_split/train_10_percent/silky_terrier/n02097658_595.jpg\nimages_split/train_10_percent/eskimo_dog\nCopying: images_split/train/eskimo_dog/n02109961_4369.jpg to images_split/train_10_percent/eskimo_dog/n02109961_4369.jpg\nimages_split/train_10_percent/sussex_spaniel\nCopying: images_split/train/sussex_spaniel/n02102480_5706.jpg to images_split/train_10_percent/sussex_spaniel/n02102480_5706.jpg\nimages_split/train_10_percent/gordon_setter\nCopying: images_split/train/gordon_setter/n02101006_2629.jpg to images_split/train_10_percent/gordon_setter/n02101006_2629.jpg\nimages_split/train_10_percent/pekinese\nCopying: images_split/train/pekinese/n02086079_2893.jpg to images_split/train_10_percent/pekinese/n02086079_2893.jpg\nimages_split/train_10_percent/bull_mastiff\nCopying: images_split/train/bull_mastiff/n02108422_2379.jpg to images_split/train_10_percent/bull_mastiff/n02108422_2379.jpg\nimages_split/train_10_percent/staffordshire_bullterrier\nCopying: images_split/train/staffordshire_bullterrier/n02093256_3872.jpg to images_split/train_10_percent/staffordshire_bullterrier/n02093256_3872.jpg\nimages_split/train_10_percent/lhasa\nCopying: images_split/train/lhasa/n02098413_1279.jpg to images_split/train_10_percent/lhasa/n02098413_1279.jpg\nimages_split/train_10_percent/basenji\nCopying: images_split/train/basenji/n02110806_3981.jpg to images_split/train_10_percent/basenji/n02110806_3981.jpg\nimages_split/train_10_percent/italian_greyhound\nCopying: images_split/train/italian_greyhound/n02091032_722.jpg to images_split/train_10_percent/italian_greyhound/n02091032_722.jpg\nimages_split/train_10_percent/boxer\nCopying: images_split/train/boxer/n02108089_15702.jpg to images_split/train_10_percent/boxer/n02108089_15702.jpg\nimages_split/train_10_percent/pembroke\nCopying: images_split/train/pembroke/n02113023_7316.jpg to images_split/train_10_percent/pembroke/n02113023_7316.jpg\nimages_split/train_10_percent/whippet\nCopying: images_split/train/whippet/n02091134_14567.jpg to images_split/train_10_percent/whippet/n02091134_14567.jpg\nimages_split/train_10_percent/norwich_terrier\nCopying: images_split/train/norwich_terrier/n02094258_530.jpg to images_split/train_10_percent/norwich_terrier/n02094258_530.jpg\nimages_split/train_10_percent/flat_coated_retriever\nCopying: images_split/train/flat_coated_retriever/n02099267_1272.jpg to images_split/train_10_percent/flat_coated_retriever/n02099267_1272.jpg\nimages_split/train_10_percent/sussex_spaniel\nCopying: images_split/train/sussex_spaniel/n02102480_5158.jpg to images_split/train_10_percent/sussex_spaniel/n02102480_5158.jpg\nimages_split/train_10_percent/malinois\nCopying: images_split/train/malinois/n02105162_5600.jpg to images_split/train_10_percent/malinois/n02105162_5600.jpg\nimages_split/train_10_percent/greater_swiss_mountain_dog\nCopying: images_split/train/greater_swiss_mountain_dog/n02107574_2436.jpg to images_split/train_10_percent/greater_swiss_mountain_dog/n02107574_2436.jpg\nimages_split/train_10_percent/boston_bull\nCopying: images_split/train/boston_bull/n02096585_7542.jpg to images_split/train_10_percent/boston_bull/n02096585_7542.jpg\nimages_split/train_10_percent/brittany_spaniel\nCopying: images_split/train/brittany_spaniel/n02101388_6065.jpg to images_split/train_10_percent/brittany_spaniel/n02101388_6065.jpg\nimages_split/train_10_percent/sussex_spaniel\nCopying: images_split/train/sussex_spaniel/n02102480_6047.jpg to images_split/train_10_percent/sussex_spaniel/n02102480_6047.jpg\nimages_split/train_10_percent/cocker_spaniel\nCopying: images_split/train/cocker_spaniel/n02102318_9452.jpg to images_split/train_10_percent/cocker_spaniel/n02102318_9452.jpg\nimages_split/train_10_percent/black_and_tan_coonhound\nCopying: images_split/train/black_and_tan_coonhound/n02089078_3893.jpg to images_split/train_10_percent/black_and_tan_coonhound/n02089078_3893.jpg\nimages_split/train_10_percent/chow\nCopying: images_split/train/chow/n02112137_1830.jpg to images_split/train_10_percent/chow/n02112137_1830.jpg\nimages_split/train_10_percent/otterhound\nCopying: images_split/train/otterhound/n02091635_2112.jpg to images_split/train_10_percent/otterhound/n02091635_2112.jpg\nimages_split/train_10_percent/standard_schnauzer\nCopying: images_split/train/standard_schnauzer/n02097209_1092.jpg to images_split/train_10_percent/standard_schnauzer/n02097209_1092.jpg\nimages_split/train_10_percent/australian_terrier\nCopying: images_split/train/australian_terrier/n02096294_1409.jpg to images_split/train_10_percent/australian_terrier/n02096294_1409.jpg\nimages_split/train_10_percent/irish_wolfhound\nCopying: images_split/train/irish_wolfhound/n02090721_3302.jpg to images_split/train_10_percent/irish_wolfhound/n02090721_3302.jpg\nimages_split/train_10_percent/west_highland_white_terrier\nCopying: images_split/train/west_highland_white_terrier/n02098286_3154.jpg to images_split/train_10_percent/west_highland_white_terrier/n02098286_3154.jpg\nimages_split/train_10_percent/english_setter\nCopying: images_split/train/english_setter/n02100735_10086.jpg to images_split/train_10_percent/english_setter/n02100735_10086.jpg\nimages_split/train_10_percent/pug\nCopying: images_split/train/pug/n02110958_15217.jpg to images_split/train_10_percent/pug/n02110958_15217.jpg\nimages_split/train_10_percent/rottweiler\nCopying: images_split/train/rottweiler/n02106550_4217.jpg to images_split/train_10_percent/rottweiler/n02106550_4217.jpg\nimages_split/train_10_percent/collie\nCopying: images_split/train/collie/n02106030_18516.jpg to images_split/train_10_percent/collie/n02106030_18516.jpg\nimages_split/train_10_percent/staffordshire_bullterrier\nCopying: images_split/train/staffordshire_bullterrier/n02093256_3582.jpg to images_split/train_10_percent/staffordshire_bullterrier/n02093256_3582.jpg\nimages_split/train_10_percent/german_short_haired_pointer\nCopying: images_split/train/german_short_haired_pointer/n02100236_4181.jpg to images_split/train_10_percent/german_short_haired_pointer/n02100236_4181.jpg\nimages_split/train_10_percent/yorkshire_terrier\nCopying: images_split/train/yorkshire_terrier/n02094433_4248.jpg to images_split/train_10_percent/yorkshire_terrier/n02094433_4248.jpg\nimages_split/train_10_percent/boxer\nCopying: images_split/train/boxer/n02108089_1775.jpg to images_split/train_10_percent/boxer/n02108089_1775.jpg\nimages_split/train_10_percent/miniature_schnauzer\nCopying: images_split/train/miniature_schnauzer/n02097047_674.jpg to images_split/train_10_percent/miniature_schnauzer/n02097047_674.jpg\nimages_split/train_10_percent/brabancon_griffon\nCopying: images_split/train/brabancon_griffon/n02112706_1970.jpg to images_split/train_10_percent/brabancon_griffon/n02112706_1970.jpg\nimages_split/train_10_percent/italian_greyhound\nCopying: images_split/train/italian_greyhound/n02091032_9613.jpg to images_split/train_10_percent/italian_greyhound/n02091032_9613.jpg\nimages_split/train_10_percent/dandie_dinmont\nCopying: images_split/train/dandie_dinmont/n02096437_1793.jpg to images_split/train_10_percent/dandie_dinmont/n02096437_1793.jpg\nimages_split/train_10_percent/curly_coated_retriever\nCopying: images_split/train/curly_coated_retriever/n02099429_1234.jpg to images_split/train_10_percent/curly_coated_retriever/n02099429_1234.jpg\nimages_split/train_10_percent/english_foxhound\nCopying: images_split/train/english_foxhound/n02089973_3040.jpg to images_split/train_10_percent/english_foxhound/n02089973_3040.jpg\nimages_split/train_10_percent/basenji\nCopying: images_split/train/basenji/n02110806_4966.jpg to images_split/train_10_percent/basenji/n02110806_4966.jpg\nimages_split/train_10_percent/affenpinscher\nCopying: images_split/train/affenpinscher/n02110627_2748.jpg to images_split/train_10_percent/affenpinscher/n02110627_2748.jpg\nimages_split/train_10_percent/whippet\nCopying: images_split/train/whippet/n02091134_11775.jpg to images_split/train_10_percent/whippet/n02091134_11775.jpg\nimages_split/train_10_percent/dingo\nCopying: images_split/train/dingo/n02115641_8871.jpg to images_split/train_10_percent/dingo/n02115641_8871.jpg\nimages_split/train_10_percent/boston_bull\nCopying: images_split/train/boston_bull/n02096585_3681.jpg to images_split/train_10_percent/boston_bull/n02096585_3681.jpg\nimages_split/train_10_percent/bloodhound\nCopying: images_split/train/bloodhound/n02088466_7015.jpg to images_split/train_10_percent/bloodhound/n02088466_7015.jpg\nimages_split/train_10_percent/dandie_dinmont\nCopying: images_split/train/dandie_dinmont/n02096437_1385.jpg to images_split/train_10_percent/dandie_dinmont/n02096437_1385.jpg\nimages_split/train_10_percent/gordon_setter\nCopying: images_split/train/gordon_setter/n02101006_283.jpg to images_split/train_10_percent/gordon_setter/n02101006_283.jpg\nimages_split/train_10_percent/whippet\nCopying: images_split/train/whippet/n02091134_39.jpg to images_split/train_10_percent/whippet/n02091134_39.jpg\nimages_split/train_10_percent/schipperke\nCopying: images_split/train/schipperke/n02104365_3587.jpg to images_split/train_10_percent/schipperke/n02104365_3587.jpg\nimages_split/train_10_percent/west_highland_white_terrier\nCopying: images_split/train/west_highland_white_terrier/n02098286_1255.jpg to images_split/train_10_percent/west_highland_white_terrier/n02098286_1255.jpg\nimages_split/train_10_percent/mexican_hairless\nCopying: images_split/train/mexican_hairless/n02113978_1325.jpg to images_split/train_10_percent/mexican_hairless/n02113978_1325.jpg\nimages_split/train_10_percent/pug\nCopying: images_split/train/pug/n02110958_1975.jpg to images_split/train_10_percent/pug/n02110958_1975.jpg\nimages_split/train_10_percent/cairn\nCopying: images_split/train/cairn/n02096177_4768.jpg to images_split/train_10_percent/cairn/n02096177_4768.jpg\nimages_split/train_10_percent/irish_terrier\nCopying: images_split/train/irish_terrier/n02093991_2126.jpg to images_split/train_10_percent/irish_terrier/n02093991_2126.jpg\nimages_split/train_10_percent/border_collie\nCopying: images_split/train/border_collie/n02106166_1637.jpg to images_split/train_10_percent/border_collie/n02106166_1637.jpg\nimages_split/train_10_percent/australian_terrier\nCopying: images_split/train/australian_terrier/n02096294_1299.jpg to images_split/train_10_percent/australian_terrier/n02096294_1299.jpg\nimages_split/train_10_percent/yorkshire_terrier\nCopying: images_split/train/yorkshire_terrier/n02094433_2987.jpg to images_split/train_10_percent/yorkshire_terrier/n02094433_2987.jpg\nimages_split/train_10_percent/scottish_deerhound\nCopying: images_split/train/scottish_deerhound/n02092002_9071.jpg to images_split/train_10_percent/scottish_deerhound/n02092002_9071.jpg\nimages_split/train_10_percent/entlebucher\nCopying: images_split/train/entlebucher/n02108000_1516.jpg to images_split/train_10_percent/entlebucher/n02108000_1516.jpg\nimages_split/train_10_percent/scottish_deerhound\nCopying: images_split/train/scottish_deerhound/n02092002_6963.jpg to images_split/train_10_percent/scottish_deerhound/n02092002_6963.jpg\nimages_split/train_10_percent/basset\nCopying: images_split/train/basset/n02088238_9778.jpg to images_split/train_10_percent/basset/n02088238_9778.jpg\nimages_split/train_10_percent/italian_greyhound\nCopying: images_split/train/italian_greyhound/n02091032_4653.jpg to images_split/train_10_percent/italian_greyhound/n02091032_4653.jpg\nimages_split/train_10_percent/saint_bernard\nCopying: images_split/train/saint_bernard/n02109525_2648.jpg to images_split/train_10_percent/saint_bernard/n02109525_2648.jpg\nimages_split/train_10_percent/otterhound\nCopying: images_split/train/otterhound/n02091635_1302.jpg to images_split/train_10_percent/otterhound/n02091635_1302.jpg\nimages_split/train_10_percent/pembroke\nCopying: images_split/train/pembroke/n02113023_6702.jpg to images_split/train_10_percent/pembroke/n02113023_6702.jpg\nimages_split/train_10_percent/boston_bull\nCopying: images_split/train/boston_bull/n02096585_657.jpg to images_split/train_10_percent/boston_bull/n02096585_657.jpg\nimages_split/train_10_percent/giant_schnauzer\nCopying: images_split/train/giant_schnauzer/n02097130_5175.jpg to images_split/train_10_percent/giant_schnauzer/n02097130_5175.jpg\nimages_split/train_10_percent/borzoi\nCopying: images_split/train/borzoi/n02090622_7705.jpg to images_split/train_10_percent/borzoi/n02090622_7705.jpg\nimages_split/train_10_percent/golden_retriever\nCopying: images_split/train/golden_retriever/n02099601_3569.jpg to images_split/train_10_percent/golden_retriever/n02099601_3569.jpg\nimages_split/train_10_percent/great_dane\nCopying: images_split/train/great_dane/n02109047_32010.jpg to images_split/train_10_percent/great_dane/n02109047_32010.jpg\nimages_split/train_10_percent/brittany_spaniel\nCopying: images_split/train/brittany_spaniel/n02101388_2064.jpg to images_split/train_10_percent/brittany_spaniel/n02101388_2064.jpg\nimages_split/train_10_percent/shih_tzu\nCopying: images_split/train/shih_tzu/n02086240_6082.jpg to images_split/train_10_percent/shih_tzu/n02086240_6082.jpg\nimages_split/train_10_percent/dhole\nCopying: images_split/train/dhole/n02115913_5392.jpg to images_split/train_10_percent/dhole/n02115913_5392.jpg\nimages_split/train_10_percent/standard_poodle\nCopying: images_split/train/standard_poodle/n02113799_5372.jpg to images_split/train_10_percent/standard_poodle/n02113799_5372.jpg\nimages_split/train_10_percent/mexican_hairless\nCopying: images_split/train/mexican_hairless/n02113978_3092.jpg to images_split/train_10_percent/mexican_hairless/n02113978_3092.jpg\nimages_split/train_10_percent/japanese_spaniel\nCopying: images_split/train/japanese_spaniel/n02085782_2939.jpg to images_split/train_10_percent/japanese_spaniel/n02085782_2939.jpg\nimages_split/train_10_percent/irish_water_spaniel\nCopying: images_split/train/irish_water_spaniel/n02102973_158.jpg to images_split/train_10_percent/irish_water_spaniel/n02102973_158.jpg\nimages_split/train_10_percent/appenzeller\nCopying: images_split/train/appenzeller/n02107908_7567.jpg to images_split/train_10_percent/appenzeller/n02107908_7567.jpg\nimages_split/train_10_percent/irish_setter\nCopying: images_split/train/irish_setter/n02100877_7519.jpg to images_split/train_10_percent/irish_setter/n02100877_7519.jpg\nimages_split/train_10_percent/whippet\nCopying: images_split/train/whippet/n02091134_1131.jpg to images_split/train_10_percent/whippet/n02091134_1131.jpg\nimages_split/train_10_percent/siberian_husky\nCopying: images_split/train/siberian_husky/n02110185_1469.jpg to images_split/train_10_percent/siberian_husky/n02110185_1469.jpg\nimages_split/train_10_percent/shih_tzu\nCopying: images_split/train/shih_tzu/n02086240_4127.jpg to images_split/train_10_percent/shih_tzu/n02086240_4127.jpg\nimages_split/train_10_percent/silky_terrier\nCopying: images_split/train/silky_terrier/n02097658_319.jpg to images_split/train_10_percent/silky_terrier/n02097658_319.jpg\nimages_split/train_10_percent/staffordshire_bullterrier\nCopying: images_split/train/staffordshire_bullterrier/n02093256_11836.jpg to images_split/train_10_percent/staffordshire_bullterrier/n02093256_11836.jpg\nimages_split/train_10_percent/collie\nCopying: images_split/train/collie/n02106030_7977.jpg to images_split/train_10_percent/collie/n02106030_7977.jpg\nimages_split/train_10_percent/maltese_dog\nCopying: images_split/train/maltese_dog/n02085936_233.jpg to images_split/train_10_percent/maltese_dog/n02085936_233.jpg\nimages_split/train_10_percent/airedale\nCopying: images_split/train/airedale/n02096051_5234.jpg to images_split/train_10_percent/airedale/n02096051_5234.jpg\n</pre> In\u00a0[\u00a0]: Copied! <pre># Count images in train_10_percent_dir\ntrain_10_percent_image_class_counts = count_images_in_subdirs(train_10_percent_dir)\ntrain_10_percent_image_class_counts_df = pd.DataFrame(train_10_percent_image_class_counts).sort_values(\"image_count\", ascending=True)\ntrain_10_percent_image_class_counts_df\n</pre> # Count images in train_10_percent_dir train_10_percent_image_class_counts = count_images_in_subdirs(train_10_percent_dir) train_10_percent_image_class_counts_df = pd.DataFrame(train_10_percent_image_class_counts).sort_values(\"image_count\", ascending=True) train_10_percent_image_class_counts_df Out[\u00a0]: class_name image_count 33 doberman 3 23 pembroke 4 61 brabancon_griffon 4 64 old_english_sheepdog 4 100 scotch_terrier 5 ... ... ... 40 briard 16 88 collie 16 11 greater_swiss_mountain_dog 17 43 cocker_spaniel 18 90 boxer 19 <p>120 rows \u00d7 2 columns</p> In\u00a0[\u00a0]: Copied! <pre># TK - turn the DataFrame above into a graph\nplt.figure(figsize=(14, 7))\ntrain_10_percent_image_class_counts_df.plot(kind=\"bar\",\n                     x=\"class_name\",\n                     y=\"image_count\",\n                     legend=False,\n                     ax=plt.gca()) # plt.gca() = \"get current axis\", get the plt we setup above and put the data there\n\n# Add customization\nplt.title(\"Train 10 Percent Image Counts by Class\")\nplt.ylabel(\"Image Count\")\nplt.xticks(rotation=90, # Rotate the x labels for better visibility\n           fontsize=8) # Make the font size smaller for easier reading\nplt.tight_layout() # Ensure things fit nicely\nplt.show()\n</pre> # TK - turn the DataFrame above into a graph plt.figure(figsize=(14, 7)) train_10_percent_image_class_counts_df.plot(kind=\"bar\",                      x=\"class_name\",                      y=\"image_count\",                      legend=False,                      ax=plt.gca()) # plt.gca() = \"get current axis\", get the plt we setup above and put the data there  # Add customization plt.title(\"Train 10 Percent Image Counts by Class\") plt.ylabel(\"Image Count\") plt.xticks(rotation=90, # Rotate the x labels for better visibility            fontsize=8) # Make the font size smaller for easier reading plt.tight_layout() # Ensure things fit nicely plt.show() In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\n\nimg_size = 224\nbatch_size = 32\n\ntrain_10_percent_ds = tf.keras.utils.image_dataset_from_directory(\n    directory=train_10_percent_dir,\n    batch_size=32,\n    image_size=(img_size, img_size),\n    shuffle=True,\n    seed=42\n)\n\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n    directory=train_dir,\n    batch_size=32,\n    image_size=(img_size, img_size),\n    shuffle=True,\n    seed=42\n)\n\ntest_ds = tf.keras.utils.image_dataset_from_directory(\n    directory=test_dir,\n    batch_size=32,\n    image_size=(img_size, img_size),\n    shuffle=False,\n    seed=42\n)\n</pre> import tensorflow as tf  img_size = 224 batch_size = 32  train_10_percent_ds = tf.keras.utils.image_dataset_from_directory(     directory=train_10_percent_dir,     batch_size=32,     image_size=(img_size, img_size),     shuffle=True,     seed=42 )  train_ds = tf.keras.utils.image_dataset_from_directory(     directory=train_dir,     batch_size=32,     image_size=(img_size, img_size),     shuffle=True,     seed=42 )  test_ds = tf.keras.utils.image_dataset_from_directory(     directory=test_dir,     batch_size=32,     image_size=(img_size, img_size),     shuffle=False,     seed=42 ) <pre>Found 1200 files belonging to 120 classes.\nFound 12000 files belonging to 120 classes.\nFound 8580 files belonging to 120 classes.\n</pre> In\u00a0[\u00a0]: Copied! <pre>assert set(train_10_percent_ds.class_names) == set(train_ds.class_names) == set(test_ds.class_names)\n</pre> assert set(train_10_percent_ds.class_names) == set(train_ds.class_names) == set(test_ds.class_names) In\u00a0[\u00a0]: Copied! <pre>class_names = train_ds.class_names\nclass_names[:5]\n</pre> class_names = train_ds.class_names class_names[:5] Out[\u00a0]: <pre>['affenpinscher',\n 'afghan_hound',\n 'african_hunting_dog',\n 'airedale',\n 'american_staffordshire_terrier']</pre> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\n# TK - change this to a similar version of the above plotting function\nplt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\n    plt.title(class_names[labels[i]])\n    plt.axis(\"off\")\n</pre> import matplotlib.pyplot as plt  # TK - change this to a similar version of the above plotting function plt.figure(figsize=(10, 10)) for images, labels in train_ds.take(1):   for i in range(9):     ax = plt.subplot(3, 3, i + 1)     plt.imshow(images[i].numpy().astype(\"uint8\"))     plt.title(class_names[labels[i]])     plt.axis(\"off\") In\u00a0[\u00a0]: Copied! <pre># TK - what does a single image look like?\nimage_batch, label_batch = next(iter(train_ds))\nimage_batch.shape, label_batch.shape\n</pre> # TK - what does a single image look like? image_batch, label_batch = next(iter(train_ds)) image_batch.shape, label_batch.shape Out[\u00a0]: <pre>(TensorShape([32, 224, 224, 3]), TensorShape([32]))</pre> In\u00a0[\u00a0]: Copied! <pre>image_batch[0], label_batch[0], class_names[label_batch[0]]\n</pre> image_batch[0], label_batch[0], class_names[label_batch[0]] Out[\u00a0]: <pre>(&lt;tf.Tensor: shape=(224, 224, 3), dtype=float32, numpy=\n array([[[ 95.33705 , 149.33705 , 213.33705 ],\n         [ 95.33705 , 149.33705 , 213.33705 ],\n         [ 95.33705 , 149.33705 , 213.33705 ],\n         ...,\n         [102.33705 , 154.33705 , 214.33705 ],\n         [102.33705 , 152.33705 , 213.33705 ],\n         [102.33705 , 152.33705 , 213.33705 ]],\n \n        [[ 96.      , 150.      , 214.      ],\n         [ 96.      , 150.      , 214.      ],\n         [ 96.      , 150.      , 214.      ],\n         ...,\n         [103.      , 155.      , 215.      ],\n         [103.      , 153.      , 214.      ],\n         [103.      , 153.      , 214.      ]],\n \n        [[ 96.685265, 150.68527 , 214.68527 ],\n         [ 96.685265, 150.68527 , 214.68527 ],\n         [ 96.685265, 150.68527 , 214.68527 ],\n         ...,\n         [103.685265, 155.68527 , 215.68527 ],\n         [103.685265, 153.68527 , 214.68527 ],\n         [103.685265, 153.68527 , 214.68527 ]],\n \n        ...,\n \n        [[ 98.23341 , 113.76687 ,  63.121964],\n         [148.32869 , 175.53044 , 116.02563 ],\n         [103.368706, 137.75781 ,  75.07858 ],\n         ...,\n         [100.0989  , 116.259605,  67.17925 ],\n         [102.490685, 123.42379 ,  74.376015],\n         [ 95.4561  , 117.6978  ,  68.57695 ]],\n \n        [[141.0309  , 154.13097 , 114.59556 ],\n         [113.637314, 139.43416 ,  87.43759 ],\n         [ 89.02714 , 123.854385,  66.06294 ],\n         ...,\n         [ 56.74781 ,  74.727325,  24.896484],\n         [113.408585, 137.84    ,  85.72163 ],\n         [ 97.09375 , 122.219025,  69.86849 ]],\n \n        [[ 55.66023 ,  65.21129 ,  39.81113 ],\n         [ 96.22576 , 121.01147 ,  76.0117  ],\n         [ 48.16883 ,  83.11556 ,  28.655476],\n         ...,\n         [ 74.006485,  92.11303 ,  43.439846],\n         [ 80.433014, 107.080246,  53.912422],\n         [105.41931 , 134.51312 ,  79.14029 ]]], dtype=float32)&gt;,\n &lt;tf.Tensor: shape=(), dtype=int32, numpy=41&gt;,\n 'english_springer')</pre> In\u00a0[\u00a0]: Copied! <pre>AUTOTUNE = tf.data.AUTOTUNE\n\n# Shuffle training datasets but don't need to shuffle test datasets (for easier evaluation)\ntrain_10_percent_ds = train_10_percent_ds.cache().shuffle(100).prefetch(buffer_size=AUTOTUNE)\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n</pre> AUTOTUNE = tf.data.AUTOTUNE  # Shuffle training datasets but don't need to shuffle test datasets (for easier evaluation) train_10_percent_ds = train_10_percent_ds.cache().shuffle(100).prefetch(buffer_size=AUTOTUNE) train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE) test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE) In\u00a0[\u00a0]: Copied! <pre>num_classes = len(class_names)\n</pre> num_classes = len(class_names) In\u00a0[\u00a0]: Copied! <pre>base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(\n    include_top=False,\n    weights=\"imagenet\",\n    input_shape=(img_size, img_size, 3),\n    include_preprocessing=True\n)\n\n# base_model.summary()\n\n# Freeze the base model\nbase_model.trainable = False\nbase_model.trainable\n</pre> base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(     include_top=False,     weights=\"imagenet\",     input_shape=(img_size, img_size, 3),     include_preprocessing=True )  # base_model.summary()  # Freeze the base model base_model.trainable = False base_model.trainable <pre>Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b0_notop.h5\n24274472/24274472 [==============================] - 1s 0us/step\n</pre> Out[\u00a0]: <pre>False</pre> In\u00a0[\u00a0]: Copied! <pre># TK - functionize this model creation step\n\n# Create new model\ninputs = tf.keras.Input(shape=(224, 224, 3))\n\n# TK - Create data augmentation\n# x = data_augmentation(inputs)\n\n# Craft model\nx = base_model(inputs, training=False)\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = tf.keras.layers.Dropout(0.2)(x)\noutputs = tf.keras.layers.Dense(units=num_classes,\n                                activation=\"softmax\", # TK - which activation function should you use?\n                                name=\"output_layer\")(x)\nmodel_0 = tf.keras.Model(inputs, outputs, name=\"model_0\")\nmodel_0.summary()\n</pre> # TK - functionize this model creation step  # Create new model inputs = tf.keras.Input(shape=(224, 224, 3))  # TK - Create data augmentation # x = data_augmentation(inputs)  # Craft model x = base_model(inputs, training=False) x = tf.keras.layers.GlobalAveragePooling2D()(x) x = tf.keras.layers.Dropout(0.2)(x) outputs = tf.keras.layers.Dense(units=num_classes,                                 activation=\"softmax\", # TK - which activation function should you use?                                 name=\"output_layer\")(x) model_0 = tf.keras.Model(inputs, outputs, name=\"model_0\") model_0.summary() <pre>Model: \"model_0\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n                                                                 \n efficientnetv2-b0 (Functio  (None, 7, 7, 1280)        5919312   \n nal)                                                            \n                                                                 \n global_average_pooling2d (  (None, 1280)              0         \n GlobalAveragePooling2D)                                         \n                                                                 \n dropout (Dropout)           (None, 1280)              0         \n                                                                 \n output_layer (Dense)        (None, 120)               153720    \n                                                                 \n=================================================================\nTotal params: 6073032 (23.17 MB)\nTrainable params: 153720 (600.47 KB)\nNon-trainable params: 5919312 (22.58 MB)\n_________________________________________________________________\n</pre> In\u00a0[\u00a0]: Copied! <pre># Functionize model creation\nfrom typing import Tuple\n\ndef create_model(include_top: bool = False,\n                 num_classes: int = 1000,\n                 input_shape: Tuple[int, int, int] = (224, 224, 3),\n                 include_preprocessing: bool = True,\n                 trainable: bool = False,\n                 dropout: float = 0.2,\n                 model_name: str = \"model\") -&gt; tf.keras.Model:\n  \"\"\"\n  Create an EfficientNetV2 B0 feature extractor model with a custom classifier layer.\n\n  Args:\n      include_top (bool, optional): Whether to include the top (classifier) layers of the model.\n      num_classes (int, optional): Number of output classes for the classifier layer.\n      input_shape (Tuple[int, int, int], optional): Input shape for the model's images (height, width, channels).\n      include_preprocessing (bool, optional): Whether to include preprocessing layers for image normalization.\n      trainable (bool, optional): Whether to make the base model trainable.\n      dropout (float, optional): Dropout rate for the global average pooling layer.\n      model_name (str, optional): Name for the created model.\n\n  Returns:\n      tf.keras.Model: A TensorFlow Keras model with the specified configuration.\n  \"\"\"\n  # Create base model\n  base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(\n    include_top=include_top,\n    weights=\"imagenet\",\n    input_shape=input_shape,\n    include_preprocessing=include_preprocessing\n  )\n\n  # Freeze the base model (if necessary)\n  base_model.trainable = trainable\n\n  # Create input layer\n  inputs = tf.keras.Input(shape=input_shape, name=\"input_layer\")\n\n  # Create model backbone\n  x = base_model(inputs, training=trainable)\n  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n  x = tf.keras.layers.Dropout(0.2)(x)\n\n  # Create output layer (also known as \"classifier\" layer)\n  outputs = tf.keras.layers.Dense(units=num_classes,\n                                  activation=\"softmax\",\n                                  name=\"output_layer\")(x)\n\n  # Connect input and output layer\n  model = tf.keras.Model(inputs,\n                         outputs,\n                         name=model_name)\n\n  return model\n\nmodel_0 = create_model(num_classes=len(class_names))\nmodel_0.summary()\n</pre> # Functionize model creation from typing import Tuple  def create_model(include_top: bool = False,                  num_classes: int = 1000,                  input_shape: Tuple[int, int, int] = (224, 224, 3),                  include_preprocessing: bool = True,                  trainable: bool = False,                  dropout: float = 0.2,                  model_name: str = \"model\") -&gt; tf.keras.Model:   \"\"\"   Create an EfficientNetV2 B0 feature extractor model with a custom classifier layer.    Args:       include_top (bool, optional): Whether to include the top (classifier) layers of the model.       num_classes (int, optional): Number of output classes for the classifier layer.       input_shape (Tuple[int, int, int], optional): Input shape for the model's images (height, width, channels).       include_preprocessing (bool, optional): Whether to include preprocessing layers for image normalization.       trainable (bool, optional): Whether to make the base model trainable.       dropout (float, optional): Dropout rate for the global average pooling layer.       model_name (str, optional): Name for the created model.    Returns:       tf.keras.Model: A TensorFlow Keras model with the specified configuration.   \"\"\"   # Create base model   base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(     include_top=include_top,     weights=\"imagenet\",     input_shape=input_shape,     include_preprocessing=include_preprocessing   )    # Freeze the base model (if necessary)   base_model.trainable = trainable    # Create input layer   inputs = tf.keras.Input(shape=input_shape, name=\"input_layer\")    # Create model backbone   x = base_model(inputs, training=trainable)   x = tf.keras.layers.GlobalAveragePooling2D()(x)   x = tf.keras.layers.Dropout(0.2)(x)    # Create output layer (also known as \"classifier\" layer)   outputs = tf.keras.layers.Dense(units=num_classes,                                   activation=\"softmax\",                                   name=\"output_layer\")(x)    # Connect input and output layer   model = tf.keras.Model(inputs,                          outputs,                          name=model_name)    return model  model_0 = create_model(num_classes=len(class_names)) model_0.summary() <pre>Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n                                                                 \n efficientnetv2-b0 (Functio  (None, 7, 7, 1280)        5919312   \n nal)                                                            \n                                                                 \n global_average_pooling2d_1  (None, 1280)              0         \n  (GlobalAveragePooling2D)                                       \n                                                                 \n dropout_1 (Dropout)         (None, 1280)              0         \n                                                                 \n output_layer (Dense)        (None, 120)               153720    \n                                                                 \n=================================================================\nTotal params: 6073032 (23.17 MB)\nTrainable params: 153720 (600.47 KB)\nNon-trainable params: 5919312 (22.58 MB)\n_________________________________________________________________\n</pre> <ul> <li>TK - turn this into a table</li> <li>TK - add docs for compile + fit + evaluate + predict</li> </ul> <p>A note on losses:</p> <ul> <li>If your labels are integers (e.g. <code>[[1], [23], [43], [16]...]</code>), use <code>tf.keras.losses.SparseCategoricalCrossentropy()</code></li> <li>If your labels are one-hot encoded (e.g. <code>[0, 1, 0, 0, 1, 0...]</code>), use <code>tf.keras.losses.CategoricalCrossEntropy()</code></li> </ul> In\u00a0[\u00a0]: Copied! <pre># Create model\nmodel_0 = create_model(num_classes=len(class_names),\n                       model_name=\"model_0\")\n\n# Compile model\nmodel_0.compile(optimizer=\"adam\", # or can use tf.keras.optimizers.Adam()\n                # Set from_logits=True if the last layer of your net is a Dense layer with no activation\n                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n                metrics=[\"accuracy\"],\n                steps_per_execution=\"auto\")# new in TensorFlow 2.14.0\n\n# Fit model\nepochs=5\nhistory_0 = model_0.fit(\n  train_10_percent_ds,\n  validation_data=test_ds,\n  epochs=epochs)\n</pre> # Create model model_0 = create_model(num_classes=len(class_names),                        model_name=\"model_0\")  # Compile model model_0.compile(optimizer=\"adam\", # or can use tf.keras.optimizers.Adam()                 # Set from_logits=True if the last layer of your net is a Dense layer with no activation                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),                 metrics=[\"accuracy\"],                 steps_per_execution=\"auto\")# new in TensorFlow 2.14.0  # Fit model epochs=5 history_0 = model_0.fit(   train_10_percent_ds,   validation_data=test_ds,   epochs=epochs) <pre>Epoch 1/5\n38/38 [==============================] - 25s 259ms/step - loss: 4.1107 - accuracy: 0.2150 - val_loss: 3.2047 - val_accuracy: 0.5185\nEpoch 2/5\n38/38 [==============================] - 7s 184ms/step - loss: 2.3102 - accuracy: 0.7217 - val_loss: 2.0488 - val_accuracy: 0.6721\nEpoch 3/5\n38/38 [==============================] - 7s 184ms/step - loss: 1.2899 - accuracy: 0.8500 - val_loss: 1.4266 - val_accuracy: 0.7488\nEpoch 4/5\n38/38 [==============================] - 7s 181ms/step - loss: 0.7874 - accuracy: 0.9158 - val_loss: 1.1094 - val_accuracy: 0.7873\nEpoch 5/5\n38/38 [==============================] - 7s 180ms/step - loss: 0.5392 - accuracy: 0.9467 - val_loss: 0.9386 - val_accuracy: 0.8014\n</pre> In\u00a0[\u00a0]: Copied! <pre>def plot_model_loss_curves(history):\n  acc = history.history['accuracy']\n  val_acc = history.history['val_accuracy']\n\n  loss = history.history['loss']\n  val_loss = history.history['val_loss']\n\n  epochs_range = range(epochs)\n\n  plt.figure(figsize=(14, 7))\n  plt.subplot(1, 2, 1)\n  plt.plot(epochs_range, acc, label='Training Accuracy')\n  plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n  plt.legend(loc='lower right')\n  plt.title('Training and Validation Accuracy')\n\n  plt.subplot(1, 2, 2)\n  plt.plot(epochs_range, loss, label='Training Loss')\n  plt.plot(epochs_range, val_loss, label='Validation Loss')\n  plt.legend(loc='upper right')\n  plt.title('Training and Validation Loss')\n  plt.show()\n\nplot_model_loss_curves(history=history_0)\n</pre> def plot_model_loss_curves(history):   acc = history.history['accuracy']   val_acc = history.history['val_accuracy']    loss = history.history['loss']   val_loss = history.history['val_loss']    epochs_range = range(epochs)    plt.figure(figsize=(14, 7))   plt.subplot(1, 2, 1)   plt.plot(epochs_range, acc, label='Training Accuracy')   plt.plot(epochs_range, val_acc, label='Validation Accuracy')   plt.legend(loc='lower right')   plt.title('Training and Validation Accuracy')    plt.subplot(1, 2, 2)   plt.plot(epochs_range, loss, label='Training Loss')   plt.plot(epochs_range, val_loss, label='Validation Loss')   plt.legend(loc='upper right')   plt.title('Training and Validation Loss')   plt.show()  plot_model_loss_curves(history=history_0) In\u00a0[\u00a0]: Copied! <pre># Evaluate model_0, see: https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate\nmodel_0_results = model_0.evaluate(test_ds)\n</pre> # Evaluate model_0, see: https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate model_0_results = model_0.evaluate(test_ds) <pre>269/269 [==============================] - 6s 23ms/step - loss: 0.9386 - accuracy: 0.8014\n</pre> In\u00a0[\u00a0]: Copied! <pre>model_1 = create_model(num_classes=len(class_names),\n                       model_name=\"model_1\")\nmodel_1.summary()\n</pre> model_1 = create_model(num_classes=len(class_names),                        model_name=\"model_1\") model_1.summary() <pre>Model: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n                                                                 \n efficientnetv2-b0 (Functio  (None, 7, 7, 1280)        5919312   \n nal)                                                            \n                                                                 \n global_average_pooling2d_3  (None, 1280)              0         \n  (GlobalAveragePooling2D)                                       \n                                                                 \n dropout_3 (Dropout)         (None, 1280)              0         \n                                                                 \n output_layer (Dense)        (None, 120)               153720    \n                                                                 \n=================================================================\nTotal params: 6073032 (23.17 MB)\nTrainable params: 153720 (600.47 KB)\nNon-trainable params: 5919312 (22.58 MB)\n_________________________________________________________________\n</pre> In\u00a0[\u00a0]: Copied! <pre># Compile the model\nmodel_1.compile(optimizer=\"adam\",\n                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), # if no activation function at end of model, use \"from_logits=True\"\n                metrics=[\"accuracy\"])\n\n# Train the model\ntf.keras.utils.set_random_seed(42)\nhistory_1 = model_1.fit(train_ds,\n                        epochs=epochs,\n                        validation_data=test_ds)\n</pre> # Compile the model model_1.compile(optimizer=\"adam\",                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), # if no activation function at end of model, use \"from_logits=True\"                 metrics=[\"accuracy\"])  # Train the model tf.keras.utils.set_random_seed(42) history_1 = model_1.fit(train_ds,                         epochs=epochs,                         validation_data=test_ds) <pre>Epoch 1/5\n375/375 [==============================] - 27s 44ms/step - loss: 1.3750 - accuracy: 0.7448 - val_loss: 0.5082 - val_accuracy: 0.8716\nEpoch 2/5\n375/375 [==============================] - 15s 41ms/step - loss: 0.4004 - accuracy: 0.8932 - val_loss: 0.4060 - val_accuracy: 0.8809\nEpoch 3/5\n375/375 [==============================] - 15s 41ms/step - loss: 0.2884 - accuracy: 0.9206 - val_loss: 0.3812 - val_accuracy: 0.8817\nEpoch 4/5\n375/375 [==============================] - 15s 40ms/step - loss: 0.2308 - accuracy: 0.9369 - val_loss: 0.3724 - val_accuracy: 0.8841\nEpoch 5/5\n375/375 [==============================] - 15s 39ms/step - loss: 0.1896 - accuracy: 0.9482 - val_loss: 0.3617 - val_accuracy: 0.8829\n</pre> <p>TK - How many epochs should I fit for? ... generally with transfer learning you can get pretty good results quite quickly, however, you may want to look into training for longer, e.g. more epochs, fine-tuning the whole model and using callbacks such as Early Stopping to prevent the model from training for too long</p> In\u00a0[\u00a0]: Copied! <pre>plot_model_loss_curves(history=history_1)\n</pre> plot_model_loss_curves(history=history_1) In\u00a0[\u00a0]: Copied! <pre># Evaluate model_1\nmodel_1_results = model_1.evaluate(test_ds)\n</pre> # Evaluate model_1 model_1_results = model_1.evaluate(test_ds) <pre>269/269 [==============================] - 6s 23ms/step - loss: 0.3617 - accuracy: 0.8829\n</pre> In\u00a0[\u00a0]: Copied! <pre># This will output logits (as long as softmax activation isn't in the model)\ntest_preds = model_1.predict(test_ds)\n\n# Note: If not using activation=\"softmax\" in last layer of model, may need to turn them into prediction probabilities (easier to understand)\n# test_preds = tf.keras.activations.softmax(tf.constant(test_preds), axis=-1)\n</pre> # This will output logits (as long as softmax activation isn't in the model) test_preds = model_1.predict(test_ds)  # Note: If not using activation=\"softmax\" in last layer of model, may need to turn them into prediction probabilities (easier to understand) # test_preds = tf.keras.activations.softmax(tf.constant(test_preds), axis=-1) <pre>269/269 [==============================] - 7s 21ms/step\n</pre> In\u00a0[\u00a0]: Copied! <pre>test_preds.shape\n</pre> test_preds.shape Out[\u00a0]: <pre>(8580, 120)</pre> In\u00a0[\u00a0]: Copied! <pre>test_preds[0].shape, tf.argmax(test_preds[0])\n</pre> test_preds[0].shape, tf.argmax(test_preds[0]) Out[\u00a0]: <pre>((120,), &lt;tf.Tensor: shape=(), dtype=int64, numpy=0&gt;)</pre> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\ntest_ds_images = np.concatenate([images for images, labels in test_ds], axis=0)\ntest_ds_labels = np.concatenate([labels for images, labels in test_ds], axis=0)\ntest_ds_labels[0], test_ds_images[0]\n</pre> import numpy as np test_ds_images = np.concatenate([images for images, labels in test_ds], axis=0) test_ds_labels = np.concatenate([labels for images, labels in test_ds], axis=0) test_ds_labels[0], test_ds_images[0] Out[\u00a0]: <pre>(0,\n array([[[ 43.804947,  44.804947,  38.804947],\n         [ 39.12483 ,  40.12483 ,  34.12483 ],\n         [ 82.701065,  83.701065,  77.62723 ],\n         ...,\n         [ 21.578135,  25.578135,  24.578135],\n         [ 19.741274,  23.741274,  22.741274],\n         [ 15.660867,  19.660868,  18.660868]],\n \n        [[ 40.762886,  41.762886,  35.41467 ],\n         [ 38.87469 ,  39.87469 ,  33.526478],\n         [ 84.99161 ,  85.99161 ,  78.59259 ],\n         ...,\n         [ 20.462063,  24.462063,  23.462063],\n         [ 19.207607,  23.207607,  22.207607],\n         [ 18.408989,  22.408989,  21.408989]],\n \n        [[ 37.69817 ,  38.69817 ,  30.698172],\n         [ 42.096752,  43.096752,  35.096752],\n         [ 93.62746 ,  94.81206 ,  86.258255],\n         ...,\n         [ 19.530594,  23.530594,  22.530594],\n         [ 18.091536,  22.091536,  21.091536],\n         [ 19.202106,  23.202106,  22.202106]],\n \n        ...,\n \n        [[106.28673 ,  70.28673 ,  96.28673 ],\n         [105.43164 ,  69.43164 ,  95.43164 ],\n         [106.95825 ,  70.95825 ,  96.95825 ],\n         ...,\n         [140.48886 , 111.48886 , 129.48886 ],\n         [135.05005 , 106.05005 , 124.05005 ],\n         [141.57098 , 112.57099 , 130.57098 ]],\n \n        [[105.29309 ,  69.29309 ,  95.29309 ],\n         [108.00053 ,  72.00053 ,  98.00053 ],\n         [108.30019 ,  72.30019 ,  98.30019 ],\n         ...,\n         [140.50948 , 111.50948 , 129.50948 ],\n         [137.474   , 108.474   , 126.474   ],\n         [142.99866 , 113.99865 , 131.99866 ]],\n \n        [[104.55957 ,  68.55957 ,  94.55957 ],\n         [108.76339 ,  72.76339 ,  98.76339 ],\n         [108.72768 ,  72.72768 ,  98.72768 ],\n         ...,\n         [140.63617 , 111.63617 , 129.63617 ],\n         [138.14514 , 109.14514 , 127.14514 ],\n         [141.46515 , 112.46516 , 130.46515 ]]], dtype=float32))</pre> In\u00a0[\u00a0]: Copied! <pre># Choose a random 10 indexes from the test data and compare the values\nimport random\n\nrandom_indexes = random.sample(range(len(test_ds_images)), 10)\n\n# TK - this is why we don't shuffle the test data\nfig, axes = plt.subplots(2, 5, figsize=(15, 7))\nfor i, ax in enumerate(axes.flatten()):\n  target_index = random_indexes[i]\n\n  # Get relevant target image, label, prediction and prediction probabilities\n  test_image = test_ds_images[target_index]\n  test_image_truth_label = class_names[test_ds_labels[target_index]]\n  test_image_pred_probs = test_preds[target_index]\n  test_image_pred_class = class_names[tf.argmax(test_image_pred_probs)]\n\n  # Plot the image\n  ax.imshow(test_image.astype(\"uint8\"))\n\n  # Create sample title\n  title = f\"\"\"True: {test_image_truth_label}\n  Pred: {test_image_pred_class}\n  Prob: {np.max(test_image_pred_probs):.2f}\"\"\"\n\n  # Colour the title based on correctness of pred\n  ax.set_title(title,\n               color=\"green\" if test_image_truth_label == test_image_pred_class else \"red\")\n  ax.axis(\"off\")\n</pre> # Choose a random 10 indexes from the test data and compare the values import random  random_indexes = random.sample(range(len(test_ds_images)), 10)  # TK - this is why we don't shuffle the test data fig, axes = plt.subplots(2, 5, figsize=(15, 7)) for i, ax in enumerate(axes.flatten()):   target_index = random_indexes[i]    # Get relevant target image, label, prediction and prediction probabilities   test_image = test_ds_images[target_index]   test_image_truth_label = class_names[test_ds_labels[target_index]]   test_image_pred_probs = test_preds[target_index]   test_image_pred_class = class_names[tf.argmax(test_image_pred_probs)]    # Plot the image   ax.imshow(test_image.astype(\"uint8\"))    # Create sample title   title = f\"\"\"True: {test_image_truth_label}   Pred: {test_image_pred_class}   Prob: {np.max(test_image_pred_probs):.2f}\"\"\"    # Colour the title based on correctness of pred   ax.set_title(title,                color=\"green\" if test_image_truth_label == test_image_pred_class else \"red\")   ax.axis(\"off\") In\u00a0[\u00a0]: Copied! <pre># TK - get accuracy values per class and show how they compare to the original results\n# see: http://vision.stanford.edu/aditya86/ImageNetDogs/ -&gt; http://vision.stanford.edu/aditya86/ImageNetDogs/bar_graph_full.png\n\n# Want to compare test_preds + test_labels on a per class basis\n# Can I convert both of these into a DataFrame and see what happens?\ntest_preds_labels = test_preds.argmax(axis=-1)\ntest_preds_labels\n</pre> # TK - get accuracy values per class and show how they compare to the original results # see: http://vision.stanford.edu/aditya86/ImageNetDogs/ -&gt; http://vision.stanford.edu/aditya86/ImageNetDogs/bar_graph_full.png  # Want to compare test_preds + test_labels on a per class basis # Can I convert both of these into a DataFrame and see what happens? test_preds_labels = test_preds.argmax(axis=-1) test_preds_labels Out[\u00a0]: <pre>array([  0,   0,   0, ..., 102, 119, 119])</pre> In\u00a0[\u00a0]: Copied! <pre>test_ds_labels\n</pre> test_ds_labels Out[\u00a0]: <pre>array([  0,   0,   0, ..., 119, 119, 119], dtype=int32)</pre> In\u00a0[\u00a0]: Copied! <pre>test_results_df = pd.DataFrame({\"test_pred_label\": test_preds_labels,\n                                \"test_pred_prob\": np.max(test_preds, axis=-1),\n                                \"test_pred_class_name\": [class_names[test_pred_label] for test_pred_label in test_preds_labels],\n                                \"test_truth_label\": test_ds_labels,\n                                \"test_truth_class_name\": [class_names[test_truth_label] for test_truth_label in test_ds_labels]})\n\ntest_results_df[\"correct\"] = (test_results_df[\"test_pred_class_name\"] == test_results_df[\"test_truth_class_name\"]).astype(int)\ntest_results_df\n</pre> test_results_df = pd.DataFrame({\"test_pred_label\": test_preds_labels,                                 \"test_pred_prob\": np.max(test_preds, axis=-1),                                 \"test_pred_class_name\": [class_names[test_pred_label] for test_pred_label in test_preds_labels],                                 \"test_truth_label\": test_ds_labels,                                 \"test_truth_class_name\": [class_names[test_truth_label] for test_truth_label in test_ds_labels]})  test_results_df[\"correct\"] = (test_results_df[\"test_pred_class_name\"] == test_results_df[\"test_truth_class_name\"]).astype(int) test_results_df Out[\u00a0]: test_pred_label test_pred_prob test_pred_class_name test_truth_label test_truth_class_name correct 0 0 0.981926 affenpinscher 0 affenpinscher 1 1 0 0.747863 affenpinscher 0 affenpinscher 1 2 0 0.995609 affenpinscher 0 affenpinscher 1 3 44 0.467855 flat_coated_retriever 0 affenpinscher 0 4 0 0.997168 affenpinscher 0 affenpinscher 1 ... ... ... ... ... ... ... 8575 119 0.785783 yorkshire_terrier 119 yorkshire_terrier 1 8576 102 0.735301 silky_terrier 119 yorkshire_terrier 0 8577 102 0.828518 silky_terrier 119 yorkshire_terrier 0 8578 119 0.940582 yorkshire_terrier 119 yorkshire_terrier 1 8579 119 0.603093 yorkshire_terrier 119 yorkshire_terrier 1 <p>8580 rows \u00d7 6 columns</p> In\u00a0[\u00a0]: Copied! <pre># Calculate accuracy per class\naccuracy_per_class = test_results_df.groupby(\"test_truth_class_name\")[\"correct\"].mean()\naccuracy_per_class_df = pd.DataFrame(accuracy_per_class).reset_index().sort_values(\"correct\", ascending=False)\naccuracy_per_class_df\n# pd.DataFrame(accuracy_per_class).sort_values(\"correct\", ascending=False)\n</pre> # Calculate accuracy per class accuracy_per_class = test_results_df.groupby(\"test_truth_class_name\")[\"correct\"].mean() accuracy_per_class_df = pd.DataFrame(accuracy_per_class).reset_index().sort_values(\"correct\", ascending=False) accuracy_per_class_df # pd.DataFrame(accuracy_per_class).sort_values(\"correct\", ascending=False) Out[\u00a0]: test_truth_class_name correct 62 keeshond 1.000000 10 bedlington_terrier 1.000000 30 chow 0.989583 92 saint_bernard 0.985714 2 african_hunting_dog 0.985507 ... ... ... 76 miniature_poodle 0.600000 5 appenzeller 0.588235 104 staffordshire_bullterrier 0.581818 16 border_collie 0.560000 43 eskimo_dog 0.440000 <p>120 rows \u00d7 2 columns</p> In\u00a0[\u00a0]: Copied! <pre># Let's create a horizontal bar chart to replicate a similar plot to the original Stanford Dogs page\nplt.figure(figsize=(10, 17))\nplt.barh(y=accuracy_per_class_df[\"test_truth_class_name\"],\n         width=accuracy_per_class_df[\"correct\"])\nplt.xlabel(\"Accuracy\")\nplt.ylabel(\"Class Name\")\nplt.title(\"Dog Vision Accuracy per Class\")\nplt.ylim(-0.5, len(accuracy_per_class_df[\"test_truth_class_name\"]) - 0.5)  # Adjust y-axis limits to reduce white space\nplt.gca().invert_yaxis()  # This will display the first class at the top\nplt.tight_layout()\nplt.show()\n</pre> # Let's create a horizontal bar chart to replicate a similar plot to the original Stanford Dogs page plt.figure(figsize=(10, 17)) plt.barh(y=accuracy_per_class_df[\"test_truth_class_name\"],          width=accuracy_per_class_df[\"correct\"]) plt.xlabel(\"Accuracy\") plt.ylabel(\"Class Name\") plt.title(\"Dog Vision Accuracy per Class\") plt.ylim(-0.5, len(accuracy_per_class_df[\"test_truth_class_name\"]) - 0.5)  # Adjust y-axis limits to reduce white space plt.gca().invert_yaxis()  # This will display the first class at the top plt.tight_layout() plt.show() <p>TK - How does this compare to the original results?</p> In\u00a0[\u00a0]: Copied! <pre># Get most wrong\ntop_100_most_wrong = test_results_df[test_results_df[\"correct\"] == 0].sort_values(\"test_pred_prob\", ascending=False)[:100]\ntop_100_most_wrong\n</pre> # Get most wrong top_100_most_wrong = test_results_df[test_results_df[\"correct\"] == 0].sort_values(\"test_pred_prob\", ascending=False)[:100] top_100_most_wrong Out[\u00a0]: test_pred_label test_pred_prob test_pred_class_name test_truth_label test_truth_class_name correct 2727 75 0.993720 miniature_pinscher 38 doberman 0 6884 54 0.993490 groenendael 95 schipperke 0 5480 44 0.990781 flat_coated_retriever 78 newfoundland 0 7630 4 0.988580 american_staffordshire_terrier 104 staffordshire_bullterrier 0 4155 55 0.986820 ibizan_hound 60 italian_greyhound 0 ... ... ... ... ... ... ... 2644 63 0.882950 kelpie 37 dingo 0 7934 73 0.882824 maltese_dog 109 tibetan_terrier 0 1059 14 0.881923 bloodhound 12 black_and_tan_coonhound 0 2047 86 0.879732 pembroke 27 cardigan 0 4601 15 0.878603 bluetick 67 labrador_retriever 0 <p>100 rows \u00d7 6 columns</p> In\u00a0[\u00a0]: Copied! <pre>top_100_most_wrong.sample(n=10).index\n</pre> top_100_most_wrong.sample(n=10).index Out[\u00a0]: <pre>7804</pre> In\u00a0[\u00a0]: Copied! <pre># Choose a random 10 indexes from the test data and compare the values\nimport random\n\nrandom_most_wrong_indexes = top_100_most_wrong.sample(n=10).index\n\n# TK - this is why we don't shuffle the test data\nfig, axes = plt.subplots(2, 5, figsize=(15, 7))\nfor i, ax in enumerate(axes.flatten()):\n  target_index = random_most_wrong_indexes[i]\n\n  # Get relevant target image, label, prediction and prediction probabilities\n  test_image = test_ds_images[target_index]\n  test_image_truth_label = class_names[test_ds_labels[target_index]]\n  test_image_pred_probs = test_preds[target_index]\n  test_image_pred_class = class_names[tf.argmax(test_image_pred_probs)]\n\n  # Plot the image\n  ax.imshow(test_image.astype(\"uint8\"))\n\n  # Create sample title\n  title = f\"\"\"True: {test_image_truth_label}\n  Pred: {test_image_pred_class}\n  Prob: {np.max(test_image_pred_probs):.2f}\"\"\"\n\n  # Colour the title based on correctness of pred\n  ax.set_title(title,\n               color=\"green\" if test_image_truth_label == test_image_pred_class else \"red\",\n               fontsize=10)\n  ax.axis(\"off\")\n</pre> # Choose a random 10 indexes from the test data and compare the values import random  random_most_wrong_indexes = top_100_most_wrong.sample(n=10).index  # TK - this is why we don't shuffle the test data fig, axes = plt.subplots(2, 5, figsize=(15, 7)) for i, ax in enumerate(axes.flatten()):   target_index = random_most_wrong_indexes[i]    # Get relevant target image, label, prediction and prediction probabilities   test_image = test_ds_images[target_index]   test_image_truth_label = class_names[test_ds_labels[target_index]]   test_image_pred_probs = test_preds[target_index]   test_image_pred_class = class_names[tf.argmax(test_image_pred_probs)]    # Plot the image   ax.imshow(test_image.astype(\"uint8\"))    # Create sample title   title = f\"\"\"True: {test_image_truth_label}   Pred: {test_image_pred_class}   Prob: {np.max(test_image_pred_probs):.2f}\"\"\"    # Colour the title based on correctness of pred   ax.set_title(title,                color=\"green\" if test_image_truth_label == test_image_pred_class else \"red\",                fontsize=10)   ax.axis(\"off\") In\u00a0[\u00a0]: Copied! <pre>from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\nfig, ax = plt.subplots(figsize=(25, 25))\nconfusion_matrix_dog_preds = confusion_matrix(y_true=test_ds_labels,\n                                              y_pred=test_preds_labels)\n\nconfusion_matrix_display = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_dog_preds,\n                                                  display_labels=class_names)\n\n# See: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.plot\nax.set_title(\"Dog Vision Confusion Matrix\")\nconfusion_matrix_display.plot(xticks_rotation=\"vertical\",\n                              cmap=\"Blues\",\n                              colorbar=False,\n                              ax=ax);\n</pre> from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay  fig, ax = plt.subplots(figsize=(25, 25)) confusion_matrix_dog_preds = confusion_matrix(y_true=test_ds_labels,                                               y_pred=test_preds_labels)  confusion_matrix_display = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_dog_preds,                                                   display_labels=class_names)  # See: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.plot ax.set_title(\"Dog Vision Confusion Matrix\") confusion_matrix_display.plot(xticks_rotation=\"vertical\",                               cmap=\"Blues\",                               colorbar=False,                               ax=ax); In\u00a0[\u00a0]: Copied! <pre># Save the model to .keras\nmodel_1.save(\"dog_vision_model.keras\")\n</pre> # Save the model to .keras model_1.save(\"dog_vision_model.keras\") In\u00a0[\u00a0]: Copied! <pre># Load the model\nloaded_model = tf.keras.models.load_model(\"dog_vision_model.keras\")\n\n# Evaluate the loaded model\nloaded_model_results = loaded_model.evaluate(test_ds)\n</pre> # Load the model loaded_model = tf.keras.models.load_model(\"dog_vision_model.keras\")  # Evaluate the loaded model loaded_model_results = loaded_model.evaluate(test_ds) <pre>269/269 [==============================] - 10s 26ms/step - loss: 0.3711 - accuracy: 0.8787\n</pre> In\u00a0[\u00a0]: Copied! <pre>assert model_1_results == loaded_model_results\n</pre> assert model_1_results == loaded_model_results In\u00a0[\u00a0]: Copied! <pre># TK - load custom image(s)\n!wget -nc https://github.com/mrdbourke/zero-to-mastery-ml/raw/master/images/dog-photos.zip\n!unzip dog-photos.zip\n</pre> # TK - load custom image(s) !wget -nc https://github.com/mrdbourke/zero-to-mastery-ml/raw/master/images/dog-photos.zip !unzip dog-photos.zip <pre>--2023-10-26 03:02:51--  https://github.com/mrdbourke/zero-to-mastery-ml/raw/master/images/dog-photos.zip\nResolving github.com (github.com)... 140.82.121.4\nConnecting to github.com (github.com)|140.82.121.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/mrdbourke/zero-to-mastery-ml/master/images/dog-photos.zip [following]\n--2023-10-26 03:02:51--  https://raw.githubusercontent.com/mrdbourke/zero-to-mastery-ml/master/images/dog-photos.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1091355 (1.0M) [application/zip]\nSaving to: \u2018dog-photos.zip\u2019\n\ndog-photos.zip      100%[===================&gt;]   1.04M  --.-KB/s    in 0.02s   \n\n2023-10-26 03:02:52 (51.5 MB/s) - \u2018dog-photos.zip\u2019 saved [1091355/1091355]\n\nArchive:  dog-photos.zip\n  inflating: dog-photo-4.jpeg        \n  inflating: dog-photo-1.jpeg        \n  inflating: dog-photo-2.jpeg        \n  inflating: dog-photo-3.jpeg        \n</pre> In\u00a0[\u00a0]: Copied! <pre># View images\ncustom_image_paths = [\"dog-photo-1.jpeg\",\n                      \"dog-photo-2.jpeg\",\n                      \"dog-photo-3.jpeg\",\n                      \"dog-photo-4.jpeg\"]\n\nfig, axes = plt.subplots(1, 4, figsize=(15, 7))\nfor i, ax in enumerate(axes.flatten()):\n  ax.imshow(plt.imread(custom_image_paths[i]))\n  ax.axis(\"off\")\n\n# def plot_10_random_images_from_path_list(path_list: list):\n#     fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n#     samples = random.sample(path_list, 10)\n#     for i, ax in enumerate(axes.flatten()):\n#         sample_path = samples[i]\n#         sample_title = sample_path.parent.stem\n#         ax.imshow(plt.imread(sample_path))\n#         ax.set_title(sample_title)\n#         ax.axis(\"off\")\n</pre> # View images custom_image_paths = [\"dog-photo-1.jpeg\",                       \"dog-photo-2.jpeg\",                       \"dog-photo-3.jpeg\",                       \"dog-photo-4.jpeg\"]  fig, axes = plt.subplots(1, 4, figsize=(15, 7)) for i, ax in enumerate(axes.flatten()):   ax.imshow(plt.imread(custom_image_paths[i]))   ax.axis(\"off\")  # def plot_10_random_images_from_path_list(path_list: list): #     fig, axes = plt.subplots(2, 5, figsize=(20, 10)) #     samples = random.sample(path_list, 10) #     for i, ax in enumerate(axes.flatten()): #         sample_path = samples[i] #         sample_title = sample_path.parent.stem #         ax.imshow(plt.imread(sample_path)) #         ax.set_title(sample_title) #         ax.axis(\"off\") In\u00a0[\u00a0]: Copied! <pre># This will error...\nloaded_model.predict(\"dog-photo-1.jpeg\")\n</pre> # This will error... loaded_model.predict(\"dog-photo-1.jpeg\") <pre>\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n&lt;ipython-input-97-bf77d5e16d11&gt; in &lt;cell line: 2&gt;()\n      1 # This will error...\n----&gt; 2 loaded_model.predict(\"dog-photo-1.jpeg\")\n\n/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)\n     68             # To get the full stack trace, call:\n     69             # `tf.debugging.disable_traceback_filtering()`\n---&gt; 70             raise e.with_traceback(filtered_tb) from None\n     71         finally:\n     72             del filtered_tb\n\n/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_shape.py in __getitem__(self, key)\n    957       else:\n    958         if self._v2_behavior:\n--&gt; 959           return self._dims[key]\n    960         else:\n    961           return self.dims[key]\n\nIndexError: tuple index out of range</pre> In\u00a0[\u00a0]: Copied! <pre># Model needs to make predictions on images in same format it was trained on\n\n# Load the image (into PIL format)\ncustom_image = tf.keras.utils.load_img(\n  path=\"dog-photo-1.jpeg\",\n  color_mode=\"rgb\",\n  target_size=(img_size, img_size),\n)\n\ncustom_image\n</pre> # Model needs to make predictions on images in same format it was trained on  # Load the image (into PIL format) custom_image = tf.keras.utils.load_img(   path=\"dog-photo-1.jpeg\",   color_mode=\"rgb\",   target_size=(img_size, img_size), )  custom_image Out[\u00a0]: In\u00a0[\u00a0]: Copied! <pre># Turn the image into a tensor\ncustom_image_tensor = tf.keras.utils.img_to_array(custom_image)\ncustom_image_tensor.shape\n</pre> # Turn the image into a tensor custom_image_tensor = tf.keras.utils.img_to_array(custom_image) custom_image_tensor.shape Out[\u00a0]: <pre>(224, 224, 3)</pre> In\u00a0[\u00a0]: Copied! <pre>loaded_model.predict(custom_image_tensor)\n</pre> loaded_model.predict(custom_image_tensor) <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-101-f7a0c54105c0&gt; in &lt;cell line: 1&gt;()\n----&gt; 1 loaded_model.predict(custom_image_tensor)\n\n/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)\n     68             # To get the full stack trace, call:\n     69             # `tf.debugging.disable_traceback_filtering()`\n---&gt; 70             raise e.with_traceback(filtered_tb) from None\n     71         finally:\n     72             del filtered_tb\n\n/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py in tf__predict_function(iterator)\n     13                 try:\n     14                     do_return = True\n---&gt; 15                     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n     16                 except:\n     17                     do_return = False\n\nValueError: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2416, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2401, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2389, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2357, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(32, 224, 3)\n</pre> In\u00a0[\u00a0]: Copied! <pre>pred_probs = loaded_model.predict(tf.expand_dims(custom_image_tensor, axis=0))\n# pred_probs = tf.keras.activations.softmax(tf.constant(pred_probs)) # if you have no activation=\"softmax\" in your model\nclass_names[tf.argmax(pred_probs, axis=-1).numpy()[0]]\n</pre> pred_probs = loaded_model.predict(tf.expand_dims(custom_image_tensor, axis=0)) # pred_probs = tf.keras.activations.softmax(tf.constant(pred_probs)) # if you have no activation=\"softmax\" in your model class_names[tf.argmax(pred_probs, axis=-1).numpy()[0]] <pre>1/1 [==============================] - 2s 2s/step\n</pre> Out[\u00a0]: <pre>'labrador_retriever'</pre> In\u00a0[\u00a0]: Copied! <pre>tf.expand_dims(custom_image_tensor, axis=0).shape\n</pre> tf.expand_dims(custom_image_tensor, axis=0).shape Out[\u00a0]: <pre>TensorShape([1, 224, 224, 3])</pre> <p>Note: TK - In the case of some models you may need to rescale your values here, in our case the Rescaling layer is built-in to the model.</p> In\u00a0[\u00a0]: Copied! <pre>def pred_on_custom_image(image_path,\n                         model,\n                         target_size=img_size,\n                         class_names=class_names,\n                         plot=True):\n\n  # Prepare and load image\n  custom_image = tf.keras.utils.load_img(\n    path=image_path,\n    color_mode=\"rgb\",\n    target_size=(target_size, target_size),\n  )\n\n  # Turn the image into a tensor\n  custom_image_tensor = tf.keras.utils.img_to_array(custom_image)\n\n  # Add a batch dimension to the target tensor (e.g. (224, 224, 3) -&gt; (1, 224, 224, 3))\n  custom_image_tensor = tf.expand_dims(custom_image_tensor, axis=0)\n\n  # Make a prediction with the target model\n  pred_probs = model.predict(custom_image_tensor)\n  # pred_probs = tf.keras.activations.softmax(tf.constant(pred_probs))\n  pred_class = class_names[tf.argmax(pred_probs, axis=-1).numpy()[0]]\n\n  # Plot if we want\n  if not plot:\n    return pred_class\n  else:\n    plt.figure(figsize=(5, 3))\n    plt.imshow(plt.imread(image_path))\n    plt.title(pred_class)\n    plt.axis(\"off\")\n</pre> def pred_on_custom_image(image_path,                          model,                          target_size=img_size,                          class_names=class_names,                          plot=True):    # Prepare and load image   custom_image = tf.keras.utils.load_img(     path=image_path,     color_mode=\"rgb\",     target_size=(target_size, target_size),   )    # Turn the image into a tensor   custom_image_tensor = tf.keras.utils.img_to_array(custom_image)    # Add a batch dimension to the target tensor (e.g. (224, 224, 3) -&gt; (1, 224, 224, 3))   custom_image_tensor = tf.expand_dims(custom_image_tensor, axis=0)    # Make a prediction with the target model   pred_probs = model.predict(custom_image_tensor)   # pred_probs = tf.keras.activations.softmax(tf.constant(pred_probs))   pred_class = class_names[tf.argmax(pred_probs, axis=-1).numpy()[0]]    # Plot if we want   if not plot:     return pred_class   else:     plt.figure(figsize=(5, 3))     plt.imshow(plt.imread(image_path))     plt.title(pred_class)     plt.axis(\"off\") In\u00a0[\u00a0]: Copied! <pre>pred_on_custom_image(image_path=\"dog-photo-2.jpeg\", model=loaded_model)\n</pre> pred_on_custom_image(image_path=\"dog-photo-2.jpeg\", model=loaded_model) In\u00a0[\u00a0]: Copied! <pre># Predict on multiple images\nfig, axes = plt.subplots(1, 4, figsize=(15, 7))\nfor i, ax in enumerate(axes.flatten()):\n  image_path = custom_image_paths[i]\n  pred_class = pred_on_custom_image(image_path=image_path,\n                                    model=loaded_model,\n                                    plot=False)\n  ax.imshow(plt.imread(image_path))\n  ax.set_title(pred_class)\n  ax.axis(\"off\")\n</pre> # Predict on multiple images fig, axes = plt.subplots(1, 4, figsize=(15, 7)) for i, ax in enumerate(axes.flatten()):   image_path = custom_image_paths[i]   pred_class = pred_on_custom_image(image_path=image_path,                                     model=loaded_model,                                     plot=False)   ax.imshow(plt.imread(image_path))   ax.set_title(pred_class)   ax.axis(\"off\") In\u00a0[\u00a0]: Copied! <pre>from tensorflow.keras import layers\n\ndata_augmentation = tf.keras.Sequential(\n    [\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(factor=0.2),\n        layers.RandomZoom(\n            height_factor=0.2, width_factor=0.2\n        ),\n    ],\n    name=\"data_augmentation\"\n)\n\nbase_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(\n    include_top=False,\n    weights='imagenet',\n    input_shape=(img_size, img_size, 3),\n    include_preprocessing=True\n)\n\n# base_model.summary()\n\n# Freeze the base model\nbase_model.trainable = False\n\n# TK - functionize this\n\n# Create new model\ninputs = tf.keras.Input(shape=(224, 224, 3))\n\n# TK - Create data augmentation\nx = data_augmentation(inputs)\n\n# Craft model\nx = base_model(x, training=False)\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = tf.keras.layers.Dropout(0.2)(x)\noutputs = tf.keras.layers.Dense(num_classes,\n                                name=\"output_layer\",\n                                activation=\"softmax\")(x) # Note: If you have \"softmax\" activation, use from_logits=False in loss function\nmodel_2 = tf.keras.Model(inputs, outputs, name=\"model_2\")\nmodel_2.summary()\n</pre> from tensorflow.keras import layers  data_augmentation = tf.keras.Sequential(     [         layers.RandomFlip(\"horizontal\"),         layers.RandomRotation(factor=0.2),         layers.RandomZoom(             height_factor=0.2, width_factor=0.2         ),     ],     name=\"data_augmentation\" )  base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(     include_top=False,     weights='imagenet',     input_shape=(img_size, img_size, 3),     include_preprocessing=True )  # base_model.summary()  # Freeze the base model base_model.trainable = False  # TK - functionize this  # Create new model inputs = tf.keras.Input(shape=(224, 224, 3))  # TK - Create data augmentation x = data_augmentation(inputs)  # Craft model x = base_model(x, training=False) x = tf.keras.layers.GlobalAveragePooling2D()(x) x = tf.keras.layers.Dropout(0.2)(x) outputs = tf.keras.layers.Dense(num_classes,                                 name=\"output_layer\",                                 activation=\"softmax\")(x) # Note: If you have \"softmax\" activation, use from_logits=False in loss function model_2 = tf.keras.Model(inputs, outputs, name=\"model_2\") model_2.summary()"},{"location":"end-to-end-dog-vision-v2/#introduction-to-tensorflow-deep-learning-and-transfer-learning-work-in-progress","title":"Introduction to TensorFlow, Deep Learning and Transfer Learning (work in progress)\u00b6","text":"<ul> <li>Project: Dog Vision \ud83d\udc36\ud83d\udc41 - Using computer vision to classify dog photos into different breeds.</li> <li>Goals: Learn TensorFlow, deep learning and transfer learning.</li> <li>Domain: Computer vision.</li> <li>Data: Images of dogs from Stanford Dogs Dataset (120 dog breeds, 20,000+ images).</li> <li>Problem type: Multi-class classification (120 different classes).</li> </ul> <p>Welcome, welcome!</p> <p>The focus of this notebook is to give a quick overview of deep learning with TensorFlow.</p> <p>How?</p> <p>We're going to go through the machine learning workflow steps and build a computer vision project to classify photos of dogs into their respective dog breed.</p> <p>TK - image of workflow - e.g. dog photo -&gt; model -&gt; dog breed</p>"},{"location":"end-to-end-dog-vision-v2/#tk-what-were-going-to-cover","title":"TK - What we're going to cover\u00b6","text":"<p>In this project, we're going to be introduced to the power of deep learning and more specifically, transfer learning using TensorFlow.</p> <p>We'll go through each of these in the context of the 6 step machine learning framework:</p> <ol> <li>Problem defintion - Use computer vision to classify photos of dogs into different dog breeds.</li> <li>Data - 20,000+ images of dogs from 120 different dog breeds from the Stanford Dogs dataset.</li> <li>Evaluation - We'd like to beat the original paper's results (22% mean accuracy across all classes, tip: A good way to practice your skills is to find some results online and try to beat them).</li> <li>Features - Because we're using deep learning, our model will learn the features on its own.</li> <li>Modelling - We're going to use a pretrained convolutional neural network (CNN) and transfer learning.</li> <li>Experiments - We'll try different amounts of data with the same model to see the effects on our results.</li> </ol> <p>Note: It's okay not to know these exact steps ahead of time. When starting a new project, it's often the case you'll figure it out as you go. These steps are only filled out because I've had practice working on several machine learning projects. You'll pick up these ideas overtime.</p>"},{"location":"end-to-end-dog-vision-v2/#tk-table-of-contents","title":"TK - Table of contents\u00b6","text":"<ul> <li>Problem type (e.g. multi-class classification)</li> <li>Domain (e.g. computer vision)</li> <li>Data type (e.g. unstructured vs structured)</li> </ul>"},{"location":"end-to-end-dog-vision-v2/#problem-defintion","title":"Problem Defintion\u00b6","text":""},{"location":"end-to-end-dog-vision-v2/#data","title":"Data\u00b6","text":""},{"location":"end-to-end-dog-vision-v2/#evaluation","title":"Evaluation\u00b6","text":""},{"location":"end-to-end-dog-vision-v2/#features","title":"Features\u00b6","text":""},{"location":"end-to-end-dog-vision-v2/#modelling","title":"Modelling\u00b6","text":""},{"location":"end-to-end-dog-vision-v2/#experiments","title":"Experiments\u00b6","text":""},{"location":"end-to-end-dog-vision-v2/#tk-where-can-can-you-get-help","title":"TK - Where can can you get help?\u00b6","text":"<p>All of the materials for this course live on GitHub.</p> <p>If you run into trouble, you can ask a question on the course GitHub Discussions page there too.</p>"},{"location":"end-to-end-dog-vision-v2/#quick-definitions","title":"Quick definitions\u00b6","text":"<p>Let's start by breaking down some of the most important topics we're going to go through.</p>"},{"location":"end-to-end-dog-vision-v2/#tk-what-is-tensorflow","title":"TK - What is TensorFlow?\u00b6","text":"<p>TensorFlow is an open source machine learning and deep learning framework originally developed by Google.</p>"},{"location":"end-to-end-dog-vision-v2/#tk-why-use-tensorflow","title":"TK - Why use TensorFlow?\u00b6","text":"<p>TensorFlow allows you to manipulate data and write deep learning algorithms using Python code.</p> <p>It also has several built-in capabilities to leverage accelerated computing hardware (e.g. GPUs, Graphics Processing Units and TPUs, Tensor Processing Units).</p> <p>Many of world's largest companies power their machine learning workloads with TensorFlow.</p>"},{"location":"end-to-end-dog-vision-v2/#tk-what-is-deep-learning","title":"TK - What is deep learning?\u00b6","text":"<p>Deep learning is a form of machine learning where data passes through a series of progressive layers which all contribute to learning an overall representation of that data.</p> <p>The series of progressive layers combines to form what's referred to as a neural network.</p> <p>For example, a photo may be turned into numbers and those numbers are then manipulated mathematically through each progressive layer to learn patterns in the photo.</p> <p>The \"deep\" in deep learning comes from the number of layers used in the neural network.</p> <p>So when someone says deep learning or (artificial neural networks), they're typically referring to same thing.</p>"},{"location":"end-to-end-dog-vision-v2/#tk-what-can-deep-learning-be-used-for","title":"TK - What can deep learning be used for?\u00b6","text":"<p>Deep learning is such a powerful technique that new use cases are being discovered everyday.</p> <p>Most of the modern forms of artifical intelligence (AI) applications you see, are powered by deep learning.</p> <p>ChatGPT uses deep learning to process text and return a response.</p> <p>Tesla's self-driving cars use deep learning to power their computer vision systems.</p> <p>Apple's Photos app uses deep learning to recognize faces in images and create Photo Memories.</p> <p>Nutrify (an app my brother and I build) uses deep learning to recognize food in images.</p> <p>TK - image of examples</p>"},{"location":"end-to-end-dog-vision-v2/#tk-what-is-transfer-learning","title":"TK - What is transfer learning?\u00b6","text":"<p>Transfer learning is one of the most powerful and useful techniques in modern AI and machine learning.</p> <p>It involves taking what one model (or neural network) has learned in a similar domain and applying to your own.</p> <p>In our case, we're going to use transfer learning to take the patterns a neural network has learned from the 1 million+ images and over 1000 classes in ImageNet (a gold standard computer vision benchmark) and apply them to our own problem of recognizing dog breeds.</p> <p>The biggest benefit of transfer learning is that it often allows you to get outstanding results with less data and time.</p> <p>TK - Transfer learning workflow - Large data -&gt; Large model -&gt; Patterns -&gt; Custom data -&gt; Custom model</p>"},{"location":"end-to-end-dog-vision-v2/#tk-getting-setup","title":"TK - Getting setup\u00b6","text":"<p>This section of the course is taught with Google Colab, an online Jupyter Notebook that provides free access to GPUs (Graphics Processing Units, we'll hear more on these later).</p> <p>For a quick rundown on how to use Google Colab, see their introductory guide (it's quite similar to a Jupyter Notebook with a few different options).</p> <p>Google Colab also comes with many data science and machine learning libraries, including TensorFlow, pre-installed.</p>"},{"location":"end-to-end-dog-vision-v2/#getting-a-gpu","title":"Getting a GPU\u00b6","text":"<p>Before running any code, we'll make sure our Google Colab instance is connected to a GPU.</p> <p>You can do this via going to Runtime -&gt; Change runtime type -&gt; GPU (this may restart your existing runtime).</p> <p>Why use a GPU?</p> <p>Since neural networks perform a large amount of calculations behind the scenes (the main one being matrix multiplication), you need a computer chip that perform these calculations quickly, otherwise you'll be waiting all day for a model to train.</p> <p>And in short, GPUs are much faster at performing matrix multiplications than CPUs.</p> <p>Why this is the case is behind the scope of this project (you can search \"why are GPUs faster than CPUs for machine learning?\" for more).</p> <p>The main thing to remember is: generally, in deep learning, GPUs = faster than CPUs.</p> <p>Note: A good experiment would be to run the neural networks we're going to build later on with and without a GPU and see the difference in their training times.</p> <p>Ok, enough talking, let's start by importing TensorFlow!</p> <p>We'll do so using the common abbreviation <code>tf</code>.</p>"},{"location":"end-to-end-dog-vision-v2/#tk-getting-data","title":"TK - Getting Data\u00b6","text":"<p>There are several options and locations to get data for a deep learning project.</p> Resource Description Kaggle Datasets A collection of datasets across a wide range of topics. TensorFlow Datasets A collection of ready-to-use machine learning datasets ready for use under the <code>tf.data.Datasets</code> API. You can see a list of all available datasets in the TensorFlow documentation. Hugging Face Datasets A continually growing resource of datasets broken into several different kinds of topics. Google Dataset Search A search engine by Google specifically focused on searching online datasets. Original sources Datasets which are made available by researchers or companies with the release of a product or research paper (sources for these will vary, they could be a link on a website or a link to an application form). Custom datasets These are datasets comprised of your own custom source of data. You may build these from scratch on your own or have access to them from an existing product or service. For example, your entire photos library could be your own custom dataset or your entire notes and documents folder or your company's custom order history. <p>In our case, the dataset we're going to use is called the Stanford Dogs dataset (or ImageNet dogs, as the images are dogs separated from ImageNet).</p> <p>Because the Stanford Dogs dataset has been around for a while (since 2011, which as of writing this in 2023 is like a lifetime in deep learning), it's available from several resources:</p> <ul> <li>The original project website via link download</li> <li>Inside TensorFlow datasets under <code>stanford_dogs</code></li> <li>On Kaggle as a downloadable dataset</li> </ul> <p>The point here is that when you're starting out with practicing deep learning projects, there's no shortage of datasets available.</p> <p>However, when you start wanting to work on your own projects or within a company environment, you'll likely start to work on custom datasets (datasets you build yourself or aren't available publicly online).</p> <p>The main difference between existing datasets and custom datasets is that existing datasets often come preformatted and ready to use.</p> <p>Where as custom datasets often require some preprocessing before they're ready to use within a machine learning project.</p> <p>To practice formatting a dataset for a machine learning problem, we're going to download the Stanford Dogs dataset from the original website.</p> <p>Before we do so, the following code is an example of how we'd get the Stanford Dogs dataset from TensorFlow Datasets.</p>"},{"location":"end-to-end-dog-vision-v2/#tk-download-data-directly-from-stanford-dogs-website","title":"TK - Download data directly from Stanford Dogs website\u00b6","text":"<p>Our overall project goal is to build a computer vision model which performs better than the original Stanford Dogs paper (average of 22% accuracy per class across 120 classes).</p> <p>To do so, we need some data.</p> <p>Let's download the original Stanford Dogs dataset from the project website.</p> <p>The data comes in three main files:</p> <ol> <li>Images (757MB) - <code>images.tar</code></li> <li>Annotations (21MB) - <code>annotation.tar</code></li> <li>Lists, with train/test splits (0.5MB) - <code>lists.tar</code></li> </ol> <p>Our goal is to get a file structure like this:</p> <pre><code>dog_vision_data/\n    images.tar\n    annotation.tar\n    lists.tar\n</code></pre> <p>Note: If you're using Google Colab for this project, remember that any data uploaded to the Google Colab session gets deleted if the session disconnects. So to save us redownloading the data every time, we're going to download it once and save it to Google Drive.</p> <p>Resource: For a good guide on getting data in and out of Google Colab, see the Google Colab <code>io.ipynb</code> tutorial.</p> <p>To make sure we don't have to keep redownloading the data every time we leave and come back to Google Colab, we're going to:</p> <ol> <li>Download the data if it doesn't already exist on Google Drive.</li> <li>Copy it to Google Drive (because Google Colab connects nicely with Google Drive) if it isn't already there.</li> <li>If the data already exists on Google Drive (we've been through steps 1 &amp; 2), we'll import it instead.</li> </ol> <p>There are two main options to connect Google Colab instances to Google Drive:</p> <ol> <li>Click \"Mount Drive\" in \"Files\" menu on the left.</li> <li>Mount programmatically with <code>from google.colab import drive</code> -&gt; <code>drive.mount('/content/drive')</code>.</li> </ol> <p>More specifically, we're going to follow the following steps:</p> <ol> <li>Mount Google Drive.</li> <li>Setup constants such as our base directory to save files to, the target files we'd like to download and target URL we'd like to download from.</li> <li>Setup our target local path to save to.</li> <li>Check if the target files all exist in Google Drive and if they do, copy them locally.</li> <li>If the target files don't exist in Google Drive, download them from the target URL with the <code>!wget</code> command.</li> <li>Create a file on Google Drive to store the download files.</li> <li>Copy the downloaded files to Google Drive for use later if needed.</li> </ol> <p>A fair few steps, but nothing we can't handle!</p> <p>Plus, this is all good practice for dealing with and manipulating data, a very important skill in the machine learning engineers toolbox.</p>"},{"location":"end-to-end-dog-vision-v2/#exploring-the-data","title":"Exploring the data\u00b6","text":"<p>Once you've got a dataset, before building a model, it's wise to explore it for a bit to see what kind of data you're working with.</p> <ul> <li>TK - things you should do when you start with a new dataset</li> <li>visualize</li> <li>check the distributions (e.g. number of samples per class)</li> </ul> <p>TK - daniel bourke tweet about abraham loss function - https://twitter.com/mrdbourke/status/1456087631641473033</p>"},{"location":"end-to-end-dog-vision-v2/#discussing-our-target-data-format","title":"Discussing our target data format\u00b6","text":"<p>Since our goal is to build a computer vision model to classify dog breeds, we need a way to tell our model what breed of dog is in what image.</p> <p>A common data format for a classification problem is to have samples stored in folders named after their class name.</p> <p>For example:</p> <pre><code>images_split/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 class_1/\n\u2502   \u2502   \u251c\u2500\u2500 train_image1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 train_image2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 class_2/\n\u2502   \u2502   \u251c\u2500\u2500 train_image1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 train_image2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 test/\n    \u251c\u2500\u2500 class_1/\n    \u2502   \u251c\u2500\u2500 test_image1.jpg\n    \u2502   \u251c\u2500\u2500 test_image2.jpg\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 class_2/\n    \u2502   \u251c\u2500\u2500 test_image1.jpg\n    \u2502   \u251c\u2500\u2500 test_image2.jpg\n...\n</code></pre> <p>In the case of dog images, we'd put all of the images labelled \"chihuahua\" in a folder called <code>chihuahua/</code> (and so on for all the other classes and images).</p> <p>We could split these folders so that training images go in <code>train/chihuahua/</code> and testing images go in <code>test/chihuahua/</code>.</p> <p>This is what we'll be working towards creating.</p> <p>Note: This structure of folder format doesn't just work for only images, it works for text, audio and other kind of classification data too.</p>"},{"location":"end-to-end-dog-vision-v2/#exploring-the-file-lists","title":"Exploring the file lists\u00b6","text":"<p>How about we check out the <code>train_list.mat</code>, <code>test_list.mat</code> and <code>full_list.mat</code> files?</p> <p>Searching online, for \"what is a .mat file?\", I found that it's a MATLAB file. Before Python became the default language for machine learning and deep learning, many models and datasets were built in MATLAB.</p> <p>Then I searched, \"how to open a .mat file with Python?\" and found an answer on Stack Overflow saying I could use the <code>scipy</code> library (a scientific computing library).</p> <p>The good news is, Google Colab comes with <code>scipy</code> preinstalled.</p> <p>We can use the <code>scipy.io.loadmat()</code> method to open a <code>.mat</code> file.</p>"},{"location":"end-to-end-dog-vision-v2/#exploring-the-annotation-folder","title":"Exploring the Annotation folder\u00b6","text":"<p>How about we look at the <code>Annotation</code> folder next?</p> <p>We can click the folder on the file explorer on the left to see what's inside.</p> <p>But we can also explore the contents of the folder with Python.</p> <p>Let's use <code>os.listdir()</code> to see what's inside.</p>"},{"location":"end-to-end-dog-vision-v2/#exploring-the-images-folder","title":"Exploring the Images folder\u00b6","text":"<p>We've explored the <code>Annotations</code> folder, now let's check out our <code>Images</code> folder.</p> <p>We know that the image file names come in the format <code>class_name/image_name</code>, for example, <code>n02085620-Chihuahua/n02085620_5927.jpg</code>.</p> <p>To make things a little simpler, let's create the following:</p> <ol> <li>A mapping from folder name -&gt; class name in dictionary form, for example, <code>{'n02113712-miniature_poodle': 'miniature_poodle',  'n02092339-Weimaraner': 'weimaraner',  'n02093991-Irish_terrier': 'irish_terrier'...}</code>. This will help us when visualizing our data from its original folder.</li> <li>A list of all unique dog class names with simple formatting, for example, <code>['affenpinscher',  'afghan_hound',  'african_hunting_dog',  'airedale',  'american_staffordshire_terrier'...]</code>.</li> </ol> <p>Let's start by getting a list of all the folders in the <code>Images</code> directory with <code>os.listdir()</code>.</p>"},{"location":"end-to-end-dog-vision-v2/#visualize-a-group-of-random-images","title":"Visualize a group of random images\u00b6","text":"<p>How about we follow the data explorers motto of visualize, visualize, visualize and view some random images?</p> <p>To help us visualize, let's create a function that takes in a list of image paths and then randomly selects 10 of those paths to display.</p> <p>The function will:</p> <ol> <li>Take in a select list of image paths.</li> <li>Create a grid of matplotlib plots (e.g. 2x5 = 10 plots to plot on).</li> <li>Randomly sample 10 image paths from the input image path list (using <code>random.sample()</code>).</li> <li>Iterate through the flattened axes via <code>axes.flat</code> which is a reference to the attribute <code>numpy.ndarray.flat</code>.</li> <li>Extract the sample path from the list of samples.</li> <li>Get the sample title from the parent folder of the path using <code>Path.parent.stem</code> and then extract the formatted dog breed name by indexing <code>folder_to_class_name_dict</code>.</li> <li>Read the image with <code>plt.imread()</code> and show it on the target <code>ax</code> with <code>ax.imshow()</code>.</li> <li>Set the title of the plot to the parent folder name with <code>ax.set_title()</code> and turn the axis marks of with <code>ax.axis(\"off\")</code> (this makes for pretty plots).</li> <li>Show the plot with <code>plt.show()</code>.</li> </ol> <p>Woah!</p> <p>A lot of steps! But nothing we can't handle, let's do it.</p>"},{"location":"end-to-end-dog-vision-v2/#exploring-the-distribution-of-our-data","title":"Exploring the distribution of our data\u00b6","text":"<p>After visualization, another valuable way to explore the data is by checking the data distribution.</p> <p>Distribution refers to the \"spread\" of data.</p> <p>In our case, how many images of dogs do we have per breed?</p> <p>A balanced distribution would mean having roughly the same number of images for each breed (e.g. 100 images per dog breed).</p> <p>Note: There's a deeper level of distribution than just images per dog breed. Ideally, the images for each different breed are well distributed as well. For example, we wouldn't want to have 100 of the same image per dog breed. Not only would we like a similar number of images per breed, we'd like the images of each particular breed to be in different scenarios, different lighting, different angles. We want this because we want to our model to be able to recognize the correct dog breed no matter what angle the photo is taken from.</p> <p>To figure out how many images we have per class, let's write a function count the number of images per subfolder in a given directory.</p> <p>Specifically, we'll want the function to:</p> <ol> <li>Take in a target directory/folder.</li> <li>Create a list of all the subdirectories/subfolders in the target folder.</li> <li>Create an empty list, <code>image_class_counts</code> to append subfolders and their counts to.</li> <li>Iterate through all of the subdirectories.</li> <li>Get the class name of the target folder as the name of the folder.</li> <li>Count the number of images in the target folder using the length of the list of image paths (we can get these with <code>Path().rglob(*.jpg)</code> where <code>*.jpg</code> means \"all files with the extension <code>.jpg</code>.</li> <li>Append a dictionary of <code>{\"class_name\": class_name, \"image_count\": image_count}</code> to the <code>image_class_counts</code> list (we create a list of dictionaries so we can turn this into a pandas DataFrame).</li> <li>Return the <code>image_class_counts</code> list.</li> </ol>"},{"location":"end-to-end-dog-vision-v2/#tk-creating-training-and-test-data-split-directories","title":"TK - Creating training and test data split directories\u00b6","text":"<p>After exploring the data, one of the next best things you can do is create experimental data splits.</p> <p>This includes:</p> Set Name Description Typical Percentage of Data Training Set A dataset for the model to learn on 70-80% Testing Set A dataset for the model to be evaluated on 20-30% (Optional) Validation Set A dataset to tune the model on 50% of the test data (Optional) Smaller Training Set A smaller size dataset to run quick experiments on 5-20% of the training set <p>Our dog dataset already comes with specified training and test set splits.</p> <p>So we'll stick with those.</p> <p>But we'll also create a smaller training set (a random 10% of the training data) so we can stick to the machine learning engineers motto of experiment, experiment, experiment! and run quicker experiments.</p> <p>Note: One of the most important things in machine learning is being able to experiment quickly. As in, try a new model, try a new set of hyperparameters or try a new training setup. When you start out, you want the time between your experiments to be as small as possible so you can quickly figure out what doesn't work so you can spend more time on and run larger experiments with what does work.</p> <p>As previously discussed, we're working towards a directory structure of:</p> <pre><code>images_split/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 class_1/\n\u2502   \u2502   \u251c\u2500\u2500 train_image1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 train_image2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 class_2/\n\u2502   \u2502   \u251c\u2500\u2500 train_image1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 train_image2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 test/\n    \u251c\u2500\u2500 class_1/\n    \u2502   \u251c\u2500\u2500 test_image1.jpg\n    \u2502   \u251c\u2500\u2500 test_image2.jpg\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 class_2/\n    \u2502   \u251c\u2500\u2500 test_image1.jpg\n    \u2502   \u251c\u2500\u2500 test_image2.jpg\n...\n</code></pre> <p>So let's write some code to create:</p> <ul> <li><code>images/train/</code> directory.</li> <li><code>images/test/</code> directory.</li> <li>Make a directory inside each of <code>images/train/</code> and <code>images/test/</code> for each of the dog breed classes.</li> </ul> <p>We can make each of the directories we need using <code>Path.mkdir()</code>.</p> <p>For the dog breed directories, we'll loop through the list of <code>dog_names</code> and create a folder for each inside the <code>images/train/</code> and <code>images/test/</code> directories.</p>"},{"location":"end-to-end-dog-vision-v2/#tk-make-a-10-training-dataset","title":"TK - Make a 10% training dataset\u00b6","text":"<p>UPTOHERE</p> <ul> <li><p>TK image - make an image diagram of the image split folder we're going to make e.g. train_10_percent...</p> </li> <li><p>Why make this folder? (quicker experiments, faster to see if something works before scaling things up)</p> </li> <li><p>Can copy a random 10% of the training images into a new folder</p> </li> <li><p>Try to train a model on a smaller amount of data</p> </li> </ul>"},{"location":"end-to-end-dog-vision-v2/#tk-turn-datasets-into-tensorflow-datasets","title":"TK - Turn datasets into TensorFlow Dataset(s)\u00b6","text":"<ul> <li>See here: https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory</li> <li>Also here: https://www.tensorflow.org/tutorials/load_data/images</li> </ul> <p>Note: TK - If you're working with similar styles of data (e.g. all dog photos), it's best practice to shuffle training datasets to prevent the model from learning any order in the data, no need to shuffle testing datasets (this makes for easier evaluation).</p>"},{"location":"end-to-end-dog-vision-v2/#tk-configure-the-dataset-for-performance","title":"TK - Configure the dataset for performance\u00b6","text":"<ul> <li>See here: https://www.tensorflow.org/guide/data_performance</li> </ul>"},{"location":"end-to-end-dog-vision-v2/#tk-create-model","title":"TK - Create model\u00b6","text":""},{"location":"end-to-end-dog-vision-v2/#tk-try-transfer-learning","title":"TK - Try transfer learning\u00b6","text":"<ul> <li>Only want to train the last layer of the model...</li> <li>See here: https://keras.io/guides/transfer_learning/</li> </ul>"},{"location":"end-to-end-dog-vision-v2/#tk-model-0-train-a-model-on-10-of-the-training-data","title":"TK - Model 0 - Train a model on 10% of the training data\u00b6","text":""},{"location":"end-to-end-dog-vision-v2/#tk-evaluate-model-0-on-the-test-data","title":"TK - Evaluate Model 0 on the test data\u00b6","text":""},{"location":"end-to-end-dog-vision-v2/#tk-model-1-train-a-model-on-100-of-the-training-data","title":"TK - Model 1 - Train a model on 100% of the training data\u00b6","text":"<p>Repeat the process for the above and compare the results</p>"},{"location":"end-to-end-dog-vision-v2/#tk-evaluate-model-1-on-the-test-data","title":"TK - Evaluate Model 1 on the test data\u00b6","text":""},{"location":"end-to-end-dog-vision-v2/#tk-make-and-evaluate-predictions-of-the-best-model","title":"TK - Make and evaluate predictions of the best model\u00b6","text":""},{"location":"end-to-end-dog-vision-v2/#tk-accuracy-per-class","title":"TK - Accuracy per class\u00b6","text":""},{"location":"end-to-end-dog-vision-v2/#tk-finding-the-most-wrong-examples","title":"TK - Finding the most wrong examples\u00b6","text":""},{"location":"end-to-end-dog-vision-v2/#tk-create-a-confusion-matrix","title":"TK - Create a confusion matrix\u00b6","text":"<p>TK see docs: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn-metrics-confusionmatrixdisplay</p>"},{"location":"end-to-end-dog-vision-v2/#tk-save-and-load-the-best-model","title":"TK - Save and load the best model\u00b6","text":"<p>See here: https://www.tensorflow.org/tutorials/keras/save_and_load#new_high-level_keras_format</p> <p>TK Note: You may also see the \"SavedModel\" format as well as \".hdf5\" formats...</p>"},{"location":"end-to-end-dog-vision-v2/#tk-make-predictions-on-custom-images-with-the-best-model","title":"TK - Make predictions on custom images with the best model\u00b6","text":""},{"location":"end-to-end-dog-vision-v2/#tk-extensions-exercises","title":"TK - Extensions &amp; Exercises\u00b6","text":"<ul> <li>Create a machine learning app with Gradio to predict on images of dogs - https://www.gradio.app/</li> <li>Try a prediction on your own images of dogs and see if the model is correct</li> <li>Train a model on your own custom set of image classes, for example, apple vs banana vs orange</li> <li>More callbacks -</li> <li>Data augmentation -</li> <li>Other models - see tf.keras.applications or Kaggle Models</li> <li>ZTM TensorFlow course -<ul> <li>See further fine-tuning here</li> <li>See videos on my YouTube for a more comprehensive TensorFlow overview to get started</li> </ul> </li> </ul>"},{"location":"end-to-end-dog-vision-v2/#tk-try-data-augmentation","title":"TK - Try data augmentation\u00b6","text":"<p>See: https://www.tensorflow.org/tutorials/images/data_augmentation</p>"},{"location":"introduction-to-matplotlib/","title":"Introduction to Matplotlib","text":"In\u00a0[1]: Copied! <pre>import datetime\nprint(f\"Last updated: {datetime.datetime.now()}\")\n</pre> import datetime print(f\"Last updated: {datetime.datetime.now()}\") <pre>Last updated: 2023-09-15 10:31:02.471435\n</pre> In\u00a0[2]: Copied! <pre># Older versions of Jupyter Notebooks and matplotlib required this magic command \n# %matplotlib inline\n\n# Import matplotlib and matplotlib.pyplot\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nprint(f\"matplotlib version: {matplotlib.__version__}\")\n</pre> # Older versions of Jupyter Notebooks and matplotlib required this magic command  # %matplotlib inline  # Import matplotlib and matplotlib.pyplot import matplotlib import matplotlib.pyplot as plt  print(f\"matplotlib version: {matplotlib.__version__}\") <pre>matplotlib version: 3.6.3\n</pre> In\u00a0[3]: Copied! <pre># Create a simple plot, without the semi-colon\nplt.plot()\n</pre> # Create a simple plot, without the semi-colon plt.plot() Out[3]: <pre>[]</pre> In\u00a0[4]: Copied! <pre># With the semi-colon\nplt.plot();\n</pre> # With the semi-colon plt.plot(); In\u00a0[5]: Copied! <pre># You could use plt.show() if you want\nplt.plot()\nplt.show()\n</pre> # You could use plt.show() if you want plt.plot() plt.show() In\u00a0[6]: Copied! <pre># Let's add some data\nplt.plot([1, 2, 3, 4]);\n</pre> # Let's add some data plt.plot([1, 2, 3, 4]); In\u00a0[7]: Copied! <pre># Create some data\nx = [1, 2, 3, 4]\ny = [11, 22, 33, 44]\n</pre> # Create some data x = [1, 2, 3, 4] y = [11, 22, 33, 44] <p>A few quick things about a plot:</p> <ul> <li><code>x</code> is the horizontal axis.</li> <li><code>y</code> is the vertical axis.</li> <li>In a data point, <code>x</code> usually comes first, e.g. <code>(3, 4)</code> would be <code>(x=3, y=4)</code>.</li> <li>The same is happens in <code>matplotlib.pyplot.plot()</code>, <code>x</code> comes before <code>y</code>, e.g. <code>plt.plot(x, y)</code>.</li> </ul> In\u00a0[8]: Copied! <pre># Now a y-value too!\nplt.plot(x, y);\n</pre> # Now a y-value too! plt.plot(x, y); <p>Now let's try using the object-orientated version.</p> <p>We'll start by creating a figure with <code>plt.figure()</code>.</p> <p>And then we'll add an axes with <code>add_subplot</code>.</p> In\u00a0[9]: Copied! <pre># Creating a plot with the object-orientated verison\nfig = plt.figure() # create a figure\nax = fig.add_subplot() # add an axes \nplt.show()\n</pre> # Creating a plot with the object-orientated verison fig = plt.figure() # create a figure ax = fig.add_subplot() # add an axes  plt.show() <p>A note on the terminology:</p> <ul> <li>A <code>Figure</code> (e.g. <code>fig = plt.figure()</code>) is the final image in matplotlib (and it may contain one or more <code>Axes</code>), often shortened to <code>fig</code>.</li> <li>The <code>Axes</code> are an individual plot (e.g. <code>ax = fig.add_subplot()</code>), often shorted to <code>ax</code>.<ul> <li>One <code>Figure</code> can contain one or more <code>Axes</code>.</li> </ul> </li> <li>The <code>Axis</code> are x (horizontal), y (vertical), z (depth).</li> </ul> <p>Now let's add some data to our pevious plot.</p> In\u00a0[10]: Copied! <pre># Add some data to our previous plot \nfig = plt.figure()\nax = fig.add_axes([1, 1, 1, 1])\nax.plot(x, y)\nplt.show()\n</pre> # Add some data to our previous plot  fig = plt.figure() ax = fig.add_axes([1, 1, 1, 1]) ax.plot(x, y) plt.show() <p>But there's an easier way we can use <code>matplotlib.pyplot</code> to help us create a <code>Figure</code> with multiple potential <code>Axes</code>.</p> <p>And that's with <code>plt.subplots()</code>.</p> In\u00a0[11]: Copied! <pre># Create a Figure and multiple potential Axes and add some data\nfig, ax = plt.subplots()\nax.plot(x, y);\n</pre> # Create a Figure and multiple potential Axes and add some data fig, ax = plt.subplots() ax.plot(x, y); In\u00a0[12]: Copied! <pre># This is where the object orientated name comes from \ntype(fig), type(ax)\n</pre> # This is where the object orientated name comes from  type(fig), type(ax) Out[12]: <pre>(matplotlib.figure.Figure, matplotlib.axes._subplots.AxesSubplot)</pre> In\u00a0[13]: Copied! <pre># A matplotlib workflow\n\n# 0. Import and get matplotlib ready\n# %matplotlib inline # Not necessary in newer versions of Jupyter (e.g. 2022 onwards)\nimport matplotlib.pyplot as plt\n\n# 1. Prepare data\nx = [1, 2, 3, 4]\ny = [11, 22, 33, 44]\n\n# 2. Setup plot (Figure and Axes)\nfig, ax = plt.subplots(figsize=(10,10))\n\n# 3. Plot data\nax.plot(x, y)\n\n# 4. Customize plot\nax.set(title=\"Sample Simple Plot\", xlabel=\"x-axis\", ylabel=\"y-axis\")\n\n# 5. Save &amp; show\nfig.savefig(\"../images/simple-plot.png\")\n</pre> # A matplotlib workflow  # 0. Import and get matplotlib ready # %matplotlib inline # Not necessary in newer versions of Jupyter (e.g. 2022 onwards) import matplotlib.pyplot as plt  # 1. Prepare data x = [1, 2, 3, 4] y = [11, 22, 33, 44]  # 2. Setup plot (Figure and Axes) fig, ax = plt.subplots(figsize=(10,10))  # 3. Plot data ax.plot(x, y)  # 4. Customize plot ax.set(title=\"Sample Simple Plot\", xlabel=\"x-axis\", ylabel=\"y-axis\")  # 5. Save &amp; show fig.savefig(\"../images/simple-plot.png\") In\u00a0[14]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[15]: Copied! <pre># Create an array\nx = np.linspace(0, 10, 100)\nx[:10]\n</pre> # Create an array x = np.linspace(0, 10, 100) x[:10] Out[15]: <pre>array([0.        , 0.1010101 , 0.2020202 , 0.3030303 , 0.4040404 ,\n       0.50505051, 0.60606061, 0.70707071, 0.80808081, 0.90909091])</pre> In\u00a0[16]: Copied! <pre># The default plot is line\nfig, ax = plt.subplots()\nax.plot(x, x**2);\n</pre> # The default plot is line fig, ax = plt.subplots() ax.plot(x, x**2); In\u00a0[17]: Copied! <pre># Need to recreate our figure and axis instances when we want a new figure\nfig, ax = plt.subplots()\nax.scatter(x, np.exp(x));\n</pre> # Need to recreate our figure and axis instances when we want a new figure fig, ax = plt.subplots() ax.scatter(x, np.exp(x)); In\u00a0[18]: Copied! <pre>fig, ax = plt.subplots()\nax.scatter(x, np.sin(x));\n</pre> fig, ax = plt.subplots() ax.scatter(x, np.sin(x)); In\u00a0[19]: Copied! <pre># You can make plots from a dictionary\nnut_butter_prices = {\"Almond butter\": 10,\n                     \"Peanut butter\": 8,\n                     \"Cashew butter\": 12}\nfig, ax = plt.subplots()\nax.bar(nut_butter_prices.keys(), nut_butter_prices.values())\nax.set(title=\"Dan's Nut Butter Store\", ylabel=\"Price ($)\");\n</pre> # You can make plots from a dictionary nut_butter_prices = {\"Almond butter\": 10,                      \"Peanut butter\": 8,                      \"Cashew butter\": 12} fig, ax = plt.subplots() ax.bar(nut_butter_prices.keys(), nut_butter_prices.values()) ax.set(title=\"Dan's Nut Butter Store\", ylabel=\"Price ($)\"); In\u00a0[20]: Copied! <pre>fig, ax = plt.subplots()\nax.barh(list(nut_butter_prices.keys()), list(nut_butter_prices.values()));\n</pre> fig, ax = plt.subplots() ax.barh(list(nut_butter_prices.keys()), list(nut_butter_prices.values())); In\u00a0[21]: Copied! <pre># Make some data from a normal distribution\nx = np.random.randn(1000) # pulls data from a normal distribution\n\nfig, ax = plt.subplots()\nax.hist(x);\n</pre> # Make some data from a normal distribution x = np.random.randn(1000) # pulls data from a normal distribution  fig, ax = plt.subplots() ax.hist(x); In\u00a0[22]: Copied! <pre>x = np.random.random(1000) # random data from random distribution\n\nfig, ax = plt.subplots()\nax.hist(x);\n</pre> x = np.random.random(1000) # random data from random distribution  fig, ax = plt.subplots() ax.hist(x); In\u00a0[23]: Copied! <pre># Option 1: Create 4 subplots with each Axes having its own variable name\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, \n                                             ncols=2, \n                                             figsize=(10, 5))\n\n# Plot data to each axis\nax1.plot(x, x/2);\nax2.scatter(np.random.random(10), np.random.random(10));\nax3.bar(nut_butter_prices.keys(), nut_butter_prices.values());\nax4.hist(np.random.randn(1000));\n</pre> # Option 1: Create 4 subplots with each Axes having its own variable name fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2,                                               ncols=2,                                               figsize=(10, 5))  # Plot data to each axis ax1.plot(x, x/2); ax2.scatter(np.random.random(10), np.random.random(10)); ax3.bar(nut_butter_prices.keys(), nut_butter_prices.values()); ax4.hist(np.random.randn(1000)); In\u00a0[24]: Copied! <pre># Option 2: Create 4 subplots with a single ax variable\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 5))\n\n# Index the ax variable to plot data\nax[0, 0].plot(x, x/2);\nax[0, 1].scatter(np.random.random(10), np.random.random(10));\nax[1, 0].bar(nut_butter_prices.keys(), nut_butter_prices.values());\nax[1, 1].hist(np.random.randn(1000));\n</pre> # Option 2: Create 4 subplots with a single ax variable fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 5))  # Index the ax variable to plot data ax[0, 0].plot(x, x/2); ax[0, 1].scatter(np.random.random(10), np.random.random(10)); ax[1, 0].bar(nut_butter_prices.keys(), nut_butter_prices.values()); ax[1, 1].hist(np.random.randn(1000)); In\u00a0[25]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd <p>Now we need some data to check out.</p> In\u00a0[26]: Copied! <pre># Let's import the car_sales dataset\ncar_sales = pd.read_csv(\"../data/car-sales.csv\")\ncar_sales\n</pre> # Let's import the car_sales dataset car_sales = pd.read_csv(\"../data/car-sales.csv\") car_sales Out[26]: Make Colour Odometer (KM) Doors Price 0 Toyota White 150043 4 $4,000.00 1 Honda Red 87899 4 $5,000.00 2 Toyota Blue 32549 3 $7,000.00 3 BMW Black 11179 5 $22,000.00 4 Nissan White 213095 4 $3,500.00 5 Toyota Green 99213 4 $4,500.00 6 Honda Blue 45698 4 $7,500.00 7 Honda Blue 54738 4 $7,000.00 8 Toyota White 60000 4 $6,250.00 9 Nissan White 31600 4 $9,700.00 In\u00a0[27]: Copied! <pre># Start with some dummy data\nts = pd.Series(np.random.randn(1000),\n               index=pd.date_range('1/1/2024', periods=1000))\n\n# Note: ts = short for time series (data over time)\nts\n</pre> # Start with some dummy data ts = pd.Series(np.random.randn(1000),                index=pd.date_range('1/1/2024', periods=1000))  # Note: ts = short for time series (data over time) ts Out[27]: <pre>2024-01-01   -0.195994\n2024-01-02   -1.022610\n2024-01-03   -0.202821\n2024-01-04    0.640333\n2024-01-05   -0.999877\n                ...   \n2026-09-22    0.096283\n2026-09-23    1.466828\n2026-09-24   -0.149209\n2026-09-25   -0.161122\n2026-09-26   -0.168698\nFreq: D, Length: 1000, dtype: float64</pre> <p>Great! We've got some random values across time.</p> <p>Now let's add up the data cumulatively overtime with <code>DataFrame.cumsum()</code> (<code>cumsum</code> is short for cumulative sum or continaully adding one thing to the next and so on).</p> In\u00a0[28]: Copied! <pre># Add up the values cumulatively\nts.cumsum()\n</pre> # Add up the values cumulatively ts.cumsum() Out[28]: <pre>2024-01-01   -0.195994\n2024-01-02   -1.218604\n2024-01-03   -1.421425\n2024-01-04   -0.781092\n2024-01-05   -1.780968\n                ...   \n2026-09-22   -1.518964\n2026-09-23   -0.052136\n2026-09-24   -0.201345\n2026-09-25   -0.362467\n2026-09-26   -0.531165\nFreq: D, Length: 1000, dtype: float64</pre> <p>We can now visualize the values by calling the <code>plot()</code> method on the DataFrame and specifying the kind of plot we'd like with the <code>kind</code> parameter.</p> <p>In our case, the kind we'd like is a line plot, hence <code>kind=\"line\"</code> (this is the default for the <code>plot()</code> method).</p> In\u00a0[29]: Copied! <pre># Plot the values over time with a line plot (note: both of these will return the same thing)\n# ts.cumsum().plot() # kind=\"line\" is set by default\nts.cumsum().plot(kind=\"line\");\n</pre> # Plot the values over time with a line plot (note: both of these will return the same thing) # ts.cumsum().plot() # kind=\"line\" is set by default ts.cumsum().plot(kind=\"line\"); In\u00a0[30]: Copied! <pre># Import the car sales data \ncar_sales = pd.read_csv(\"../data/car-sales.csv\")\n\n# Remove price column symbols\ncar_sales[\"Price\"] = car_sales[\"Price\"].str.replace('[\\$\\,\\.]', '', \n                                                    regex=True) # Tell pandas to replace using regex\ncar_sales\n</pre> # Import the car sales data  car_sales = pd.read_csv(\"../data/car-sales.csv\")  # Remove price column symbols car_sales[\"Price\"] = car_sales[\"Price\"].str.replace('[\\$\\,\\.]', '',                                                      regex=True) # Tell pandas to replace using regex car_sales Out[30]: Make Colour Odometer (KM) Doors Price 0 Toyota White 150043 4 400000 1 Honda Red 87899 4 500000 2 Toyota Blue 32549 3 700000 3 BMW Black 11179 5 2200000 4 Nissan White 213095 4 350000 5 Toyota Green 99213 4 450000 6 Honda Blue 45698 4 750000 7 Honda Blue 54738 4 700000 8 Toyota White 60000 4 625000 9 Nissan White 31600 4 970000 In\u00a0[31]: Copied! <pre># Remove last two zeros\ncar_sales[\"Price\"] = car_sales[\"Price\"].str[:-2]\ncar_sales\n</pre> # Remove last two zeros car_sales[\"Price\"] = car_sales[\"Price\"].str[:-2] car_sales Out[31]: Make Colour Odometer (KM) Doors Price 0 Toyota White 150043 4 4000 1 Honda Red 87899 4 5000 2 Toyota Blue 32549 3 7000 3 BMW Black 11179 5 22000 4 Nissan White 213095 4 3500 5 Toyota Green 99213 4 4500 6 Honda Blue 45698 4 7500 7 Honda Blue 54738 4 7000 8 Toyota White 60000 4 6250 9 Nissan White 31600 4 9700 In\u00a0[32]: Copied! <pre># Add a date column\ncar_sales[\"Sale Date\"] = pd.date_range(\"1/1/2024\", periods=len(car_sales))\ncar_sales\n</pre> # Add a date column car_sales[\"Sale Date\"] = pd.date_range(\"1/1/2024\", periods=len(car_sales)) car_sales Out[32]: Make Colour Odometer (KM) Doors Price Sale Date 0 Toyota White 150043 4 4000 2024-01-01 1 Honda Red 87899 4 5000 2024-01-02 2 Toyota Blue 32549 3 7000 2024-01-03 3 BMW Black 11179 5 22000 2024-01-04 4 Nissan White 213095 4 3500 2024-01-05 5 Toyota Green 99213 4 4500 2024-01-06 6 Honda Blue 45698 4 7500 2024-01-07 7 Honda Blue 54738 4 7000 2024-01-08 8 Toyota White 60000 4 6250 2024-01-09 9 Nissan White 31600 4 9700 2024-01-10 In\u00a0[33]: Copied! <pre># Make total sales column (doesn't work, adds as string)\n#car_sales[\"Total Sales\"] = car_sales[\"Price\"].cumsum()\n\n# Oops... want them as int's not string\ncar_sales[\"Total Sales\"] = car_sales[\"Price\"].astype(int).cumsum()\ncar_sales\n</pre> # Make total sales column (doesn't work, adds as string) #car_sales[\"Total Sales\"] = car_sales[\"Price\"].cumsum()  # Oops... want them as int's not string car_sales[\"Total Sales\"] = car_sales[\"Price\"].astype(int).cumsum() car_sales Out[33]: Make Colour Odometer (KM) Doors Price Sale Date Total Sales 0 Toyota White 150043 4 4000 2024-01-01 4000 1 Honda Red 87899 4 5000 2024-01-02 9000 2 Toyota Blue 32549 3 7000 2024-01-03 16000 3 BMW Black 11179 5 22000 2024-01-04 38000 4 Nissan White 213095 4 3500 2024-01-05 41500 5 Toyota Green 99213 4 4500 2024-01-06 46000 6 Honda Blue 45698 4 7500 2024-01-07 53500 7 Honda Blue 54738 4 7000 2024-01-08 60500 8 Toyota White 60000 4 6250 2024-01-09 66750 9 Nissan White 31600 4 9700 2024-01-10 76450 In\u00a0[34]: Copied! <pre>car_sales.plot(x='Sale Date', y='Total Sales');\n</pre> car_sales.plot(x='Sale Date', y='Total Sales'); In\u00a0[35]: Copied! <pre># Note: In previous versions of matplotlib and pandas, have the \"Price\" column as a string would\n# return an error\ncar_sales[\"Price\"] = car_sales[\"Price\"].astype(str)\n\n# Plot a scatter plot\ncar_sales.plot(x=\"Odometer (KM)\", y=\"Price\", kind=\"scatter\");\n</pre> # Note: In previous versions of matplotlib and pandas, have the \"Price\" column as a string would # return an error car_sales[\"Price\"] = car_sales[\"Price\"].astype(str)  # Plot a scatter plot car_sales.plot(x=\"Odometer (KM)\", y=\"Price\", kind=\"scatter\"); <p>Having the <code>Price</code> column as an <code>int</code> returns a much better looking y-axis.</p> In\u00a0[36]: Copied! <pre># Convert Price to int\ncar_sales[\"Price\"] = car_sales[\"Price\"].astype(int)\n\n# Plot a scatter plot\ncar_sales.plot(x=\"Odometer (KM)\", y=\"Price\", kind='scatter');\n</pre> # Convert Price to int car_sales[\"Price\"] = car_sales[\"Price\"].astype(int)  # Plot a scatter plot car_sales.plot(x=\"Odometer (KM)\", y=\"Price\", kind='scatter'); In\u00a0[37]: Copied! <pre># Create 10 random samples across 4 columns\nx = np.random.rand(10, 4)\nx\n</pre> # Create 10 random samples across 4 columns x = np.random.rand(10, 4) x Out[37]: <pre>array([[0.65745479, 0.42745471, 0.61990211, 0.01218935],\n       [0.10699156, 0.6546944 , 0.5915984 , 0.55011077],\n       [0.50720269, 0.2725063 , 0.95817204, 0.67309876],\n       [0.33016817, 0.85921522, 0.02778741, 0.36043001],\n       [0.8850031 , 0.82582603, 0.58275893, 0.10393635],\n       [0.70596769, 0.15698541, 0.43727796, 0.03307697],\n       [0.55611843, 0.86959028, 0.49525034, 0.06849191],\n       [0.19340766, 0.69988787, 0.89546643, 0.368045  ],\n       [0.01834179, 0.74501467, 0.06589424, 0.58463789],\n       [0.31159084, 0.4001198 , 0.59601375, 0.64712406]])</pre> In\u00a0[82]: Copied! <pre># Turn the data into a DataFrame\ndf = pd.DataFrame(x, columns=['a', 'b', 'c', 'd'])\ndf\n</pre> # Turn the data into a DataFrame df = pd.DataFrame(x, columns=['a', 'b', 'c', 'd']) df Out[82]: a b c d 0 1.326093 -1.179144 -1.228776 0.320765 1 -0.473547 -0.226671 -0.784430 0.192451 2 2.288607 -1.090920 -0.204312 0.486072 3 1.591945 0.320072 2.949674 -1.306000 4 -1.873583 1.132770 1.423901 0.928743 5 -1.121281 -0.640948 -0.527283 0.242460 6 1.302475 -0.295322 3.141830 0.558532 7 -1.663926 1.767556 -0.558923 0.750767 8 -0.658601 0.278021 0.854262 0.012043 9 -0.734160 -1.011017 0.842804 -0.008819 <p>We can plot a bar chart directly with the <code>bar()</code> method on the DataFrame.</p> In\u00a0[39]: Copied! <pre># Plot a bar chart\ndf.plot.bar();\n</pre> # Plot a bar chart df.plot.bar(); <p>And we can also do the same thing passing the <code>kind=\"bar\"</code> parameter to <code>DataFrame.plot()</code>.</p> In\u00a0[40]: Copied! <pre># Plot a bar chart with the kind parameter\ndf.plot(kind='bar');\n</pre> # Plot a bar chart with the kind parameter df.plot(kind='bar'); <p>Let's try a bar plot on the <code>car_sales</code> DataFrame.</p> <p>This time we'll specify the <code>x</code> and <code>y</code> axis values.</p> In\u00a0[41]: Copied! <pre># Plot a bar chart from car_sales DataFrame\ncar_sales.plot(x=\"Make\", \n               y=\"Odometer (KM)\", \n               kind=\"bar\");\n</pre> # Plot a bar chart from car_sales DataFrame car_sales.plot(x=\"Make\",                 y=\"Odometer (KM)\",                 kind=\"bar\"); In\u00a0[42]: Copied! <pre>car_sales[\"Odometer (KM)\"].plot.hist();\n</pre> car_sales[\"Odometer (KM)\"].plot.hist(); In\u00a0[43]: Copied! <pre>car_sales[\"Odometer (KM)\"].plot(kind=\"hist\");\n</pre> car_sales[\"Odometer (KM)\"].plot(kind=\"hist\"); <p>Changing the <code>bins</code> parameter we can put our data into different numbers of collections.</p> <p>For example, by default <code>bins=10</code> (10 groups of data), let's see what happens when we change it to <code>bins=20</code>.</p> In\u00a0[44]: Copied! <pre># Default number of bins is 10 \ncar_sales[\"Odometer (KM)\"].plot.hist(bins=20);\n</pre> # Default number of bins is 10  car_sales[\"Odometer (KM)\"].plot.hist(bins=20); <p>To practice, let's create a histogram of the <code>Price</code> column.</p> In\u00a0[45]: Copied! <pre># Create a histogram of the Price column\ncar_sales[\"Price\"].plot.hist(bins=10);\n</pre> # Create a histogram of the Price column car_sales[\"Price\"].plot.hist(bins=10); <p>And to practice even further, how about we try another dataset?</p> <p>Namely, let's create some plots using the heart disease dataset we've worked on before.</p> In\u00a0[46]: Copied! <pre># Import the heart disease dataset\nheart_disease = pd.read_csv(\"../data/heart-disease.csv\")\nheart_disease.head()\n</pre> # Import the heart disease dataset heart_disease = pd.read_csv(\"../data/heart-disease.csv\") heart_disease.head() Out[46]: age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target 0 63 1 3 145 233 1 0 150 0 2.3 0 0 1 1 1 37 1 2 130 250 0 1 187 0 3.5 0 0 2 1 2 41 0 1 130 204 0 0 172 0 1.4 2 0 2 1 3 56 1 1 120 236 0 1 178 0 0.8 2 0 2 1 4 57 0 0 120 354 0 1 163 1 0.6 2 0 2 1 In\u00a0[47]: Copied! <pre># Create a histogram of the age column\nheart_disease[\"age\"].plot.hist(bins=50);\n</pre> # Create a histogram of the age column heart_disease[\"age\"].plot.hist(bins=50); <p>What does this tell you about the spread of heart disease data across different ages?</p> In\u00a0[48]: Copied! <pre># Inspect the data\nheart_disease.head()\n</pre> # Inspect the data heart_disease.head() Out[48]: age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target 0 63 1 3 145 233 1 0 150 0 2.3 0 0 1 1 1 37 1 2 130 250 0 1 187 0 3.5 0 0 2 1 2 41 0 1 130 204 0 0 172 0 1.4 2 0 2 1 3 56 1 1 120 236 0 1 178 0 0.8 2 0 2 1 4 57 0 0 120 354 0 1 163 1 0.6 2 0 2 1 <p>Since all of our columns are numeric in value, let's try and create a histogram of each column.</p> In\u00a0[49]: Copied! <pre>heart_disease.plot.hist(figsize=(5, 20), \n                        subplots=True);\n</pre> heart_disease.plot.hist(figsize=(5, 20),                          subplots=True); <p>Hmmm... is this a very helpful plot?</p> <p>Perhaps not.</p> <p>Sometimes you can visualize too much on the one plot and it becomes confusing.</p> <p>Best to start with less and gradually increase.</p> In\u00a0[50]: Copied! <pre># Perform data analysis on patients over 50\nover_50 = heart_disease[heart_disease[\"age\"] &gt; 50]\nover_50\n</pre> # Perform data analysis on patients over 50 over_50 = heart_disease[heart_disease[\"age\"] &gt; 50] over_50 Out[50]: age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target 0 63 1 3 145 233 1 0 150 0 2.3 0 0 1 1 3 56 1 1 120 236 0 1 178 0 0.8 2 0 2 1 4 57 0 0 120 354 0 1 163 1 0.6 2 0 2 1 5 57 1 0 140 192 0 1 148 0 0.4 1 0 1 1 6 56 0 1 140 294 0 0 153 0 1.3 1 0 2 1 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 297 59 1 0 164 176 1 0 90 0 1.0 1 2 1 0 298 57 0 0 140 241 0 1 123 1 0.2 1 0 3 0 300 68 1 0 144 193 1 1 141 0 3.4 1 2 3 0 301 57 1 0 130 131 0 1 115 1 1.2 1 1 3 0 302 57 0 1 130 236 0 0 174 0 0.0 1 1 2 0 <p>208 rows \u00d7 14 columns</p> <p>Now let's create a scatter plot directly from the pandas DataFrame.</p> <p>This is quite easy to do but is a bit limited in terms of customization.</p> <p>Let's visualize patients over 50 cholesterol levels.</p> <p>We can visualize which patients have or don't have heart disease by colouring the samples to be in line with the <code>target</code> column (e.g. <code>0</code> = no heart disease, <code>1</code> = heart disease).</p> In\u00a0[51]: Copied! <pre># Create a scatter plot directly from the pandas DataFrame\nover_50.plot(kind=\"scatter\",\n             x=\"age\", \n             y=\"chol\", \n             c=\"target\", # colour the dots by target value\n             figsize=(10, 6));\n</pre> # Create a scatter plot directly from the pandas DataFrame over_50.plot(kind=\"scatter\",              x=\"age\",               y=\"chol\",               c=\"target\", # colour the dots by target value              figsize=(10, 6)); <p>We can recreate the same plot using <code>plt.subplots()</code> and then passing the Axes variable (<code>ax</code>) to the pandas <code>plot()</code> method.</p> In\u00a0[52]: Copied! <pre># Create a Figure and Axes instance\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot data from the DataFrame to the ax object\nover_50.plot(kind=\"scatter\", \n             x=\"age\", \n             y=\"chol\", \n             c=\"target\", \n             ax=ax); # set the target Axes\n\n# Customize the x-axis limits (to be within our target age ranges)\nax.set_xlim([45, 100]);\n</pre> # Create a Figure and Axes instance fig, ax = plt.subplots(figsize=(10, 6))  # Plot data from the DataFrame to the ax object over_50.plot(kind=\"scatter\",               x=\"age\",               y=\"chol\",               c=\"target\",               ax=ax); # set the target Axes  # Customize the x-axis limits (to be within our target age ranges) ax.set_xlim([45, 100]); <p>Now instead of plotting directly from the pandas DataFrame, we can make a bit more of a comprehensive plot by plotting data directly to a target Axes instance.</p> In\u00a0[53]: Copied! <pre># Create Figure and Axes instance\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot data directly to the Axes intance\nscatter = ax.scatter(over_50[\"age\"], \n                     over_50[\"chol\"], \n                     c=over_50[\"target\"]) # Colour the data with the \"target\" column\n\n# Customize the plot parameters \nax.set(title=\"Heart Disease and Cholesterol Levels\",\n       xlabel=\"Age\",\n       ylabel=\"Cholesterol\");\n\n# Setup the legend\nax.legend(*scatter.legend_elements(), \n          title=\"Target\");\n</pre> # Create Figure and Axes instance fig, ax = plt.subplots(figsize=(10, 6))  # Plot data directly to the Axes intance scatter = ax.scatter(over_50[\"age\"],                       over_50[\"chol\"],                       c=over_50[\"target\"]) # Colour the data with the \"target\" column  # Customize the plot parameters  ax.set(title=\"Heart Disease and Cholesterol Levels\",        xlabel=\"Age\",        ylabel=\"Cholesterol\");  # Setup the legend ax.legend(*scatter.legend_elements(),            title=\"Target\"); <p>What if we wanted a horizontal line going across with the mean of <code>heart_disease[\"chol\"]</code>?</p> <p>We do so with the <code>Axes.axhline()</code> method.</p> In\u00a0[54]: Copied! <pre># Create the plot\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot the data\nscatter = ax.scatter(over_50[\"age\"], \n                     over_50[\"chol\"], \n                     c=over_50[\"target\"])\n\n# Customize the plot\nax.set(title=\"Heart Disease and Cholesterol Levels\",\n       xlabel=\"Age\",\n       ylabel=\"Cholesterol\");\n\n# Add a legned\nax.legend(*scatter.legend_elements(), \n          title=\"Target\")\n\n# Add a meanline\nax.axhline(over_50[\"chol\"].mean(),\n           linestyle=\"--\"); # style the line to make it look nice\n</pre> # Create the plot fig, ax = plt.subplots(figsize=(10, 6))  # Plot the data scatter = ax.scatter(over_50[\"age\"],                       over_50[\"chol\"],                       c=over_50[\"target\"])  # Customize the plot ax.set(title=\"Heart Disease and Cholesterol Levels\",        xlabel=\"Age\",        ylabel=\"Cholesterol\");  # Add a legned ax.legend(*scatter.legend_elements(),            title=\"Target\")  # Add a meanline ax.axhline(over_50[\"chol\"].mean(),            linestyle=\"--\"); # style the line to make it look nice In\u00a0[55]: Copied! <pre># Setup plot (2 rows, 1 column)\nfig, (ax0, ax1) = plt.subplots(nrows=2, # 2 rows\n                               ncols=1, # 1 column \n                               sharex=True, # both plots should use the same x-axis \n                               figsize=(10, 8))\n\n# ---------- Axis 0: Heart Disease and Cholesterol Levels ----------\n\n# Add data for ax0\nscatter = ax0.scatter(over_50[\"age\"], \n                      over_50[\"chol\"], \n                      c=over_50[\"target\"])\n# Customize ax0\nax0.set(title=\"Heart Disease and Cholesterol Levels\",\n        ylabel=\"Cholesterol\")\nax0.legend(*scatter.legend_elements(), title=\"Target\")\n\n# Setup a mean line\nax0.axhline(y=over_50[\"chol\"].mean(), \n            color='b', \n            linestyle='--', \n            label=\"Average\")\n\n# ---------- Axis 1: Heart Disease and Max Heart Rate Levels ----------\n\n# Add data for ax1\nscatter = ax1.scatter(over_50[\"age\"], \n                      over_50[\"thalach\"], \n                      c=over_50[\"target\"])\n\n# Customize ax1\nax1.set(title=\"Heart Disease and Max Heart Rate Levels\",\n        xlabel=\"Age\",\n        ylabel=\"Max Heart Rate\")\nax1.legend(*scatter.legend_elements(), title=\"Target\")\n\n# Setup a mean line\nax1.axhline(y=over_50[\"thalach\"].mean(), \n            color='b', \n            linestyle='--', \n            label=\"Average\")\n\n# Title the figure\nfig.suptitle('Heart Disease Analysis', \n             fontsize=16, \n             fontweight='bold');\n</pre> # Setup plot (2 rows, 1 column) fig, (ax0, ax1) = plt.subplots(nrows=2, # 2 rows                                ncols=1, # 1 column                                 sharex=True, # both plots should use the same x-axis                                 figsize=(10, 8))  # ---------- Axis 0: Heart Disease and Cholesterol Levels ----------  # Add data for ax0 scatter = ax0.scatter(over_50[\"age\"],                        over_50[\"chol\"],                        c=over_50[\"target\"]) # Customize ax0 ax0.set(title=\"Heart Disease and Cholesterol Levels\",         ylabel=\"Cholesterol\") ax0.legend(*scatter.legend_elements(), title=\"Target\")  # Setup a mean line ax0.axhline(y=over_50[\"chol\"].mean(),              color='b',              linestyle='--',              label=\"Average\")  # ---------- Axis 1: Heart Disease and Max Heart Rate Levels ----------  # Add data for ax1 scatter = ax1.scatter(over_50[\"age\"],                        over_50[\"thalach\"],                        c=over_50[\"target\"])  # Customize ax1 ax1.set(title=\"Heart Disease and Max Heart Rate Levels\",         xlabel=\"Age\",         ylabel=\"Max Heart Rate\") ax1.legend(*scatter.legend_elements(), title=\"Target\")  # Setup a mean line ax1.axhline(y=over_50[\"thalach\"].mean(),              color='b',              linestyle='--',              label=\"Average\")  # Title the figure fig.suptitle('Heart Disease Analysis',               fontsize=16,               fontweight='bold'); In\u00a0[56]: Copied! <pre># Check the available styles\nplt.style.available\n</pre> # Check the available styles plt.style.available Out[56]: <pre>['Solarize_Light2',\n '_classic_test_patch',\n '_mpl-gallery',\n '_mpl-gallery-nogrid',\n 'bmh',\n 'classic',\n 'dark_background',\n 'fast',\n 'fivethirtyeight',\n 'ggplot',\n 'grayscale',\n 'seaborn-v0_8',\n 'seaborn-v0_8-bright',\n 'seaborn-v0_8-colorblind',\n 'seaborn-v0_8-dark',\n 'seaborn-v0_8-dark-palette',\n 'seaborn-v0_8-darkgrid',\n 'seaborn-v0_8-deep',\n 'seaborn-v0_8-muted',\n 'seaborn-v0_8-notebook',\n 'seaborn-v0_8-paper',\n 'seaborn-v0_8-pastel',\n 'seaborn-v0_8-poster',\n 'seaborn-v0_8-talk',\n 'seaborn-v0_8-ticks',\n 'seaborn-v0_8-white',\n 'seaborn-v0_8-whitegrid',\n 'tableau-colorblind10']</pre> <p>Before we change the style of a plot, let's remind ourselves what the default plot style looks like.</p> In\u00a0[57]: Copied! <pre># Plot before changing style\ncar_sales[\"Price\"].plot();\n</pre> # Plot before changing style car_sales[\"Price\"].plot(); <p>Wonderful!</p> <p>Now let's change the style of our future plots using the <code>plt.style.use(style)</code> method.</p> <p>Where the <code>style</code> parameter is one of the available matplotlib styles.</p> <p>How about we try <code>\"seaborn-v0_8-whitegrid\"</code> (seaborn is another common visualization library built on top of matplotlib)?</p> In\u00a0[58]: Copied! <pre># Change the style of our future plots\nplt.style.use(\"seaborn-v0_8-whitegrid\")\n</pre> # Change the style of our future plots plt.style.use(\"seaborn-v0_8-whitegrid\") In\u00a0[59]: Copied! <pre># Plot the same plot as before\ncar_sales[\"Price\"].plot();\n</pre> # Plot the same plot as before car_sales[\"Price\"].plot(); <p>Wonderful!</p> <p>Notice the slightly different styling of the plot?</p> <p>Some styles change more than others.</p> <p>How about we try <code>\"fivethirtyeight\"</code>?</p> In\u00a0[60]: Copied! <pre># Change the plot style\nplt.style.use(\"fivethirtyeight\")\n</pre> # Change the plot style plt.style.use(\"fivethirtyeight\") In\u00a0[61]: Copied! <pre>car_sales[\"Price\"].plot();\n</pre> car_sales[\"Price\"].plot(); <p>Ohhh that's a nice looking plot!</p> <p>Does the style carry over for another type of plot?</p> <p>How about we try a scatter plot?</p> In\u00a0[62]: Copied! <pre>car_sales.plot(x=\"Odometer (KM)\", \n               y=\"Price\", \n               kind=\"scatter\");\n</pre> car_sales.plot(x=\"Odometer (KM)\",                 y=\"Price\",                 kind=\"scatter\"); <p>It does!</p> <p>Looks like we may need to adjust the spacing on our x-axis though.</p> <p>What about another style?</p> <p>Let's try <code>\"ggplot\"</code>.</p> In\u00a0[63]: Copied! <pre># Change the plot style\nplt.style.use(\"ggplot\")\n</pre> # Change the plot style plt.style.use(\"ggplot\") In\u00a0[64]: Copied! <pre>car_sales[\"Price\"].plot.hist();\n</pre> car_sales[\"Price\"].plot.hist(); <p>Cool!</p> <p>Now how can we go back to the default style?</p> <p>Hint: with <code>\"default\"</code>.</p> In\u00a0[65]: Copied! <pre># Change the plot style back to the default \nplt.style.use(\"default\")\n</pre> # Change the plot style back to the default  plt.style.use(\"default\") In\u00a0[66]: Copied! <pre>car_sales[\"Price\"].plot.hist();\n</pre> car_sales[\"Price\"].plot.hist(); In\u00a0[67]: Copied! <pre># Create random data\nx = np.random.randn(10, 4)\nx\n</pre> # Create random data x = np.random.randn(10, 4) x Out[67]: <pre>array([[ 1.32609318, -1.17914432, -1.22877557,  0.3207647 ],\n       [-0.47354665, -0.22667084, -0.78442964,  0.19245061],\n       [ 2.28860662, -1.09092006, -0.20431235,  0.48607205],\n       [ 1.59194535,  0.32007245,  2.94967405, -1.30600011],\n       [-1.87358324,  1.13277017,  1.42390128,  0.92874349],\n       [-1.12128117, -0.64094816, -0.52728296,  0.24245977],\n       [ 1.30247519, -0.29532163,  3.14183048,  0.55853199],\n       [-1.66392559,  1.76755595, -0.55892306,  0.75076739],\n       [-0.65860121,  0.27802076,  0.85426195,  0.01204296],\n       [-0.73415989, -1.01101698,  0.84280402, -0.00881896]])</pre> In\u00a0[68]: Copied! <pre># Turn data into DataFrame with simple column names\ndf = pd.DataFrame(x, \n                  columns=['a', 'b', 'c', 'd'])\ndf\n</pre> # Turn data into DataFrame with simple column names df = pd.DataFrame(x,                    columns=['a', 'b', 'c', 'd']) df Out[68]: a b c d 0 1.326093 -1.179144 -1.228776 0.320765 1 -0.473547 -0.226671 -0.784430 0.192451 2 2.288607 -1.090920 -0.204312 0.486072 3 1.591945 0.320072 2.949674 -1.306000 4 -1.873583 1.132770 1.423901 0.928743 5 -1.121281 -0.640948 -0.527283 0.242460 6 1.302475 -0.295322 3.141830 0.558532 7 -1.663926 1.767556 -0.558923 0.750767 8 -0.658601 0.278021 0.854262 0.012043 9 -0.734160 -1.011017 0.842804 -0.008819 <p>Now let's plot the data from the DataFrame in a bar chart.</p> <p>This time we'll save the plot to a variable called <code>ax</code> (short for Axes).</p> In\u00a0[69]: Copied! <pre># Create a bar plot\nax = df.plot(kind=\"bar\")\n\n# Check the type of the ax variable\ntype(ax)\n</pre> # Create a bar plot ax = df.plot(kind=\"bar\")  # Check the type of the ax variable type(ax) Out[69]: <pre>matplotlib.axes._subplots.AxesSubplot</pre> <p>Excellent!</p> <p>We can see the type of our <code>ax</code> variable is of <code>AxesSubplot</code> which allows us to use all of the methods available in matplotlib for <code>Axes</code>.</p> <p>Let's set a few attributes of the plot with the <code>set()</code> method.</p> <p>Namely, we'll change the <code>title</code>, <code>xlabel</code> and <code>ylabel</code> to communicate what's being displayed.</p> In\u00a0[70]: Copied! <pre># Recreate the ax object\nax = df.plot(kind=\"bar\")\n\n# Set various attributes\nax.set(title=\"Random Number Bar Graph from DataFrame\", \n       xlabel=\"Row number\", \n       ylabel=\"Random number\");\n</pre> # Recreate the ax object ax = df.plot(kind=\"bar\")  # Set various attributes ax.set(title=\"Random Number Bar Graph from DataFrame\",         xlabel=\"Row number\",         ylabel=\"Random number\"); <p>Notice the legend is up in the top left corner by default, we can change that if we like with the <code>loc</code> parameter of the <code>legend()</code> method.</p> <p><code>loc</code> can be set as a string to reflect where the legend should be.</p> <p>By default it is set to <code>loc=\"best\"</code> which means matplotlib will try to figure out the best positioning for it.</p> <p>Let's try changing it to <code>\"loc=\"upper right\"</code>.</p> In\u00a0[71]: Copied! <pre># Recreate the ax object\nax = df.plot(kind=\"bar\")\n\n# Set various attributes\nax.set(title=\"Random Number Bar Graph from DataFrame\", \n       xlabel=\"Row number\", \n       ylabel=\"Random number\")\n\n# Change the legend position\nax.legend(loc=\"upper right\");\n</pre> # Recreate the ax object ax = df.plot(kind=\"bar\")  # Set various attributes ax.set(title=\"Random Number Bar Graph from DataFrame\",         xlabel=\"Row number\",         ylabel=\"Random number\")  # Change the legend position ax.legend(loc=\"upper right\"); <p>Nice!</p> <p>Is that a better fit?</p> <p>Perhaps not, but it goes to show how you can change the legend position if needed.</p> In\u00a0[72]: Copied! <pre># Setup the Figure and Axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Create a scatter plot with no cmap change (use default colormap)\nscatter = ax.scatter(over_50[\"age\"], \n                     over_50[\"chol\"], \n                     c=over_50[\"target\"],\n                     cmap=\"viridis\") # default cmap value\n\n# Add attributes to the plot\nax.set(title=\"Heart Disease and Cholesterol Levels\",\n       xlabel=\"Age\",\n       ylabel=\"Cholesterol\");\nax.axhline(y=over_50[\"chol\"].mean(), \n           c='b', \n           linestyle='--', \n           label=\"Average\");\nax.legend(*scatter.legend_elements(), \n          title=\"Target\");\n</pre> # Setup the Figure and Axes fig, ax = plt.subplots(figsize=(10, 6))  # Create a scatter plot with no cmap change (use default colormap) scatter = ax.scatter(over_50[\"age\"],                       over_50[\"chol\"],                       c=over_50[\"target\"],                      cmap=\"viridis\") # default cmap value  # Add attributes to the plot ax.set(title=\"Heart Disease and Cholesterol Levels\",        xlabel=\"Age\",        ylabel=\"Cholesterol\"); ax.axhline(y=over_50[\"chol\"].mean(),             c='b',             linestyle='--',             label=\"Average\"); ax.legend(*scatter.legend_elements(),            title=\"Target\"); <p>Wonderful!</p> <p>That plot doesn't look too bad.</p> <p>But what if we wanted to change the colours?</p> <p>There are many different <code>cmap</code> parameter options available in the colormap reference.</p> <p>How about we try <code>cmap=\"winter\"</code>?</p> <p>We can also change the colour of the horizontal line using the <code>color</code> parameter and setting it to a string of the colour we'd like (e.g. <code>color=\"r\"</code> for red).</p> In\u00a0[73]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 6))\n\n# Setup scatter plot with different cmap\nscatter = ax.scatter(over_50[\"age\"], \n                     over_50[\"chol\"], \n                     c=over_50[\"target\"], \n                     cmap=\"winter\") # Change cmap value \n\n# Add attributes to the plot with different color line\nax.set(title=\"Heart Disease and Cholesterol Levels\",\n       xlabel=\"Age\",\n       ylabel=\"Cholesterol\")\nax.axhline(y=over_50[\"chol\"].mean(), \n           color=\"r\", # Change color of line to \"r\" (for red)\n           linestyle='--', \n           label=\"Average\");\nax.legend(*scatter.legend_elements(), \n          title=\"Target\");\n</pre> fig, ax = plt.subplots(figsize=(10, 6))  # Setup scatter plot with different cmap scatter = ax.scatter(over_50[\"age\"],                       over_50[\"chol\"],                       c=over_50[\"target\"],                       cmap=\"winter\") # Change cmap value   # Add attributes to the plot with different color line ax.set(title=\"Heart Disease and Cholesterol Levels\",        xlabel=\"Age\",        ylabel=\"Cholesterol\") ax.axhline(y=over_50[\"chol\"].mean(),             color=\"r\", # Change color of line to \"r\" (for red)            linestyle='--',             label=\"Average\"); ax.legend(*scatter.legend_elements(),            title=\"Target\"); <p>Woohoo!</p> <p>The first plot looked nice, but I think I prefer the colours of this new plot better.</p> <p>For more on choosing colormaps in matplotlib, there's a sensational and in-depth tutorial in the matplotlib documentation.</p> In\u00a0[74]: Copied! <pre># Recreate double Axes plot from above with colour updates \nfig, (ax0, ax1) = plt.subplots(nrows=2, \n                               ncols=1, \n                               sharex=True, \n                               figsize=(10, 7))\n\n# ---------- Axis 0 ----------\nscatter = ax0.scatter(over_50[\"age\"], \n                      over_50[\"chol\"], \n                      c=over_50[\"target\"],\n                      cmap=\"winter\")\nax0.set(title=\"Heart Disease and Cholesterol Levels\",\n        ylabel=\"Cholesterol\")\n\n# Setup a mean line\nax0.axhline(y=over_50[\"chol\"].mean(), \n            color=\"r\", \n            linestyle=\"--\", \n            label=\"Average\");\nax0.legend(*scatter.legend_elements(), title=\"Target\")\n\n# ---------- Axis 1 ----------\nscatter = ax1.scatter(over_50[\"age\"], \n                      over_50[\"thalach\"], \n                      c=over_50[\"target\"],\n                      cmap=\"winter\")\nax1.set(title=\"Heart Disease and Max Heart Rate Levels\",\n        xlabel=\"Age\",\n        ylabel=\"Max Heart Rate\")\n\n# Setup a mean line\nax1.axhline(y=over_50[\"thalach\"].mean(), \n            color=\"r\", \n            linestyle=\"--\", \n            label=\"Average\");\nax1.legend(*scatter.legend_elements(), \n           title=\"Target\")\n\n# Title the figure\nfig.suptitle(\"Heart Disease Analysis\", \n             fontsize=16, \n             fontweight=\"bold\");\n</pre> # Recreate double Axes plot from above with colour updates  fig, (ax0, ax1) = plt.subplots(nrows=2,                                 ncols=1,                                 sharex=True,                                 figsize=(10, 7))  # ---------- Axis 0 ---------- scatter = ax0.scatter(over_50[\"age\"],                        over_50[\"chol\"],                        c=over_50[\"target\"],                       cmap=\"winter\") ax0.set(title=\"Heart Disease and Cholesterol Levels\",         ylabel=\"Cholesterol\")  # Setup a mean line ax0.axhline(y=over_50[\"chol\"].mean(),              color=\"r\",              linestyle=\"--\",              label=\"Average\"); ax0.legend(*scatter.legend_elements(), title=\"Target\")  # ---------- Axis 1 ---------- scatter = ax1.scatter(over_50[\"age\"],                        over_50[\"thalach\"],                        c=over_50[\"target\"],                       cmap=\"winter\") ax1.set(title=\"Heart Disease and Max Heart Rate Levels\",         xlabel=\"Age\",         ylabel=\"Max Heart Rate\")  # Setup a mean line ax1.axhline(y=over_50[\"thalach\"].mean(),              color=\"r\",              linestyle=\"--\",              label=\"Average\"); ax1.legend(*scatter.legend_elements(),             title=\"Target\")  # Title the figure fig.suptitle(\"Heart Disease Analysis\",               fontsize=16,               fontweight=\"bold\"); <p>Now let's recreate the plot from above but this time we'll change the axis limits.</p> <p>We can do so by using <code>Axes.set(xlim=[50, 80])</code> or <code>Axes.set(ylim=[60, 220])</code> where the inputs to <code>xlim</code> and <code>ylim</code> are a list of integers defining a range of values.</p> <p>For example, <code>xlim=[50, 80]</code> will set the x-axis values to start at <code>50</code> and end at <code>80</code>.</p> In\u00a0[75]: Copied! <pre># Recreate the plot from above with custom x and y axis ranges\nfig, (ax0, ax1) = plt.subplots(nrows=2, \n                               ncols=1, \n                               sharex=True, \n                               figsize=(10, 7))\nscatter = ax0.scatter(over_50[\"age\"], \n                      over_50[\"chol\"], \n                      c=over_50[\"target\"],\n                      cmap='winter')\nax0.set(title=\"Heart Disease and Cholesterol Levels\",\n        ylabel=\"Cholesterol\",\n        xlim=[50, 80]) # set the x-axis ranges \n\n# Setup a mean line\nax0.axhline(y=over_50[\"chol\"].mean(), \n            color=\"r\", \n            linestyle=\"--\", \n            label=\"Average\");\nax0.legend(*scatter.legend_elements(), title=\"Target\")\n\n# Axis 1, 1 (row 1, column 1)\nscatter = ax1.scatter(over_50[\"age\"], \n                      over_50[\"thalach\"], \n                      c=over_50[\"target\"],\n                      cmap='winter')\nax1.set(title=\"Heart Disease and Max Heart Rate Levels\",\n        xlabel=\"Age\",\n        ylabel=\"Max Heart Rate\",\n        ylim=[60, 220]) # change the y-axis range\n\n# Setup a mean line\nax1.axhline(y=over_50[\"thalach\"].mean(), \n            color=\"r\", \n            linestyle=\"--\", \n            label=\"Average\");\nax1.legend(*scatter.legend_elements(), \n           title=\"Target\")\n\n# Title the figure\nfig.suptitle(\"Heart Disease Analysis\", \n             fontsize=16, \n             fontweight=\"bold\");\n</pre> # Recreate the plot from above with custom x and y axis ranges fig, (ax0, ax1) = plt.subplots(nrows=2,                                 ncols=1,                                 sharex=True,                                 figsize=(10, 7)) scatter = ax0.scatter(over_50[\"age\"],                        over_50[\"chol\"],                        c=over_50[\"target\"],                       cmap='winter') ax0.set(title=\"Heart Disease and Cholesterol Levels\",         ylabel=\"Cholesterol\",         xlim=[50, 80]) # set the x-axis ranges   # Setup a mean line ax0.axhline(y=over_50[\"chol\"].mean(),              color=\"r\",              linestyle=\"--\",              label=\"Average\"); ax0.legend(*scatter.legend_elements(), title=\"Target\")  # Axis 1, 1 (row 1, column 1) scatter = ax1.scatter(over_50[\"age\"],                        over_50[\"thalach\"],                        c=over_50[\"target\"],                       cmap='winter') ax1.set(title=\"Heart Disease and Max Heart Rate Levels\",         xlabel=\"Age\",         ylabel=\"Max Heart Rate\",         ylim=[60, 220]) # change the y-axis range  # Setup a mean line ax1.axhline(y=over_50[\"thalach\"].mean(),              color=\"r\",              linestyle=\"--\",              label=\"Average\"); ax1.legend(*scatter.legend_elements(),             title=\"Target\")  # Title the figure fig.suptitle(\"Heart Disease Analysis\",               fontsize=16,               fontweight=\"bold\"); <p>Now that's a nice looking plot!</p> <p>Let's figure out how we'd save it.</p> In\u00a0[76]: Copied! <pre># Recreate the plot from above with custom x and y axis ranges\nfig, (ax0, ax1) = plt.subplots(nrows=2, \n                               ncols=1, \n                               sharex=True, \n                               figsize=(10, 7))\nscatter = ax0.scatter(over_50[\"age\"], \n                      over_50[\"chol\"], \n                      c=over_50[\"target\"],\n                      cmap='winter')\nax0.set(title=\"Heart Disease and Cholesterol Levels\",\n        ylabel=\"Cholesterol\",\n        xlim=[50, 80]) # set the x-axis ranges \n\n# Setup a mean line\nax0.axhline(y=over_50[\"chol\"].mean(), \n            color=\"r\", \n            linestyle=\"--\", \n            label=\"Average\");\nax0.legend(*scatter.legend_elements(), title=\"Target\")\n\n# Axis 1, 1 (row 1, column 1)\nscatter = ax1.scatter(over_50[\"age\"], \n                      over_50[\"thalach\"], \n                      c=over_50[\"target\"],\n                      cmap='winter')\nax1.set(title=\"Heart Disease and Max Heart Rate Levels\",\n        xlabel=\"Age\",\n        ylabel=\"Max Heart Rate\",\n        ylim=[60, 220]) # change the y-axis range\n\n# Setup a mean line\nax1.axhline(y=over_50[\"thalach\"].mean(), \n            color=\"r\", \n            linestyle=\"--\", \n            label=\"Average\");\nax1.legend(*scatter.legend_elements(), \n           title=\"Target\")\n\n# Title the figure\nfig.suptitle(\"Heart Disease Analysis\", \n             fontsize=16, \n             fontweight=\"bold\");\n</pre> # Recreate the plot from above with custom x and y axis ranges fig, (ax0, ax1) = plt.subplots(nrows=2,                                 ncols=1,                                 sharex=True,                                 figsize=(10, 7)) scatter = ax0.scatter(over_50[\"age\"],                        over_50[\"chol\"],                        c=over_50[\"target\"],                       cmap='winter') ax0.set(title=\"Heart Disease and Cholesterol Levels\",         ylabel=\"Cholesterol\",         xlim=[50, 80]) # set the x-axis ranges   # Setup a mean line ax0.axhline(y=over_50[\"chol\"].mean(),              color=\"r\",              linestyle=\"--\",              label=\"Average\"); ax0.legend(*scatter.legend_elements(), title=\"Target\")  # Axis 1, 1 (row 1, column 1) scatter = ax1.scatter(over_50[\"age\"],                        over_50[\"thalach\"],                        c=over_50[\"target\"],                       cmap='winter') ax1.set(title=\"Heart Disease and Max Heart Rate Levels\",         xlabel=\"Age\",         ylabel=\"Max Heart Rate\",         ylim=[60, 220]) # change the y-axis range  # Setup a mean line ax1.axhline(y=over_50[\"thalach\"].mean(),              color=\"r\",              linestyle=\"--\",              label=\"Average\"); ax1.legend(*scatter.legend_elements(),             title=\"Target\")  # Title the figure fig.suptitle(\"Heart Disease Analysis\",               fontsize=16,               fontweight=\"bold\"); <p>Nice!</p> <p>We can save our plots to several different kinds of filetypes.</p> <p>And we can check these filetypes with <code>fig.canvas.get_supported_filetypes()</code>.</p> In\u00a0[77]: Copied! <pre># Check the supported filetypes\nfig.canvas.get_supported_filetypes()\n</pre> # Check the supported filetypes fig.canvas.get_supported_filetypes() Out[77]: <pre>{'eps': 'Encapsulated Postscript',\n 'jpg': 'Joint Photographic Experts Group',\n 'jpeg': 'Joint Photographic Experts Group',\n 'pdf': 'Portable Document Format',\n 'pgf': 'PGF code for LaTeX',\n 'png': 'Portable Network Graphics',\n 'ps': 'Postscript',\n 'raw': 'Raw RGBA bitmap',\n 'rgba': 'Raw RGBA bitmap',\n 'svg': 'Scalable Vector Graphics',\n 'svgz': 'Scalable Vector Graphics',\n 'tif': 'Tagged Image File Format',\n 'tiff': 'Tagged Image File Format',\n 'webp': 'WebP Image Format'}</pre> <p>Image filetypes such as <code>jpg</code> and <code>png</code> are excellent for blog posts and presentations.</p> <p>Where as the <code>pgf</code> or <code>pdf</code> filetypes may be better for reports and papers.</p> <p>One last look at our Figure, which is saved to the <code>fig</code> variable.</p> In\u00a0[78]: Copied! <pre>fig\n</pre> fig Out[78]: <p>Beautiful!</p> <p>Now let's save it to file.</p> In\u00a0[79]: Copied! <pre># Save the file\nfig.savefig(fname=\"../images/heart-disease-analysis.png\",\n            dpi=100)\n</pre> # Save the file fig.savefig(fname=\"../images/heart-disease-analysis.png\",             dpi=100) <p>File saved!</p> <p>Let's try and display it.</p> <p>We can do so with the HTML code:</p> <pre><code>&lt;img src=\"../images/heart-disease-analysis.png\" alt=\"a plot showing a heart disease analysis comparing the presense of heart disease, cholesterol levels and heart rate on patients over 50/&gt;\n</code></pre> <p>And changing the cell below to markdown.</p> <p>Note: Because the plot is highly visual, it's import to make sure there is an <code>alt=\"some_text_here\"</code> tag available when displaying the image, as this tag is used to make the plot more accessible to those with visual impairments. For more on displaying images with HTML, see the Mozzila documentation.</p> <p>Finally, if we wanted to start making more and different Figures, we can reset our <code>fig</code> variable by creating another plot.</p> In\u00a0[80]: Copied! <pre># Resets figure\nfig, ax = plt.subplots()\n</pre> # Resets figure fig, ax = plt.subplots() <p>If you're creating plots and saving them like this often, to save writing excess code, you might put it into a function.</p> <p>A function which follows the Matplotlib workflow.</p> In\u00a0[81]: Copied! <pre># Potential matplotlib workflow function\n\ndef plotting_workflow(data):\n    # 1. Manipulate data\n    \n    # 2. Create plot\n    \n    # 3. Plot data\n    \n    # 4. Customize plot\n    \n    # 5. Save plot\n    \n    # 6. Return plot\n    \n    return plot\n</pre> # Potential matplotlib workflow function  def plotting_workflow(data):     # 1. Manipulate data          # 2. Create plot          # 3. Plot data          # 4. Customize plot          # 5. Save plot          # 6. Return plot          return plot"},{"location":"introduction-to-matplotlib/#a-quick-introduction-to-matplotlib","title":"A Quick Introduction to Matplotlib\u00b6","text":""},{"location":"introduction-to-matplotlib/#what-is-matplotlib","title":"What is matplotlib?\u00b6","text":"<p>Matplotlib is a visualization library for Python.</p> <p>As in, if you want to display something in a chart or graph, matplotlib can help you do that programmatically.</p> <p>Many of the graphics you'll see in machine learning research papers or presentations are made with matplotlib.</p> <p></p>"},{"location":"introduction-to-matplotlib/#why-matplotlib","title":"Why matplotlib?\u00b6","text":"<p>Matplotlib is part of the standard Python data stack (pandas, NumPy, matplotlib, Jupyter).</p> <p>It has terrific integration with many other Python libraries.</p> <p>pandas uses matplotlib as a backend to help visualize data in DataFrames.</p>"},{"location":"introduction-to-matplotlib/#what-does-this-notebook-cover","title":"What does this notebook cover?\u00b6","text":"<p>A central idea in matplotlib is the concept of a \"plot\" (hence the name).</p> <p>So we're going to practice making a series of different plots, which is a way to visually represent data.</p> <p>Since there are basically limitless ways to create a plot, we're going to focus on a making and customizing (making them look pretty) a few common types of plots.</p>"},{"location":"introduction-to-matplotlib/#where-can-i-get-help","title":"Where can I get help?\u00b6","text":"<p>If you get stuck or think of something you'd like to do which this notebook doesn't cover, don't fear!</p> <p>The recommended steps you take are:</p> <ol> <li>Try it - Since matplotlib is very friendly, your first step should be to use what you know and try figure out the answer to your own question (getting it wrong is part of the process). If in doubt, run your code.</li> <li>Search for it - If trying it on your own doesn't work, since someone else has probably tried to do something similar, try searching for your problem in the following places (either via a search engine or direct):<ul> <li>matplotlib documentation - the best place for learning all of the vast functionality of matplotlib. Bonus: You can see a series of matplotlib cheatsheets on the matplotlib website.</li> <li>Stack Overflow - this is the developers Q&amp;A hub, it's full of questions and answers of different problems across a wide range of software development topics and chances are, there's one related to your problem.</li> <li>ChatGPT - ChatGPT is very good at explaining code, however, it can make mistakes. Best to verify the code it writes first before using it. Try asking \"Can you explain the following code for me? {your code here}\" and then continue with follow up questions from there.</li> </ul> </li> </ol> <p>An example of searching for a matplotlib feature might be:</p> <p>\"how to colour the bars of a matplotlib plot\"</p> <p>Searching this on Google leads to this documentation page on the matplotlib website: https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_colors.html</p> <p>The next steps here are to read through the post and see if it relates to your problem. If it does, great, take the code/information you need and rewrite it to suit your own problem.</p> <ol> <li>Ask for help - If you've been through the above 2 steps and you're still stuck, you might want to ask your question on Stack Overflow or in the ZTM Discord chat. Remember to be specific as possible and provide details on what you've tried.</li> </ol> <p>Remember, you don't have to learn all of these functions off by heart to begin with.</p> <p>What's most important is remembering to continually ask yourself, \"what am I trying to visualize?\"</p> <p>Start by answering that question and then practicing finding the code which does it.</p> <p>Let's get to visualizing some data!</p>"},{"location":"introduction-to-matplotlib/#0-importing-matplotlib","title":"0. Importing matplotlib\u00b6","text":"<p>We'll start by importing <code>matplotlib.pyplot</code>.</p> <p>Why <code>pyplot</code>?</p> <p>Because <code>pyplot</code> is a submodule for creating interactive plots programmatically.</p> <p><code>pyplot</code> is often imported as the alias <code>plt</code>.</p> <p>Note: In older notebooks and tutorials of matplotlib, you may see the magic command <code>%matplotlib inline</code>. This was required to view plots inside a notebook, however, as of 2020 it is mostly no longer required.</p>"},{"location":"introduction-to-matplotlib/#1-2-ways-of-creating-plots","title":"1. 2 ways of creating plots\u00b6","text":"<p>There are two main ways of creating plots in matplotlib.</p> <ol> <li><code>matplotlib.pyplot.plot()</code> - Recommended for simple plots (e.g. x and y).</li> <li><code>matplotlib.pyplot.XX</code> (where XX can be one of many methods, this is known as the object-oriented API) - Recommended for more complex plots (for example <code>plt.subplots()</code> to create multiple plots on the same Figure, we'll get to this later).</li> </ol> <p>Both of these methods are still often created by building off <code>import matplotlib.pyplot as plt</code> as a base.</p> <p>Let's start simple.</p>"},{"location":"introduction-to-matplotlib/#anatomy-of-a-matplotlib-figure","title":"Anatomy of a Matplotlib Figure\u00b6","text":"<p>Matplotlib offers almost unlimited options for creating plots.</p> <p>However, let's break down some of the main terms.</p> <ul> <li>Figure - The base canvas of all matplotlib plots. The overall thing you're plotting is a Figure, often shortened to <code>fig</code>.</li> <li>Axes - One Figure can have one or multiple Axes, for example, a Figure with multiple suplots could have 4 Axes (2 rows and 2 columns). Often shortened to <code>ax</code>.</li> <li>Axis - A particular dimension of an Axes, for example, the x-axis or y-axis.</li> </ul> <p></p>"},{"location":"introduction-to-matplotlib/#a-quick-matplotlib-workflow","title":"A quick Matplotlib Workflow\u00b6","text":"<p>The following workflow is a standard practice when creating a matplotlib plot:</p> <ol> <li>Import matplotlib - For example, <code>import matplotlib.pyplot as plt</code>).</li> <li>Prepare data - This may be from an existing dataset (data analysis) or from the outputs of a machine learning model (data science).</li> <li>Setup the plot - In other words, create the Figure and various Axes.</li> <li>Plot data to the Axes - Send the relevant data to the target Axes.</li> <li>Cutomize the plot - Add a title, decorate the colours, label each Axis.</li> <li>Save (optional) and show - See what your masterpiece looks like and save it to file if necessary.</li> </ol>"},{"location":"introduction-to-matplotlib/#2-making-the-most-common-type-of-plots-using-numpy-arrays","title":"2. Making the most common type of plots using NumPy arrays\u00b6","text":"<p>Most of figuring out what kind of plot to use is getting a feel for the data, then seeing what kind of plot suits it best.</p> <p>Matplotlib visualizations are built on NumPy arrays. So in this section we'll build some of the most common types of plots using NumPy arrays.</p> <ul> <li>Line plot - <code>ax.plot()</code> (this is the default plot in matplotlib)</li> <li>Scatter plot - <code>ax.scatter()</code></li> <li>Bar plot - <code>ax.bar()</code></li> <li>Histogram plot - <code>ax.hist()</code></li> </ul> <p>We'll see how all of these can be created as a method from <code>matplotlob.pyplot.subplots()</code>.</p> <p>Resource: Remember you can see many of the different kinds of matplotlib plot types in the documentation.</p> <p>To make sure we have access to NumPy, we'll import it as <code>np</code>.</p>"},{"location":"introduction-to-matplotlib/#creating-a-line-plot","title":"Creating a line plot\u00b6","text":"<p>Line is the default type of visualization in Matplotlib. Usually, unless specified otherwise, your plots will start out as lines.</p> <p>Line plots are great for seeing trends over time.</p>"},{"location":"introduction-to-matplotlib/#creating-a-scatter-plot","title":"Creating a scatter plot\u00b6","text":"<p>Scatter plots can be great for when you've got many different individual data points and you'd like to see how they interact with eachother without being connected.</p>"},{"location":"introduction-to-matplotlib/#creating-bar-plots","title":"Creating bar plots\u00b6","text":"<p>Bar plots are great to visualize different amounts of similar themed items.</p> <p>For example, the sales of items at a Nut Butter Store.</p> <p>You can create vertical bar plots with <code>ax.bar()</code> and horizontal bar plots with <code>ax.barh()</code>.</p>"},{"location":"introduction-to-matplotlib/#creating-a-histogram-plot","title":"Creating a histogram plot\u00b6","text":"<p>Histogram plots are excellent for showing the distribution of data.</p> <p>For example, you might want to show the distribution of ages of a population or wages of city.</p>"},{"location":"introduction-to-matplotlib/#creating-figures-with-multiple-axes-with-subplots","title":"Creating Figures with multiple Axes with Subplots\u00b6","text":"<p>Subplots allow you to create multiple Axes on the same Figure (multiple plots within the same plot).</p> <p>Subplots are helpful because you start with one plot per Figure but scale it up to more when necessary.</p> <p>For example, let's create a subplot that shows many of the above datasets on the same Figure.</p> <p>We can do so by creating multiple Axes with <code>plt.subplots()</code> and setting the <code>nrows</code> (number of rows) and <code>ncols</code> (number of columns) parameters to reflect how many Axes we'd like.</p> <p><code>nrows</code> and <code>ncols</code> parameters are multiplicative, meaning <code>plt.subplots(nrows=2, ncols=2)</code> will create <code>2*2=4</code> total Axes.</p> <p>Resource: You can see a sensational number of examples for creating Subplots in the matplotlib documentation.</p>"},{"location":"introduction-to-matplotlib/#3-plotting-data-directly-with-pandas","title":"3. Plotting data directly with pandas\u00b6","text":"<p>Matplotlib has a tight integration with pandas too.</p> <p>You can directly plot from a pandas DataFrame with <code>DataFrame.plot()</code>.</p> <p>Let's see the following plots directly from a pandas DataFrame:</p> <ul> <li>Line</li> <li>Scatter</li> <li>Bar</li> <li>Hist</li> </ul> <p>To plot data with pandas, we first have to import it as <code>pd</code>.</p>"},{"location":"introduction-to-matplotlib/#line-plot-from-a-pandas-dataframe","title":"Line plot from a pandas DataFrame\u00b6","text":"<p>To understand examples, I often find I have to repeat them (code them myself) rather than just read them.</p> <p>To begin understanding plotting with pandas, let's recreate the a section of the pandas Chart visualization documents.</p>"},{"location":"introduction-to-matplotlib/#working-with-actual-data","title":"Working with actual data\u00b6","text":"<p>Let's do a little data manipulation on our <code>car_sales</code> DataFrame.</p>"},{"location":"introduction-to-matplotlib/#scatter-plot-from-a-pandas-dataframe","title":"Scatter plot from a pandas DataFrame\u00b6","text":"<p>You can create scatter plots from a pandas DataFrame by using the <code>kind=\"scatter\"</code> parameter.</p> <p>However, you'll often find that certain plots require certain kinds of data (e.g. some plots require certain columns to be numeric).</p>"},{"location":"introduction-to-matplotlib/#bar-plot-from-a-pandas-dataframe","title":"Bar plot from a pandas DataFrame\u00b6","text":"<p>Let's see how we can plot a bar plot from a pandas DataFrame.</p> <p>First, we'll create some data.</p>"},{"location":"introduction-to-matplotlib/#histogram-plot-from-a-pandas-dataframe","title":"Histogram plot from a pandas DataFrame\u00b6","text":"<p>We can plot a histogram plot from our <code>car_sales</code> DataFrame using <code>DataFrame.plot.hist()</code> or <code>DataFrame.plot(kind=\"hist\")</code>.</p> <p>Histograms are great for seeing the distribution or the spread of data.</p>"},{"location":"introduction-to-matplotlib/#creating-a-plot-with-multiple-axes-from-a-pandas-dataframe","title":"Creating a plot with multiple Axes from a pandas DataFrame\u00b6","text":"<p>We can also create a series of plots (multiple Axes on one Figure) from a DataFrame using the <code>subplots=True</code> parameter.</p> <p>First, let's remind ourselves what the data looks like.</p>"},{"location":"introduction-to-matplotlib/#4-plotting-more-advanced-plots-from-a-pandas-dataframe","title":"4. Plotting more advanced plots from a pandas DataFrame\u00b6","text":"<p>It's possible to achieve far more complicated and detailed plots from a pandas DataFrame.</p> <p>Let's practice using the <code>heart_disease</code> DataFrame.</p> <p>And as an example, let's do some analysis on people over 50 years of age.</p> <p>To do so, let's start by creating a plot directly from pandas and then using the object-orientated API (<code>plt.subplots()</code>) to build upon it.</p>"},{"location":"introduction-to-matplotlib/#plotting-multiple-plots-on-the-same-figure-adding-another-plot-to-an-existing-one","title":"Plotting multiple plots on the same figure (adding another plot to an existing one)\u00b6","text":"<p>Sometimes you'll want to visualize multiple features of a dataset or results of a model in one Figure.</p> <p>You can achieve this by adding data to multiple Axes on the same Figure.</p> <p>The <code>plt.subplots()</code> method helps you create Figures with a desired number of Axes in a desired figuration.</p> <p>Using <code>nrows</code> (number of rows) and <code>ncols</code> (number of columns) parameters you can control the number of Axes on the Figure.</p> <p>For example:</p> <ul> <li><code>nrows=2</code>, <code>ncols=1</code> = 2x1 = a Figure with 2 Axes</li> <li><code>nrows=5</code>, <code>ncols=5</code> = 5x5 = a Figure with 25 Axes</li> </ul> <p>Let's create a plot with 2 Axes.</p> <p>One the first Axes (Axes 0), we'll plot heart disease against cholesterol levels (<code>chol</code>).</p> <p>On the second Axes (Axis 1), we'll plot heart disease against max heart rate levels (<code>thalach</code>).</p>"},{"location":"introduction-to-matplotlib/#5-customizing-your-plots-making-them-look-pretty","title":"5. Customizing your plots (making them look pretty)\u00b6","text":"<p>If you're not a fan of the default matplotlib styling, there are plenty of ways to make your plots look prettier.</p> <p>The more visually appealing your plot, the higher the chance people are going to want to look at them.</p> <p>However, be careful not to overdo the customizations, as they may hinder the information being conveyed.</p> <p>Some of the things you can customize include:</p> <ul> <li>Axis limits - The range in which your data is displayed.</li> <li>Colors - That colors appear on the plot to represent different data.</li> <li>Overall style - Matplotlib has several different styles built-in which offer different overall themes for your plots, you can see examples of these in the matplotlib style sheets reference documentation.</li> <li>Legend - One of the most informative pieces of information on a Figure can be the legend, you can modify the legend of an Axes with the <code>plt.legend()</code> method.</li> </ul> <p>Let's start by exploring different styles built into matplotlib.</p>"},{"location":"introduction-to-matplotlib/#customizing-the-style-of-plots","title":"Customizing the style of plots\u00b6","text":"<p>Matplotlib comes with several built-in styles that are all created with an overall theme.</p> <p>You can see what styles are available by using <code>plt.style.available</code>.</p> <p>Resources:</p> <ul> <li>To see what many of the available styles look like, you can refer to the matplotlib style sheets reference documentation.</li> <li>For a deeper guide on customizing, refer to the Customizing Matplotlib with style sheets and rcParams tutorial.</li> </ul>"},{"location":"introduction-to-matplotlib/#customizing-the-title-legend-and-axis-labels","title":"Customizing the title, legend and axis labels\u00b6","text":"<p>When you have a matplotlib Figure or Axes object, you can customize many of the attributes by using the <code>Axes.set()</code> method.</p> <p>For example, you can change the:</p> <ul> <li><code>xlabel</code> - Labels on the x-axis.</li> <li><code>ylim</code> - Limits of the y-axis.</li> <li><code>xticks</code> - Style of the x-ticks.</li> <li>much more in the documentation.</li> </ul> <p>Rather than talking about it, let's practice!</p> <p>First, we'll create some random data and then put it into a DataFrame.</p> <p>Then we'll make a plot from that DataFrame and see how to customize it.</p>"},{"location":"introduction-to-matplotlib/#customizing-the-colours-of-plots-with-colormaps-cmap","title":"Customizing the colours of plots with colormaps (cmap)\u00b6","text":"<p>Colour is one of the most important features of a plot.</p> <p>It can help to separate different kinds of information.</p> <p>And with the right colours, plots can be fun to look at and try to learn more.</p> <p>Matplotlib provides many different colour options through <code>matplotlib.colormaps</code>.</p> <p>Let's see how we can change the colours of a matplotlib plot via the <code>cmap</code> parameter (<code>cmap</code> is short for <code>colormaps</code>).</p> <p>We'll start by creating a scatter plot with the default <code>cmap</code> value (<code>cmap=\"viridis\"</code>).</p>"},{"location":"introduction-to-matplotlib/#customizing-the-xlim-ylim","title":"Customizing the xlim &amp; ylim\u00b6","text":"<p>Matplotlib is pretty good at setting the ranges of values on the x-axis and the y-axis.</p> <p>But as you might've guessed, you can customize these to suit your needs.</p> <p>You can change the ranges of different axis values using the <code>xlim</code> and <code>ylim</code> parameters inside of the <code>set()</code> method.</p> <p>To practice, let's recreate our double Axes plot from before with the default x-axis and y-axis values.</p> <p>We'll add in the colour updates from the previous section too.</p>"},{"location":"introduction-to-matplotlib/#6-saving-plots","title":"6. Saving plots\u00b6","text":"<p>Once you've got a nice looking plot that you're happy with, the next thing is going to be sharing it with someone else.</p> <p>In a report, blog post, presentation or something similar.</p> <p>You can save matplotlib Figures with <code>plt.savefig(fname=\"your_plot_file_name\")</code> where <code>fname</code> is the target filename you'd like to save the plot to.</p> <p>Before we save our plot, let's recreate it.</p>"},{"location":"introduction-to-matplotlib/#extra-resources","title":"Extra resources\u00b6","text":"<p>We've covered a fair bit here.</p> <p>But really we've only scratched the surface of what's possible with matplotlib.</p> <p>So for more, I'd recommend going through the following:</p> <ul> <li>Matplotlib quick start guide - Try rewriting all the code in this guide to get familiar with it.</li> <li>Matplotlib plot types guide - Inside you'll get an idea of just how many kinds of plots are possible with matplotlib.</li> <li>Matplotlib lifecycle of a plot guide - A sensational ground-up walkthrough of the many different things you can do with a plot.</li> </ul>"},{"location":"introduction-to-numpy/","title":"Introduction to NumPy","text":"In\u00a0[1]: Copied! <pre>import datetime\nprint(f\"Last updated: {datetime.datetime.now()}\")\n</pre> import datetime print(f\"Last updated: {datetime.datetime.now()}\") <pre>Last updated: 2023-10-12 10:55:52.338301\n</pre> In\u00a0[2]: Copied! <pre>import numpy as np\n\n# Check the version\nprint(np.__version__)\n</pre> import numpy as np  # Check the version print(np.__version__) <pre>1.25.2\n</pre> In\u00a0[3]: Copied! <pre># 1-dimensonal array, also referred to as a vector\na1 = np.array([1, 2, 3])\n\n# 2-dimensional array, also referred to as matrix\na2 = np.array([[1, 2.0, 3.3],\n               [4, 5, 6.5]])\n\n# 3-dimensional array, also referred to as a matrix\na3 = np.array([[[1, 2, 3],\n                [4, 5, 6],\n                [7, 8, 9]],\n                [[10, 11, 12],\n                 [13, 14, 15],\n                 [16, 17, 18]]])\n</pre> # 1-dimensonal array, also referred to as a vector a1 = np.array([1, 2, 3])  # 2-dimensional array, also referred to as matrix a2 = np.array([[1, 2.0, 3.3],                [4, 5, 6.5]])  # 3-dimensional array, also referred to as a matrix a3 = np.array([[[1, 2, 3],                 [4, 5, 6],                 [7, 8, 9]],                 [[10, 11, 12],                  [13, 14, 15],                  [16, 17, 18]]]) In\u00a0[4]: Copied! <pre>a1.shape, a1.ndim, a1.dtype, a1.size, type(a1)\n</pre> a1.shape, a1.ndim, a1.dtype, a1.size, type(a1) Out[4]: <pre>((3,), 1, dtype('int64'), 3, numpy.ndarray)</pre> In\u00a0[5]: Copied! <pre>a2.shape, a2.ndim, a2.dtype, a2.size, type(a2)\n</pre> a2.shape, a2.ndim, a2.dtype, a2.size, type(a2) Out[5]: <pre>((2, 3), 2, dtype('float64'), 6, numpy.ndarray)</pre> In\u00a0[6]: Copied! <pre>a3.shape, a3.ndim, a3.dtype, a3.size, type(a3)\n</pre> a3.shape, a3.ndim, a3.dtype, a3.size, type(a3) Out[6]: <pre>((2, 3, 3), 3, dtype('int64'), 18, numpy.ndarray)</pre> In\u00a0[7]: Copied! <pre>a1\n</pre> a1 Out[7]: <pre>array([1, 2, 3])</pre> In\u00a0[8]: Copied! <pre>a2\n</pre> a2 Out[8]: <pre>array([[1. , 2. , 3.3],\n       [4. , 5. , 6.5]])</pre> In\u00a0[9]: Copied! <pre>a3\n</pre> a3 Out[9]: <pre>array([[[ 1,  2,  3],\n        [ 4,  5,  6],\n        [ 7,  8,  9]],\n\n       [[10, 11, 12],\n        [13, 14, 15],\n        [16, 17, 18]]])</pre> In\u00a0[10]: Copied! <pre>import pandas as pd\ndf = pd.DataFrame(np.random.randint(10, size=(5, 3)), \n                                    columns=['a', 'b', 'c'])\ndf\n</pre> import pandas as pd df = pd.DataFrame(np.random.randint(10, size=(5, 3)),                                      columns=['a', 'b', 'c']) df Out[10]: a b c 0 2 3 6 1 1 5 6 2 7 0 2 3 2 1 3 4 8 0 7 In\u00a0[11]: Copied! <pre>a2\n</pre> a2 Out[11]: <pre>array([[1. , 2. , 3.3],\n       [4. , 5. , 6.5]])</pre> In\u00a0[12]: Copied! <pre>df2 = pd.DataFrame(a2)\ndf2\n</pre> df2 = pd.DataFrame(a2) df2 Out[12]: 0 1 2 0 1.0 2.0 3.3 1 4.0 5.0 6.5 In\u00a0[13]: Copied! <pre># Create a simple array\nsimple_array = np.array([1, 2, 3])\nsimple_array\n</pre> # Create a simple array simple_array = np.array([1, 2, 3]) simple_array Out[13]: <pre>array([1, 2, 3])</pre> In\u00a0[14]: Copied! <pre>simple_array = np.array((1, 2, 3))\nsimple_array, simple_array.dtype\n</pre> simple_array = np.array((1, 2, 3)) simple_array, simple_array.dtype Out[14]: <pre>(array([1, 2, 3]), dtype('int64'))</pre> In\u00a0[15]: Copied! <pre># Create an array of ones\nones = np.ones((10, 2))\nones\n</pre> # Create an array of ones ones = np.ones((10, 2)) ones Out[15]: <pre>array([[1., 1.],\n       [1., 1.],\n       [1., 1.],\n       [1., 1.],\n       [1., 1.],\n       [1., 1.],\n       [1., 1.],\n       [1., 1.],\n       [1., 1.],\n       [1., 1.]])</pre> In\u00a0[16]: Copied! <pre># The default datatype is 'float64'\nones.dtype\n</pre> # The default datatype is 'float64' ones.dtype Out[16]: <pre>dtype('float64')</pre> In\u00a0[17]: Copied! <pre># You can change the datatype with .astype()\nones.astype(int)\n</pre> # You can change the datatype with .astype() ones.astype(int) Out[17]: <pre>array([[1, 1],\n       [1, 1],\n       [1, 1],\n       [1, 1],\n       [1, 1],\n       [1, 1],\n       [1, 1],\n       [1, 1],\n       [1, 1],\n       [1, 1]])</pre> In\u00a0[18]: Copied! <pre># Create an array of zeros\nzeros = np.zeros((5, 3, 3))\nzeros\n</pre> # Create an array of zeros zeros = np.zeros((5, 3, 3)) zeros Out[18]: <pre>array([[[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]],\n\n       [[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]],\n\n       [[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]],\n\n       [[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]],\n\n       [[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]]])</pre> In\u00a0[19]: Copied! <pre>zeros.dtype\n</pre> zeros.dtype Out[19]: <pre>dtype('float64')</pre> In\u00a0[20]: Copied! <pre># Create an array within a range of values\nrange_array = np.arange(0, 10, 2)\nrange_array\n</pre> # Create an array within a range of values range_array = np.arange(0, 10, 2) range_array Out[20]: <pre>array([0, 2, 4, 6, 8])</pre> In\u00a0[21]: Copied! <pre># Random array\nrandom_array = np.random.randint(10, size=(5, 3))\nrandom_array\n</pre> # Random array random_array = np.random.randint(10, size=(5, 3)) random_array Out[21]: <pre>array([[1, 7, 2],\n       [7, 0, 2],\n       [8, 8, 8],\n       [2, 5, 2],\n       [4, 8, 6]])</pre> In\u00a0[22]: Copied! <pre># Random array of floats (between 0 &amp; 1)\nnp.random.random((5, 3))\n</pre> # Random array of floats (between 0 &amp; 1) np.random.random((5, 3)) Out[22]: <pre>array([[0.09607892, 0.034903  , 0.47743753],\n       [0.51703027, 0.90409121, 0.54436342],\n       [0.8095754 , 0.60294712, 0.71141937],\n       [0.50802295, 0.57255717, 0.99090604],\n       [0.66225284, 0.87588103, 0.25643785]])</pre> In\u00a0[23]: Copied! <pre>np.random.random((5, 3))\n</pre> np.random.random((5, 3)) Out[23]: <pre>array([[0.42800066, 0.76816054, 0.14858447],\n       [0.48390262, 0.3708042 , 0.231316  ],\n       [0.29166801, 0.64327528, 0.18039386],\n       [0.89010443, 0.51218751, 0.31543512],\n       [0.38781697, 0.25729731, 0.66219967]])</pre> In\u00a0[24]: Copied! <pre># Random 5x3 array of floats (between 0 &amp; 1), similar to above\nnp.random.rand(5, 3)\n</pre> # Random 5x3 array of floats (between 0 &amp; 1), similar to above np.random.rand(5, 3) Out[24]: <pre>array([[0.28373526, 0.10074198, 0.24643463],\n       [0.8268303 , 0.48672847, 0.57633359],\n       [0.77867161, 0.38490598, 0.53343872],\n       [0.67396616, 0.15888354, 0.47710898],\n       [0.92319417, 0.19133444, 0.51837588]])</pre> In\u00a0[25]: Copied! <pre>np.random.rand(5, 3)\n</pre> np.random.rand(5, 3) Out[25]: <pre>array([[0.73585424, 0.83359732, 0.93900774],\n       [0.27563836, 0.55971665, 0.26819222],\n       [0.29253202, 0.64152402, 0.90479721],\n       [0.6585366 , 0.36165565, 0.37515932],\n       [0.82890572, 0.54502359, 0.48398256]])</pre> <p>NumPy uses pseudo-random numbers, which means, the numbers look random but aren't really, they're predetermined.</p> <p>For consistency, you might want to keep the random numbers you generate similar throughout experiments.</p> <p>To do this, you can use <code>np.random.seed()</code>.</p> <p>What this does is it tells NumPy, \"Hey, I want you to create random numbers but keep them aligned with the seed.\"</p> <p>Let's see it.</p> In\u00a0[26]: Copied! <pre># Set random seed to 0\nnp.random.seed(0)\n\n# Make 'random' numbers\nnp.random.randint(10, size=(5, 3))\n</pre> # Set random seed to 0 np.random.seed(0)  # Make 'random' numbers np.random.randint(10, size=(5, 3)) Out[26]: <pre>array([[5, 0, 3],\n       [3, 7, 9],\n       [3, 5, 2],\n       [4, 7, 6],\n       [8, 8, 1]])</pre> <p>With <code>np.random.seed()</code> set, every time you run the cell above, the same random numbers will be generated.</p> <p>What if <code>np.random.seed()</code> wasn't set?</p> <p>Every time you run the cell below, a new set of numbers will appear.</p> In\u00a0[27]: Copied! <pre># Make more random numbers\nnp.random.randint(10, size=(5, 3))\n</pre> # Make more random numbers np.random.randint(10, size=(5, 3)) Out[27]: <pre>array([[6, 7, 7],\n       [8, 1, 5],\n       [9, 8, 9],\n       [4, 3, 0],\n       [3, 5, 0]])</pre> <p>Let's see it in action again, we'll stay consistent and set the random seed to 0.</p> In\u00a0[28]: Copied! <pre># Set random seed to same number as above\nnp.random.seed(0)\n\n# The same random numbers come out\nnp.random.randint(10, size=(5, 3))\n</pre> # Set random seed to same number as above np.random.seed(0)  # The same random numbers come out np.random.randint(10, size=(5, 3)) Out[28]: <pre>array([[5, 0, 3],\n       [3, 7, 9],\n       [3, 5, 2],\n       [4, 7, 6],\n       [8, 8, 1]])</pre> <p>Because <code>np.random.seed()</code> is set to 0, the random numbers are the same as the cell with <code>np.random.seed()</code> set to 0 as well.</p> <p>Setting <code>np.random.seed()</code> is not 100% necessary but it's helpful to keep numbers the same throughout your experiments.</p> <p>For example, say you wanted to split your data randomly into training and test sets.</p> <p>Every time you randomly split, you might get different rows in each set.</p> <p>If you shared your work with someone else, they'd get different rows in each set too.</p> <p>Setting <code>np.random.seed()</code> ensures there's still randomness, it just makes the randomness repeatable. Hence the 'pseudo-random' numbers.</p> In\u00a0[29]: Copied! <pre>np.random.seed(0)\ndf = pd.DataFrame(np.random.randint(10, size=(5, 3)))\ndf\n</pre> np.random.seed(0) df = pd.DataFrame(np.random.randint(10, size=(5, 3))) df Out[29]: 0 1 2 0 5 0 3 1 3 7 9 2 3 5 2 3 4 7 6 4 8 8 1 In\u00a0[30]: Copied! <pre># Your code here\n</pre> # Your code here In\u00a0[31]: Copied! <pre>a1\n</pre> a1 Out[31]: <pre>array([1, 2, 3])</pre> In\u00a0[32]: Copied! <pre>a2\n</pre> a2 Out[32]: <pre>array([[1. , 2. , 3.3],\n       [4. , 5. , 6.5]])</pre> In\u00a0[33]: Copied! <pre>a3\n</pre> a3 Out[33]: <pre>array([[[ 1,  2,  3],\n        [ 4,  5,  6],\n        [ 7,  8,  9]],\n\n       [[10, 11, 12],\n        [13, 14, 15],\n        [16, 17, 18]]])</pre> <p>Array shapes are always listed in the format <code>(row, column, n, n, n...)</code> where <code>n</code> is optional extra dimensions.</p> In\u00a0[34]: Copied! <pre>a1[0]\n</pre> a1[0] Out[34]: <pre>1</pre> In\u00a0[35]: Copied! <pre>a2[0]\n</pre> a2[0] Out[35]: <pre>array([1. , 2. , 3.3])</pre> In\u00a0[36]: Copied! <pre>a3[0]\n</pre> a3[0] Out[36]: <pre>array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])</pre> In\u00a0[37]: Copied! <pre># Get 2nd row (index 1) of a2\na2[1]\n</pre> # Get 2nd row (index 1) of a2 a2[1] Out[37]: <pre>array([4. , 5. , 6.5])</pre> In\u00a0[38]: Copied! <pre># Get the first 2 values of the first 2 rows of both arrays\na3[:2, :2, :2]\n</pre> # Get the first 2 values of the first 2 rows of both arrays a3[:2, :2, :2] Out[38]: <pre>array([[[ 1,  2],\n        [ 4,  5]],\n\n       [[10, 11],\n        [13, 14]]])</pre> <p>This takes a bit of practice, especially when the dimensions get higher. Usually, it takes me a little trial and error of trying to get certain values, viewing the output in the notebook and trying again.</p> <p>NumPy arrays get printed from outside to inside. This means the number at the end of the shape comes first, and the number at the start of the shape comes last.</p> In\u00a0[39]: Copied! <pre>a4 = np.random.randint(10, size=(2, 3, 4, 5))\na4\n</pre> a4 = np.random.randint(10, size=(2, 3, 4, 5)) a4 Out[39]: <pre>array([[[[6, 7, 7, 8, 1],\n         [5, 9, 8, 9, 4],\n         [3, 0, 3, 5, 0],\n         [2, 3, 8, 1, 3]],\n\n        [[3, 3, 7, 0, 1],\n         [9, 9, 0, 4, 7],\n         [3, 2, 7, 2, 0],\n         [0, 4, 5, 5, 6]],\n\n        [[8, 4, 1, 4, 9],\n         [8, 1, 1, 7, 9],\n         [9, 3, 6, 7, 2],\n         [0, 3, 5, 9, 4]]],\n\n\n       [[[4, 6, 4, 4, 3],\n         [4, 4, 8, 4, 3],\n         [7, 5, 5, 0, 1],\n         [5, 9, 3, 0, 5]],\n\n        [[0, 1, 2, 4, 2],\n         [0, 3, 2, 0, 7],\n         [5, 9, 0, 2, 7],\n         [2, 9, 2, 3, 3]],\n\n        [[2, 3, 4, 1, 2],\n         [9, 1, 4, 6, 8],\n         [2, 3, 0, 0, 6],\n         [0, 6, 3, 3, 8]]]])</pre> In\u00a0[40]: Copied! <pre>a4.shape\n</pre> a4.shape Out[40]: <pre>(2, 3, 4, 5)</pre> In\u00a0[41]: Copied! <pre># Get only the first 4 numbers of each single vector\na4[:, :, :, :4]\n</pre> # Get only the first 4 numbers of each single vector a4[:, :, :, :4] Out[41]: <pre>array([[[[6, 7, 7, 8],\n         [5, 9, 8, 9],\n         [3, 0, 3, 5],\n         [2, 3, 8, 1]],\n\n        [[3, 3, 7, 0],\n         [9, 9, 0, 4],\n         [3, 2, 7, 2],\n         [0, 4, 5, 5]],\n\n        [[8, 4, 1, 4],\n         [8, 1, 1, 7],\n         [9, 3, 6, 7],\n         [0, 3, 5, 9]]],\n\n\n       [[[4, 6, 4, 4],\n         [4, 4, 8, 4],\n         [7, 5, 5, 0],\n         [5, 9, 3, 0]],\n\n        [[0, 1, 2, 4],\n         [0, 3, 2, 0],\n         [5, 9, 0, 2],\n         [2, 9, 2, 3]],\n\n        [[2, 3, 4, 1],\n         [9, 1, 4, 6],\n         [2, 3, 0, 0],\n         [0, 6, 3, 3]]]])</pre> <p><code>a4</code>'s shape is (2, 3, 4, 5), this means it gets displayed like so:</p> <ul> <li>Inner most array = size 5</li> <li>Next array = size 4</li> <li>Next array = size 3</li> <li>Outer most array = size 2</li> </ul> In\u00a0[42]: Copied! <pre>a1\n</pre> a1 Out[42]: <pre>array([1, 2, 3])</pre> In\u00a0[43]: Copied! <pre>ones = np.ones(3)\nones\n</pre> ones = np.ones(3) ones Out[43]: <pre>array([1., 1., 1.])</pre> In\u00a0[44]: Copied! <pre># Add two arrays\na1 + ones\n</pre> # Add two arrays a1 + ones Out[44]: <pre>array([2., 3., 4.])</pre> In\u00a0[45]: Copied! <pre># Subtract two arrays\na1 - ones\n</pre> # Subtract two arrays a1 - ones Out[45]: <pre>array([0., 1., 2.])</pre> In\u00a0[46]: Copied! <pre># Multiply two arrays\na1 * ones\n</pre> # Multiply two arrays a1 * ones Out[46]: <pre>array([1., 2., 3.])</pre> In\u00a0[47]: Copied! <pre># Multiply two arrays\na1 * a2\n</pre> # Multiply two arrays a1 * a2 Out[47]: <pre>array([[ 1. ,  4. ,  9.9],\n       [ 4. , 10. , 19.5]])</pre> In\u00a0[48]: Copied! <pre>a1.shape, a2.shape\n</pre> a1.shape, a2.shape Out[48]: <pre>((3,), (2, 3))</pre> In\u00a0[49]: Copied! <pre># This will error as the arrays have a different number of dimensions (2, 3) vs. (2, 3, 3) \na2 * a3\n</pre> # This will error as the arrays have a different number of dimensions (2, 3) vs. (2, 3, 3)  a2 * a3 <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[49], line 2\n      1 # This will error as the arrays have a different number of dimensions (2, 3) vs. (2, 3, 3) \n----&gt; 2 a2 * a3\n\nValueError: operands could not be broadcast together with shapes (2,3) (2,3,3) </pre> In\u00a0[50]: Copied! <pre>a3\n</pre> a3 Out[50]: <pre>array([[[ 1,  2,  3],\n        [ 4,  5,  6],\n        [ 7,  8,  9]],\n\n       [[10, 11, 12],\n        [13, 14, 15],\n        [16, 17, 18]]])</pre> In\u00a0[51]: Copied! <pre>a1\n</pre> a1 Out[51]: <pre>array([1, 2, 3])</pre> In\u00a0[52]: Copied! <pre>a1.shape\n</pre> a1.shape Out[52]: <pre>(3,)</pre> In\u00a0[53]: Copied! <pre>a2.shape\n</pre> a2.shape Out[53]: <pre>(2, 3)</pre> In\u00a0[54]: Copied! <pre>a2\n</pre> a2 Out[54]: <pre>array([[1. , 2. , 3.3],\n       [4. , 5. , 6.5]])</pre> In\u00a0[55]: Copied! <pre>a1 + a2\n</pre> a1 + a2 Out[55]: <pre>array([[2. , 4. , 6.3],\n       [5. , 7. , 9.5]])</pre> In\u00a0[56]: Copied! <pre>a2 + 2\n</pre> a2 + 2 Out[56]: <pre>array([[3. , 4. , 5.3],\n       [6. , 7. , 8.5]])</pre> In\u00a0[57]: Copied! <pre># Raises an error because there's a shape mismatch (2, 3) vs. (2, 3, 3)\na2 + a3\n</pre> # Raises an error because there's a shape mismatch (2, 3) vs. (2, 3, 3) a2 + a3 <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[57], line 2\n      1 # Raises an error because there's a shape mismatch (2, 3) vs. (2, 3, 3)\n----&gt; 2 a2 + a3\n\nValueError: operands could not be broadcast together with shapes (2,3) (2,3,3) </pre> In\u00a0[58]: Copied! <pre># Divide two arrays\na1 / ones\n</pre> # Divide two arrays a1 / ones Out[58]: <pre>array([1., 2., 3.])</pre> In\u00a0[59]: Copied! <pre># Divide using floor division\na2 // a1\n</pre> # Divide using floor division a2 // a1 Out[59]: <pre>array([[1., 1., 1.],\n       [4., 2., 2.]])</pre> In\u00a0[60]: Copied! <pre># Take an array to a power\na1 ** 2\n</pre> # Take an array to a power a1 ** 2 Out[60]: <pre>array([1, 4, 9])</pre> In\u00a0[61]: Copied! <pre># You can also use np.square()\nnp.square(a1)\n</pre> # You can also use np.square() np.square(a1) Out[61]: <pre>array([1, 4, 9])</pre> In\u00a0[62]: Copied! <pre># Modulus divide (what's the remainder)\na1 % 2\n</pre> # Modulus divide (what's the remainder) a1 % 2 Out[62]: <pre>array([1, 0, 1])</pre> <p>You can also find the log or exponential of an array using <code>np.log()</code> and <code>np.exp()</code>.</p> In\u00a0[63]: Copied! <pre># Find the log of an array\nnp.log(a1)\n</pre> # Find the log of an array np.log(a1) Out[63]: <pre>array([0.        , 0.69314718, 1.09861229])</pre> In\u00a0[64]: Copied! <pre># Find the exponential of an array\nnp.exp(a1)\n</pre> # Find the exponential of an array np.exp(a1) Out[64]: <pre>array([ 2.71828183,  7.3890561 , 20.08553692])</pre> In\u00a0[65]: Copied! <pre>sum(a1)\n</pre> sum(a1) Out[65]: <pre>6</pre> In\u00a0[66]: Copied! <pre>np.sum(a1)\n</pre> np.sum(a1) Out[66]: <pre>6</pre> <p>Tip: Use NumPy's <code>np.sum()</code> on NumPy arrays and Python's <code>sum()</code> on Python <code>list</code>s.</p> In\u00a0[67]: Copied! <pre>massive_array = np.random.random(100000)\nmassive_array.size, type(massive_array)\n</pre> massive_array = np.random.random(100000) massive_array.size, type(massive_array) Out[67]: <pre>(100000, numpy.ndarray)</pre> In\u00a0[68]: Copied! <pre>%timeit sum(massive_array) # Python sum()\n%timeit np.sum(massive_array) # NumPy np.sum()\n</pre> %timeit sum(massive_array) # Python sum() %timeit np.sum(massive_array) # NumPy np.sum() <pre>4.38 ms \u00b1 119 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n20.3 \u00b5s \u00b1 110 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\n</pre> <p>Notice <code>np.sum()</code> is faster on the Numpy array (<code>numpy.ndarray</code>) than Python's <code>sum()</code>.</p> <p>Now let's try it out on a Python list.</p> In\u00a0[69]: Copied! <pre>import random \nmassive_list = [random.randint(0, 10) for i in range(100000)]\nlen(massive_list), type(massive_list)\n</pre> import random  massive_list = [random.randint(0, 10) for i in range(100000)] len(massive_list), type(massive_list) Out[69]: <pre>(100000, list)</pre> In\u00a0[70]: Copied! <pre>massive_list[:10]\n</pre> massive_list[:10] Out[70]: <pre>[0, 4, 5, 9, 7, 0, 1, 7, 8, 1]</pre> In\u00a0[71]: Copied! <pre>%timeit sum(massive_list)\n%timeit np.sum(massive_list)\n</pre> %timeit sum(massive_list) %timeit np.sum(massive_list) <pre>598 \u00b5s \u00b1 959 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\n2.72 ms \u00b1 10.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> <p>NumPy's <code>np.sum()</code> is still fast but Python's <code>sum()</code> is faster on Python <code>list</code>s.</p> In\u00a0[72]: Copied! <pre>a2\n</pre> a2 Out[72]: <pre>array([[1. , 2. , 3.3],\n       [4. , 5. , 6.5]])</pre> In\u00a0[73]: Copied! <pre># Find the mean\nnp.mean(a2)\n</pre> # Find the mean np.mean(a2) Out[73]: <pre>3.6333333333333333</pre> In\u00a0[74]: Copied! <pre># Find the max\nnp.max(a2)\n</pre> # Find the max np.max(a2) Out[74]: <pre>6.5</pre> In\u00a0[75]: Copied! <pre># Find the min\nnp.min(a2)\n</pre> # Find the min np.min(a2) Out[75]: <pre>1.0</pre> In\u00a0[76]: Copied! <pre># Find the standard deviation\nnp.std(a2)\n</pre> # Find the standard deviation np.std(a2) Out[76]: <pre>1.8226964152656422</pre> In\u00a0[77]: Copied! <pre># Find the variance\nnp.var(a2)\n</pre> # Find the variance np.var(a2) Out[77]: <pre>3.3222222222222224</pre> In\u00a0[78]: Copied! <pre># The standard deviation is the square root of the variance\nnp.sqrt(np.var(a2))\n</pre> # The standard deviation is the square root of the variance np.sqrt(np.var(a2)) Out[78]: <pre>1.8226964152656422</pre> <p>What's mean?</p> <p>Mean is the same as average. You can find the average of a set of numbers by adding them up and dividing them by how many there are.</p> <p>What's standard deviation?</p> <p>Standard deviation is a measure of how spread out numbers are.</p> <p>What's variance?</p> <p>The variance is the averaged squared differences of the mean.</p> <p>To work it out, you:</p> <ol> <li>Work out the mean</li> <li>For each number, subtract the mean and square the result</li> <li>Find the average of the squared differences</li> </ol> In\u00a0[79]: Copied! <pre># Demo of variance\nhigh_var_array = np.array([1, 100, 200, 300, 4000, 5000])\nlow_var_array = np.array([2, 4, 6, 8, 10])\n\nnp.var(high_var_array), np.var(low_var_array)\n</pre> # Demo of variance high_var_array = np.array([1, 100, 200, 300, 4000, 5000]) low_var_array = np.array([2, 4, 6, 8, 10])  np.var(high_var_array), np.var(low_var_array) Out[79]: <pre>(4296133.472222221, 8.0)</pre> In\u00a0[80]: Copied! <pre>np.std(high_var_array), np.std(low_var_array)\n</pre> np.std(high_var_array), np.std(low_var_array) Out[80]: <pre>(2072.711623024829, 2.8284271247461903)</pre> In\u00a0[81]: Copied! <pre># The standard deviation is the square root of the variance\nnp.sqrt(np.var(high_var_array))\n</pre> # The standard deviation is the square root of the variance np.sqrt(np.var(high_var_array)) Out[81]: <pre>2072.711623024829</pre> In\u00a0[82]: Copied! <pre>%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.hist(high_var_array)\nplt.show()\n</pre> %matplotlib inline import matplotlib.pyplot as plt plt.hist(high_var_array) plt.show() In\u00a0[83]: Copied! <pre>plt.hist(low_var_array)\nplt.show()\n</pre> plt.hist(low_var_array) plt.show() In\u00a0[84]: Copied! <pre>a2\n</pre> a2 Out[84]: <pre>array([[1. , 2. , 3.3],\n       [4. , 5. , 6.5]])</pre> In\u00a0[85]: Copied! <pre>a2.shape\n</pre> a2.shape Out[85]: <pre>(2, 3)</pre> In\u00a0[86]: Copied! <pre>a2 + a3\n</pre> a2 + a3 <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[86], line 1\n----&gt; 1 a2 + a3\n\nValueError: operands could not be broadcast together with shapes (2,3) (2,3,3) </pre> In\u00a0[\u00a0]: Copied! <pre>a2.reshape(2, 3, 1)\n</pre> a2.reshape(2, 3, 1) In\u00a0[\u00a0]: Copied! <pre>a2.reshape(2, 3, 1) + a3\n</pre> a2.reshape(2, 3, 1) + a3 In\u00a0[\u00a0]: Copied! <pre>a2.shape\n</pre> a2.shape In\u00a0[\u00a0]: Copied! <pre>a2.T\n</pre> a2.T In\u00a0[\u00a0]: Copied! <pre>a2.transpose()\n</pre> a2.transpose() In\u00a0[\u00a0]: Copied! <pre>a2.T.shape\n</pre> a2.T.shape <p>For larger arrays, the default value of a tranpose is to swap the first and last axes.</p> <p>For example, <code>(5, 3, 3)</code> -&gt; <code>(3, 3, 5)</code>.</p> In\u00a0[\u00a0]: Copied! <pre>matrix = np.random.random(size=(5, 3, 3))\nmatrix\n</pre> matrix = np.random.random(size=(5, 3, 3)) matrix In\u00a0[\u00a0]: Copied! <pre>matrix.shape\n</pre> matrix.shape In\u00a0[\u00a0]: Copied! <pre>matrix.T\n</pre> matrix.T In\u00a0[\u00a0]: Copied! <pre>matrix.T.shape\n</pre> matrix.T.shape In\u00a0[\u00a0]: Copied! <pre># Check to see if the reverse shape is same as tranpose shape\nmatrix.T.shape == matrix.shape[::-1]\n</pre> # Check to see if the reverse shape is same as tranpose shape matrix.T.shape == matrix.shape[::-1] In\u00a0[\u00a0]: Copied! <pre># Check to see if the first and last axes are swapped\nmatrix.T == matrix.swapaxes(0, -1) # swap first (0) and last (-1) axes\n</pre> # Check to see if the first and last axes are swapped matrix.T == matrix.swapaxes(0, -1) # swap first (0) and last (-1) axes <p>You can see more advanced forms of tranposing in the NumPy documentation under <code>numpy.transpose</code>.</p> In\u00a0[\u00a0]: Copied! <pre>np.random.seed(0)\nmat1 = np.random.randint(10, size=(3, 3))\nmat2 = np.random.randint(10, size=(3, 2))\n\nmat1.shape, mat2.shape\n</pre> np.random.seed(0) mat1 = np.random.randint(10, size=(3, 3)) mat2 = np.random.randint(10, size=(3, 2))  mat1.shape, mat2.shape In\u00a0[\u00a0]: Copied! <pre>mat1\n</pre> mat1 In\u00a0[\u00a0]: Copied! <pre>mat2\n</pre> mat2 In\u00a0[\u00a0]: Copied! <pre>np.dot(mat1, mat2)\n</pre> np.dot(mat1, mat2) In\u00a0[\u00a0]: Copied! <pre># Can also achieve np.dot() with \"@\" \n# (however, they may behave differently at 3D+ arrays)\nmat1 @ mat2\n</pre> # Can also achieve np.dot() with \"@\"  # (however, they may behave differently at 3D+ arrays) mat1 @ mat2 In\u00a0[\u00a0]: Copied! <pre>np.random.seed(0)\nmat3 = np.random.randint(10, size=(4,3))\nmat4 = np.random.randint(10, size=(4,3))\nmat3\n</pre> np.random.seed(0) mat3 = np.random.randint(10, size=(4,3)) mat4 = np.random.randint(10, size=(4,3)) mat3 In\u00a0[\u00a0]: Copied! <pre>mat4\n</pre> mat4 In\u00a0[\u00a0]: Copied! <pre># This will fail as the inner dimensions of the matrices do not match\nnp.dot(mat3, mat4)\n</pre> # This will fail as the inner dimensions of the matrices do not match np.dot(mat3, mat4) In\u00a0[\u00a0]: Copied! <pre>mat3.T.shape\n</pre> mat3.T.shape In\u00a0[\u00a0]: Copied! <pre># Dot product\nnp.dot(mat3.T, mat4)\n</pre> # Dot product np.dot(mat3.T, mat4) In\u00a0[\u00a0]: Copied! <pre># Element-wise multiplication, also known as Hadamard product\nmat3 * mat4\n</pre> # Element-wise multiplication, also known as Hadamard product mat3 * mat4 In\u00a0[\u00a0]: Copied! <pre>np.random.seed(0)\nsales_amounts = np.random.randint(20, size=(5, 3))\nsales_amounts\n</pre> np.random.seed(0) sales_amounts = np.random.randint(20, size=(5, 3)) sales_amounts In\u00a0[\u00a0]: Copied! <pre>weekly_sales = pd.DataFrame(sales_amounts,\n                            index=[\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\"],\n                            columns=[\"Almond butter\", \"Peanut butter\", \"Cashew butter\"])\nweekly_sales\n</pre> weekly_sales = pd.DataFrame(sales_amounts,                             index=[\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\"],                             columns=[\"Almond butter\", \"Peanut butter\", \"Cashew butter\"]) weekly_sales In\u00a0[87]: Copied! <pre>prices = np.array([10, 8, 12])\nprices\n</pre> prices = np.array([10, 8, 12]) prices Out[87]: <pre>array([10,  8, 12])</pre> In\u00a0[88]: Copied! <pre>butter_prices = pd.DataFrame(prices.reshape(1, 3),\n                             index=[\"Price\"],\n                             columns=[\"Almond butter\", \"Peanut butter\", \"Cashew butter\"])\nbutter_prices.shape\n</pre> butter_prices = pd.DataFrame(prices.reshape(1, 3),                              index=[\"Price\"],                              columns=[\"Almond butter\", \"Peanut butter\", \"Cashew butter\"]) butter_prices.shape Out[88]: <pre>(1, 3)</pre> In\u00a0[89]: Copied! <pre>weekly_sales.shape\n</pre> weekly_sales.shape <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[89], line 1\n----&gt; 1 weekly_sales.shape\n\nNameError: name 'weekly_sales' is not defined</pre> In\u00a0[90]: Copied! <pre># Find the total amount of sales for a whole day\ntotal_sales = prices.dot(sales_amounts)\ntotal_sales\n</pre> # Find the total amount of sales for a whole day total_sales = prices.dot(sales_amounts) total_sales <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[90], line 2\n      1 # Find the total amount of sales for a whole day\n----&gt; 2 total_sales = prices.dot(sales_amounts)\n      3 total_sales\n\nNameError: name 'sales_amounts' is not defined</pre> <p>The shapes aren't aligned, we need the middle two numbers to be the same.</p> In\u00a0[91]: Copied! <pre>prices\n</pre> prices Out[91]: <pre>array([10,  8, 12])</pre> In\u00a0[92]: Copied! <pre>sales_amounts.T.shape\n</pre> sales_amounts.T.shape <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[92], line 1\n----&gt; 1 sales_amounts.T.shape\n\nNameError: name 'sales_amounts' is not defined</pre> In\u00a0[93]: Copied! <pre># To make the middle numbers the same, we can transpose\ntotal_sales = prices.dot(sales_amounts.T)\ntotal_sales\n</pre> # To make the middle numbers the same, we can transpose total_sales = prices.dot(sales_amounts.T) total_sales <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[93], line 2\n      1 # To make the middle numbers the same, we can transpose\n----&gt; 2 total_sales = prices.dot(sales_amounts.T)\n      3 total_sales\n\nNameError: name 'sales_amounts' is not defined</pre> In\u00a0[94]: Copied! <pre>butter_prices.shape, weekly_sales.shape\n</pre> butter_prices.shape, weekly_sales.shape <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[94], line 1\n----&gt; 1 butter_prices.shape, weekly_sales.shape\n\nNameError: name 'weekly_sales' is not defined</pre> In\u00a0[95]: Copied! <pre>daily_sales = butter_prices.dot(weekly_sales.T)\ndaily_sales\n</pre> daily_sales = butter_prices.dot(weekly_sales.T) daily_sales <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[95], line 1\n----&gt; 1 daily_sales = butter_prices.dot(weekly_sales.T)\n      2 daily_sales\n\nNameError: name 'weekly_sales' is not defined</pre> In\u00a0[96]: Copied! <pre># Need to transpose again\nweekly_sales[\"Total\"] = daily_sales.T\nweekly_sales\n</pre> # Need to transpose again weekly_sales[\"Total\"] = daily_sales.T weekly_sales <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[96], line 2\n      1 # Need to transpose again\n----&gt; 2 weekly_sales[\"Total\"] = daily_sales.T\n      3 weekly_sales\n\nNameError: name 'daily_sales' is not defined</pre> In\u00a0[97]: Copied! <pre>a1\n</pre> a1 Out[97]: <pre>array([1, 2, 3])</pre> In\u00a0[98]: Copied! <pre>a2\n</pre> a2 Out[98]: <pre>array([[1. , 2. , 3.3],\n       [4. , 5. , 6.5]])</pre> In\u00a0[99]: Copied! <pre>a1 &gt; a2\n</pre> a1 &gt; a2 Out[99]: <pre>array([[False, False, False],\n       [False, False, False]])</pre> In\u00a0[100]: Copied! <pre>a1 &gt;= a2\n</pre> a1 &gt;= a2 Out[100]: <pre>array([[ True,  True, False],\n       [False, False, False]])</pre> In\u00a0[101]: Copied! <pre>a1 &gt; 5\n</pre> a1 &gt; 5 Out[101]: <pre>array([False, False, False])</pre> In\u00a0[102]: Copied! <pre>a1 == a1\n</pre> a1 == a1 Out[102]: <pre>array([ True,  True,  True])</pre> In\u00a0[103]: Copied! <pre>a1 == a2\n</pre> a1 == a2 Out[103]: <pre>array([[ True,  True, False],\n       [False, False, False]])</pre> In\u00a0[104]: Copied! <pre>random_array\n</pre> random_array Out[104]: <pre>array([[1, 7, 2],\n       [7, 0, 2],\n       [8, 8, 8],\n       [2, 5, 2],\n       [4, 8, 6]])</pre> In\u00a0[105]: Copied! <pre>np.sort(random_array)\n</pre> np.sort(random_array) Out[105]: <pre>array([[1, 2, 7],\n       [0, 2, 7],\n       [8, 8, 8],\n       [2, 2, 5],\n       [4, 6, 8]])</pre> In\u00a0[106]: Copied! <pre>np.argsort(random_array)\n</pre> np.argsort(random_array) Out[106]: <pre>array([[0, 2, 1],\n       [1, 2, 0],\n       [0, 1, 2],\n       [0, 2, 1],\n       [0, 2, 1]])</pre> In\u00a0[107]: Copied! <pre>a1\n</pre> a1 Out[107]: <pre>array([1, 2, 3])</pre> In\u00a0[108]: Copied! <pre># Return the indices that would sort an array\nnp.argsort(a1)\n</pre> # Return the indices that would sort an array np.argsort(a1) Out[108]: <pre>array([0, 1, 2])</pre> In\u00a0[109]: Copied! <pre># No axis\nnp.argmin(a1)\n</pre> # No axis np.argmin(a1) Out[109]: <pre>0</pre> In\u00a0[110]: Copied! <pre>random_array\n</pre> random_array Out[110]: <pre>array([[1, 7, 2],\n       [7, 0, 2],\n       [8, 8, 8],\n       [2, 5, 2],\n       [4, 8, 6]])</pre> In\u00a0[111]: Copied! <pre># Down the vertical\nnp.argmax(random_array, axis=1)\n</pre> # Down the vertical np.argmax(random_array, axis=1) Out[111]: <pre>array([1, 0, 0, 1, 1])</pre> In\u00a0[112]: Copied! <pre># Across the horizontal\nnp.argmin(random_array, axis=0)\n</pre> # Across the horizontal np.argmin(random_array, axis=0) Out[112]: <pre>array([0, 1, 0])</pre> In\u00a0[\u00a0]: Copied! <pre>from matplotlib.image import imread\n\npanda = imread('../images/numpy-panda.jpeg')\nprint(type(panda))\n</pre> from matplotlib.image import imread  panda = imread('../images/numpy-panda.jpeg') print(type(panda)) <pre>&lt;class 'numpy.ndarray'&gt;\n</pre> In\u00a0[114]: Copied! <pre>panda.shape\n</pre> panda.shape Out[114]: <pre>(2330, 3500, 3)</pre> In\u00a0[115]: Copied! <pre>panda\n</pre> panda Out[115]: <pre>array([[[0.05490196, 0.10588235, 0.06666667],\n        [0.05490196, 0.10588235, 0.06666667],\n        [0.05490196, 0.10588235, 0.06666667],\n        ...,\n        [0.16470589, 0.12941177, 0.09411765],\n        [0.16470589, 0.12941177, 0.09411765],\n        [0.16470589, 0.12941177, 0.09411765]],\n\n       [[0.05490196, 0.10588235, 0.06666667],\n        [0.05490196, 0.10588235, 0.06666667],\n        [0.05490196, 0.10588235, 0.06666667],\n        ...,\n        [0.16470589, 0.12941177, 0.09411765],\n        [0.16470589, 0.12941177, 0.09411765],\n        [0.16470589, 0.12941177, 0.09411765]],\n\n       [[0.05490196, 0.10588235, 0.06666667],\n        [0.05490196, 0.10588235, 0.06666667],\n        [0.05490196, 0.10588235, 0.06666667],\n        ...,\n        [0.16470589, 0.12941177, 0.09411765],\n        [0.16470589, 0.12941177, 0.09411765],\n        [0.16470589, 0.12941177, 0.09411765]],\n\n       ...,\n\n       [[0.13333334, 0.07450981, 0.05490196],\n        [0.12156863, 0.0627451 , 0.04313726],\n        [0.10980392, 0.05098039, 0.03137255],\n        ...,\n        [0.02745098, 0.02745098, 0.03529412],\n        [0.02745098, 0.02745098, 0.03529412],\n        [0.02745098, 0.02745098, 0.03529412]],\n\n       [[0.13333334, 0.07450981, 0.05490196],\n        [0.12156863, 0.0627451 , 0.04313726],\n        [0.12156863, 0.0627451 , 0.04313726],\n        ...,\n        [0.02352941, 0.02352941, 0.03137255],\n        [0.02352941, 0.02352941, 0.03137255],\n        [0.02352941, 0.02352941, 0.03137255]],\n\n       [[0.13333334, 0.07450981, 0.05490196],\n        [0.12156863, 0.0627451 , 0.04313726],\n        [0.12156863, 0.0627451 , 0.04313726],\n        ...,\n        [0.02352941, 0.02352941, 0.03137255],\n        [0.02352941, 0.02352941, 0.03137255],\n        [0.02352941, 0.02352941, 0.03137255]]], dtype=float32)</pre> <p></p> In\u00a0[\u00a0]: Copied! <pre>car = imread(\"../images/numpy-car-photo.png\")\ncar.shape\n</pre> car = imread(\"../images/numpy-car-photo.png\") car.shape Out[\u00a0]: <pre>(431, 575, 4)</pre> In\u00a0[117]: Copied! <pre>car[:,:,:3].shape\n</pre> car[:,:,:3].shape Out[117]: <pre>(431, 575, 3)</pre> <p></p> In\u00a0[118]: Copied! <pre>dog = imread(\"../images/numpy-dog-photo.png\")\ndog.shape\n</pre> dog = imread(\"../images/numpy-dog-photo.png\") dog.shape Out[118]: <pre>(432, 575, 4)</pre> In\u00a0[119]: Copied! <pre>dog\n</pre> dog Out[119]: <pre>array([[[0.70980394, 0.80784315, 0.88235295, 1.        ],\n        [0.72156864, 0.8117647 , 0.8862745 , 1.        ],\n        [0.7411765 , 0.8156863 , 0.8862745 , 1.        ],\n        ...,\n        [0.49803922, 0.6862745 , 0.8392157 , 1.        ],\n        [0.49411765, 0.68235296, 0.8392157 , 1.        ],\n        [0.49411765, 0.68235296, 0.8352941 , 1.        ]],\n\n       [[0.69411767, 0.8039216 , 0.8862745 , 1.        ],\n        [0.7019608 , 0.8039216 , 0.88235295, 1.        ],\n        [0.7058824 , 0.80784315, 0.88235295, 1.        ],\n        ...,\n        [0.5019608 , 0.6862745 , 0.84705883, 1.        ],\n        [0.49411765, 0.68235296, 0.84313726, 1.        ],\n        [0.49411765, 0.68235296, 0.8392157 , 1.        ]],\n\n       [[0.6901961 , 0.8       , 0.88235295, 1.        ],\n        [0.69803923, 0.8039216 , 0.88235295, 1.        ],\n        [0.7058824 , 0.80784315, 0.88235295, 1.        ],\n        ...,\n        [0.5019608 , 0.6862745 , 0.84705883, 1.        ],\n        [0.49803922, 0.6862745 , 0.84313726, 1.        ],\n        [0.49803922, 0.6862745 , 0.84313726, 1.        ]],\n\n       ...,\n\n       [[0.9098039 , 0.81960785, 0.654902  , 1.        ],\n        [0.8352941 , 0.7490196 , 0.6509804 , 1.        ],\n        [0.72156864, 0.6313726 , 0.5372549 , 1.        ],\n        ...,\n        [0.01568628, 0.07058824, 0.02352941, 1.        ],\n        [0.03921569, 0.09411765, 0.03529412, 1.        ],\n        [0.03921569, 0.09019608, 0.05490196, 1.        ]],\n\n       [[0.9137255 , 0.83137256, 0.6784314 , 1.        ],\n        [0.8117647 , 0.7294118 , 0.627451  , 1.        ],\n        [0.65882355, 0.5686275 , 0.47843137, 1.        ],\n        ...,\n        [0.00392157, 0.05490196, 0.03529412, 1.        ],\n        [0.03137255, 0.09019608, 0.05490196, 1.        ],\n        [0.04705882, 0.10588235, 0.06666667, 1.        ]],\n\n       [[0.9137255 , 0.83137256, 0.68235296, 1.        ],\n        [0.76862746, 0.68235296, 0.5882353 , 1.        ],\n        [0.59607846, 0.5058824 , 0.44313726, 1.        ],\n        ...,\n        [0.03921569, 0.10196079, 0.07058824, 1.        ],\n        [0.02745098, 0.08235294, 0.05882353, 1.        ],\n        [0.05098039, 0.11372549, 0.07058824, 1.        ]]], dtype=float32)</pre>"},{"location":"introduction-to-numpy/#a-quick-introduction-to-numerical-data-manipulation-with-python-and-numpy","title":"A Quick Introduction to Numerical Data Manipulation with Python and NumPy\u00b6","text":""},{"location":"introduction-to-numpy/#what-is-numpy","title":"What is NumPy?\u00b6","text":"<p>NumPy stands for numerical Python. It's the backbone of all kinds of scientific and numerical computing in Python.</p> <p>And since machine learning is all about turning data into numbers and then figuring out the patterns, NumPy often comes into play.</p> <p></p>"},{"location":"introduction-to-numpy/#why-numpy","title":"Why NumPy?\u00b6","text":"<p>You can do numerical calculations using pure Python. In the beginning, you might think Python is fast but once your data gets large, you'll start to notice slow downs.</p> <p>One of the main reasons you use NumPy is because it's fast. Behind the scenes, the code has been optimized to run using C. Which is another programming language, which can do things much faster than Python.</p> <p>The benefit of this being behind the scenes is you don't need to know any C to take advantage of it. You can write your numerical computations in Python using NumPy and get the added speed benefits.</p> <p>If your curious as to what causes this speed benefit, it's a process called vectorization. Vectorization aims to do calculations by avoiding loops as loops can create potential bottlenecks.</p> <p>NumPy achieves vectorization through a process called broadcasting.</p>"},{"location":"introduction-to-numpy/#what-does-this-notebook-cover","title":"What does this notebook cover?\u00b6","text":"<p>The NumPy library is very capable. However, learning everything off by heart isn't necessary. Instead, this notebook focuses on the main concepts of NumPy and the <code>ndarray</code> datatype.</p> <p>You can think of the <code>ndarray</code> datatype as a very flexible array of numbers.</p> <p>More specifically, we'll look at:</p> <ul> <li>NumPy datatypes &amp; attributes</li> <li>Creating arrays</li> <li>Viewing arrays &amp; matrices (indexing)</li> <li>Manipulating &amp; comparing arrays</li> <li>Sorting arrays</li> <li>Use cases (examples of turning things into numbers)</li> </ul> <p>After going through it, you'll have the base knolwedge of NumPy you need to keep moving forward.</p>"},{"location":"introduction-to-numpy/#where-can-i-get-help","title":"Where can I get help?\u00b6","text":"<p>If you get stuck or think of something you'd like to do which this notebook doesn't cover, don't fear!</p> <p>The recommended steps you take are:</p> <ol> <li>Try it - Since NumPy is very friendly, your first step should be to use what you know and try figure out the answer to your own question (getting it wrong is part of the process). If in doubt, run your code.</li> <li>Search for it - If trying it on your own doesn't work, since someone else has probably tried to do something similar, try searching for your problem in the following places (either via a search engine or direct):<ul> <li>NumPy documentation - The ground truth for everything NumPy, this resource covers all of the NumPy functionality.</li> <li>Stack Overflow - This is the developers Q&amp;A hub, it's full of questions and answers of different problems across a wide range of software development topics and chances are, there's one related to your problem.</li> <li>ChatGPT - ChatGPT is very good at explaining code, however, it can make mistakes. Best to verify the code it writes first before using it. Try asking \"Can you explain the following code for me? {your code here}\" and then continue with follow up questions from there.</li> </ul> </li> </ol> <p>An example of searching for a NumPy function might be:</p> <p>\"how to find unique elements in a numpy array\"</p> <p>Searching this on Google leads to the NumPy documentation for the <code>np.unique()</code> function: https://numpy.org/doc/stable/reference/generated/numpy.unique.html</p> <p>The next steps here are to read through the documentation, check the examples and see if they line up to the problem you're trying to solve.</p> <p>If they do, rewrite the code to suit your needs, run it, and see what the outcomes are.</p> <ol> <li>Ask for help - If you've been through the above 2 steps and you're still stuck, you might want to ask your question on Stack Overflow. Be as specific as possible and provide details on what you've tried.</li> </ol> <p>Remember, you don't have to learn all of the functions off by heart to begin with.</p> <p>What's most important is continually asking yourself, \"what am I trying to do with the data?\".</p> <p>Start by answering that question and then practicing finding the code which does it.</p> <p>Let's get started.</p>"},{"location":"introduction-to-numpy/#0-importing-numpy","title":"0. Importing NumPy\u00b6","text":"<p>To get started using NumPy, the first step is to import it.</p> <p>The most common way (and method you should use) is to import NumPy as the abbreviation <code>np</code>.</p> <p>If you see the letters <code>np</code> used anywhere in machine learning or data science, it's probably referring to the NumPy library.</p>"},{"location":"introduction-to-numpy/#1-datatypes-and-attributes","title":"1. DataTypes and attributes\u00b6","text":"<p>Note: Important to remember the main type in NumPy is <code>ndarray</code>, even seemingly different kinds of arrays are still <code>ndarray</code>'s. This means an operation you do on one array, will work on another.</p>"},{"location":"introduction-to-numpy/#anatomy-of-an-array","title":"Anatomy of an array\u00b6","text":"<p>Key terms:</p> <ul> <li>Array - A list of numbers, can be multi-dimensional.</li> <li>Scalar - A single number (e.g. <code>7</code>).</li> <li>Vector - A list of numbers with 1-dimension (e.g. <code>np.array([1, 2, 3])</code>).</li> <li>Matrix - A (usually) multi-dimensional list of numbers (e.g. <code>np.array([[1, 2, 3], [4, 5, 6]])</code>).</li> </ul>"},{"location":"introduction-to-numpy/#pandas-dataframe-out-of-numpy-arrays","title":"pandas DataFrame out of NumPy arrays\u00b6","text":"<p>This is to examplify how NumPy is the backbone of many other libraries.</p>"},{"location":"introduction-to-numpy/#2-creating-arrays","title":"2. Creating arrays\u00b6","text":"<ul> <li><code>np.array()</code></li> <li><code>np.ones()</code></li> <li><code>np.zeros()</code></li> <li><code>np.random.rand(5, 3)</code></li> <li><code>np.random.randint(10, size=5)</code></li> <li><code>np.random.seed()</code> - pseudo random numbers</li> <li>Searching the documentation example (finding <code>np.unique()</code> and using it)</li> </ul>"},{"location":"introduction-to-numpy/#what-unique-values-are-in-the-array-a3","title":"What unique values are in the array a3?\u00b6","text":"<p>Now you've seen a few different ways to create arrays, as an exercise, try find out what NumPy function you could use to find the unique values are within the <code>a3</code> array.</p> <p>You might want to search some like, \"how to find the unqiue values in a numpy array\".</p>"},{"location":"introduction-to-numpy/#3-viewing-arrays-and-matrices-indexing","title":"3. Viewing arrays and matrices (indexing)\u00b6","text":"<p>Remember, because arrays and matrices are both <code>ndarray</code>'s, they can be viewed in similar ways.</p> <p>Let's check out our 3 arrays again.</p>"},{"location":"introduction-to-numpy/#4-manipulating-and-comparing-arrays","title":"4. Manipulating and comparing arrays\u00b6","text":"<ul> <li>Arithmetic<ul> <li><code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>//</code>, <code>**</code>, <code>%</code></li> <li><code>np.exp()</code></li> <li><code>np.log()</code></li> <li>Dot product - <code>np.dot()</code></li> <li>Broadcasting</li> </ul> </li> <li>Aggregation<ul> <li><code>np.sum()</code> - faster than Python's <code>.sum()</code> for NumPy arrays</li> <li><code>np.mean()</code></li> <li><code>np.std()</code></li> <li><code>np.var()</code></li> <li><code>np.min()</code></li> <li><code>np.max()</code></li> <li><code>np.argmin()</code> - find index of minimum value</li> <li><code>np.argmax()</code> - find index of maximum value</li> <li>These work on all <code>ndarray</code>'s<ul> <li><code>a4.min(axis=0)</code> -- you can use axis as well</li> </ul> </li> </ul> </li> <li>Reshaping<ul> <li><code>np.reshape()</code></li> </ul> </li> <li>Transposing<ul> <li><code>a3.T</code></li> </ul> </li> <li>Comparison operators<ul> <li><code>&gt;</code></li> <li><code>&lt;</code></li> <li><code>&lt;=</code></li> <li><code>&gt;=</code></li> <li><code>x != 3</code></li> <li><code>x == 3</code></li> <li><code>np.sum(x &gt; 3)</code></li> </ul> </li> </ul>"},{"location":"introduction-to-numpy/#arithmetic","title":"Arithmetic\u00b6","text":""},{"location":"introduction-to-numpy/#broadcasting","title":"Broadcasting\u00b6","text":"<ul> <li><p>What is broadcasting?</p> <ul> <li>Broadcasting is a feature of NumPy which performs an operation across multiple dimensions of data without replicating the data. This saves time and space. For example, if you have a 3x3 array (A) and want to add a 1x3 array (B), NumPy will add the row of (B) to every row of (A).</li> </ul> </li> <li><p>Rules of Broadcasting</p> <ol> <li>If the two arrays differ in their number of dimensions, the shape of the one with fewer dimensions is padded with ones on its leading (left) side.</li> <li>If the shape of the two arrays does not match in any dimension, the array with shape equal to 1 in that dimension is stretched to match the other shape.</li> <li>If in any dimension the sizes disagree and neither is equal to 1, an error is raised.</li> </ol> </li> </ul> <p>The broadcasting rule: In order to broadcast, the size of the trailing axes for both arrays in an operation must be either the same size or one of them must be one.</p>"},{"location":"introduction-to-numpy/#aggregation","title":"Aggregation\u00b6","text":"<p>Aggregation - bringing things together, doing a similar thing on a number of things.</p>"},{"location":"introduction-to-numpy/#reshaping","title":"Reshaping\u00b6","text":""},{"location":"introduction-to-numpy/#transpose","title":"Transpose\u00b6","text":"<p>A tranpose reverses the order of the axes.</p> <p>For example, an array with shape <code>(2, 3)</code> becomes <code>(3, 2)</code>.</p>"},{"location":"introduction-to-numpy/#dot-product","title":"Dot product\u00b6","text":"<p>The main two rules for dot product to remember are:</p> <ol> <li>The inner dimensions must match:</li> </ol> <ul> <li><code>(3, 2) @ (3, 2)</code> won't work</li> <li><code>(2, 3) @ (3, 2)</code> will work</li> <li><code>(3, 2) @ (2, 3)</code> will work</li> </ul> <ol> <li>The resulting matrix has the shape of the outer dimensions:</li> </ol> <ul> <li><code>(2, 3) @ (3, 2)</code> -&gt; <code>(2, 2)</code></li> <li><code>(3, 2) @ (2, 3)</code> -&gt; <code>(3, 3)</code></li> </ul> <p>Note: In NumPy, <code>np.dot()</code> and <code>@</code> can be used to acheive the same result for 1-2 dimension arrays. However, their behaviour begins to differ at arrays with 3+ dimensions.</p>"},{"location":"introduction-to-numpy/#dot-product-practical-example-nut-butter-sales","title":"Dot product practical example, nut butter sales\u00b6","text":""},{"location":"introduction-to-numpy/#comparison-operators","title":"Comparison operators\u00b6","text":"<p>Finding out if one array is larger, smaller or equal to another.</p>"},{"location":"introduction-to-numpy/#5-sorting-arrays","title":"5. Sorting arrays\u00b6","text":"<ul> <li><code>np.sort()</code> - sort values in a specified dimension of an array.</li> <li><code>np.argsort()</code> - return the indices to sort the array on a given axis.</li> <li><code>np.argmax()</code> - return the index/indicies which gives the highest value(s) along an axis.</li> <li><code>np.argmin()</code> - return the index/indices which gives the lowest value(s) along an axis.</li> </ul>"},{"location":"introduction-to-numpy/#6-use-case","title":"6. Use case\u00b6","text":"<p>Turning an image into a NumPy array.</p> <p>Why?</p> <p>Because computers can use the numbers in the NumPy array to find patterns in the image and in turn use those patterns to figure out what's in the image.</p> <p>This is what happens in modern computer vision algorithms.</p> <p>Let's start with this beautiful image of a panda:</p>"},{"location":"introduction-to-pandas/","title":"Introduction to pandas","text":"In\u00a0[1]: Copied! <pre>import datetime\nprint(f\"Last updated: {datetime.datetime.now()}\")\n</pre> import datetime print(f\"Last updated: {datetime.datetime.now()}\") <pre>Last updated: 2023-09-08 11:44:43.964728\n</pre> In\u00a0[2]: Copied! <pre>import pandas as pd\n\n# Print the version\nprint(f\"pandas version: {pd.__version__}\")\n</pre> import pandas as pd  # Print the version print(f\"pandas version: {pd.__version__}\") <pre>pandas version: 1.5.2\n</pre> In\u00a0[3]: Copied! <pre># Creating a series of car types\ncars = pd.Series([\"BMW\", \"Toyota\", \"Honda\"])\ncars\n</pre> # Creating a series of car types cars = pd.Series([\"BMW\", \"Toyota\", \"Honda\"]) cars Out[3]: <pre>0       BMW\n1    Toyota\n2     Honda\ndtype: object</pre> In\u00a0[4]: Copied! <pre># Creating a series of colours\ncolours = pd.Series([\"Blue\", \"Red\", \"White\"])\ncolours\n</pre> # Creating a series of colours colours = pd.Series([\"Blue\", \"Red\", \"White\"]) colours Out[4]: <pre>0     Blue\n1      Red\n2    White\ndtype: object</pre> <p>You can create a <code>DataFrame</code> by using <code>pd.DataFrame()</code> and passing it a Python dictionary.</p> <p>Let's use our two <code>Series</code> as the values.</p> In\u00a0[5]: Copied! <pre># Creating a DataFrame of cars and colours\ncar_data = pd.DataFrame({\"Car type\": cars, \n                         \"Colour\": colours})\ncar_data\n</pre> # Creating a DataFrame of cars and colours car_data = pd.DataFrame({\"Car type\": cars,                           \"Colour\": colours}) car_data Out[5]: Car type Colour 0 BMW Blue 1 Toyota Red 2 Honda White <p>You can see the keys of the dictionary became the column headings (text in bold) and the values of the two <code>Series</code>'s became the values in the DataFrame.</p> <p>It's important to note, many different types of data could go into the DataFrame.</p> <p>Here we've used only text but you could use floats, integers, dates and more.</p> In\u00a0[6]: Copied! <pre># Your code here\n</pre> # Your code here In\u00a0[7]: Copied! <pre># Example solution\n\n# Make a Series of different foods\nfoods = pd.Series([\"Almond butter\", \"Eggs\", \"Avocado\"])\n\n# Make a Series of different dollar values \nprices = pd.Series([9, 6, 2])\n\n# Combine your Series of foods and dollar values into a DataFrame\nfood_data = pd.DataFrame({\"Foods\": foods,\n                          \"Price\": prices})\n\nfood_data\n</pre> # Example solution  # Make a Series of different foods foods = pd.Series([\"Almond butter\", \"Eggs\", \"Avocado\"])  # Make a Series of different dollar values  prices = pd.Series([9, 6, 2])  # Combine your Series of foods and dollar values into a DataFrame food_data = pd.DataFrame({\"Foods\": foods,                           \"Price\": prices})  food_data Out[7]: Foods Price 0 Almond butter 9 1 Eggs 6 2 Avocado 2 In\u00a0[8]: Copied! <pre># Import car sales data\ncar_sales = pd.read_csv(\"../data/car-sales.csv\") # takes a filename as string as input\ncar_sales\n</pre> # Import car sales data car_sales = pd.read_csv(\"../data/car-sales.csv\") # takes a filename as string as input car_sales Out[8]: Make Colour Odometer (KM) Doors Price 0 Toyota White 150043 4 $4,000.00 1 Honda Red 87899 4 $5,000.00 2 Toyota Blue 32549 3 $7,000.00 3 BMW Black 11179 5 $22,000.00 4 Nissan White 213095 4 $3,500.00 5 Toyota Green 99213 4 $4,500.00 6 Honda Blue 45698 4 $7,500.00 7 Honda Blue 54738 4 $7,000.00 8 Toyota White 60000 4 $6,250.00 9 Nissan White 31600 4 $9,700.00 <p>Now we've got the same data from the spreadsheet available in a pandas <code>DataFrame</code> called <code>car_sales</code>.</p> <p>Having your data available in a <code>DataFrame</code> allows you to take advantage of all of pandas functionality on it.</p> <p>Another common practice you'll see is data being imported to <code>DataFrame</code> called <code>df</code> (short for <code>DataFrame</code>).</p> In\u00a0[9]: Copied! <pre># Import the car sales data and save it to df\ndf = pd.read_csv(\"../data/car-sales.csv\")\ndf\n</pre> # Import the car sales data and save it to df df = pd.read_csv(\"../data/car-sales.csv\") df Out[9]: Make Colour Odometer (KM) Doors Price 0 Toyota White 150043 4 $4,000.00 1 Honda Red 87899 4 $5,000.00 2 Toyota Blue 32549 3 $7,000.00 3 BMW Black 11179 5 $22,000.00 4 Nissan White 213095 4 $3,500.00 5 Toyota Green 99213 4 $4,500.00 6 Honda Blue 45698 4 $7,500.00 7 Honda Blue 54738 4 $7,000.00 8 Toyota White 60000 4 $6,250.00 9 Nissan White 31600 4 $9,700.00 <p>Now <code>car_sales</code> and <code>df</code> contain the exact same information, the only difference is the name. Like any other variable, you can name your <code>DataFrame</code>'s whatever you want. But best to choose something simple.</p> In\u00a0[10]: Copied! <pre># Export the car sales DataFrame to csv\ncar_sales.to_csv(\"../data/exported-car-sales.csv\")\n</pre> # Export the car sales DataFrame to csv car_sales.to_csv(\"../data/exported-car-sales.csv\") <p>Running this will save a file called <code>export-car-sales.csv</code> to the current folder.</p> <p></p> In\u00a0[11]: Copied! <pre># Your code here\n</pre> # Your code here  In\u00a0[12]: Copied! <pre># Importing heart-disease.csv\npatient_data = pd.read_csv(\"../data/heart-disease.csv\")\npatient_data\n</pre> # Importing heart-disease.csv patient_data = pd.read_csv(\"../data/heart-disease.csv\") patient_data Out[12]: age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target 0 63 1 3 145 233 1 0 150 0 2.3 0 0 1 1 1 37 1 2 130 250 0 1 187 0 3.5 0 0 2 1 2 41 0 1 130 204 0 0 172 0 1.4 2 0 2 1 3 56 1 1 120 236 0 1 178 0 0.8 2 0 2 1 4 57 0 0 120 354 0 1 163 1 0.6 2 0 2 1 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 298 57 0 0 140 241 0 1 123 1 0.2 1 0 3 0 299 45 1 3 110 264 0 1 132 0 1.2 1 0 3 0 300 68 1 0 144 193 1 1 141 0 3.4 1 2 3 0 301 57 1 0 130 131 0 1 115 1 1.2 1 1 3 0 302 57 0 1 130 236 0 0 174 0 0.0 1 1 2 0 <p>303 rows \u00d7 14 columns</p> In\u00a0[13]: Copied! <pre># Exporting the patient_data DataFrame to csv\npatient_data.to_csv(\"../data/exported-patient-data.csv\")\n</pre> # Exporting the patient_data DataFrame to csv patient_data.to_csv(\"../data/exported-patient-data.csv\") <p></p> In\u00a0[14]: Copied! <pre>car_sales\n</pre> car_sales Out[14]: Make Colour Odometer (KM) Doors Price 0 Toyota White 150043 4 $4,000.00 1 Honda Red 87899 4 $5,000.00 2 Toyota Blue 32549 3 $7,000.00 3 BMW Black 11179 5 $22,000.00 4 Nissan White 213095 4 $3,500.00 5 Toyota Green 99213 4 $4,500.00 6 Honda Blue 45698 4 $7,500.00 7 Honda Blue 54738 4 $7,000.00 8 Toyota White 60000 4 $6,250.00 9 Nissan White 31600 4 $9,700.00 <p><code>.dtypes</code> shows us what datatype each column contains.</p> In\u00a0[15]: Copied! <pre>car_sales.dtypes\n</pre> car_sales.dtypes Out[15]: <pre>Make             object\nColour           object\nOdometer (KM)     int64\nDoors             int64\nPrice            object\ndtype: object</pre> <p>Notice how the <code>Price</code> column isn't an integer like <code>Odometer</code> or <code>Doors</code>. Don't worry, pandas makes this easy to fix.</p> <p><code>.describe()</code> gives you a quick statistical overview of the numerical columns.</p> In\u00a0[16]: Copied! <pre>car_sales.describe()\n</pre> car_sales.describe() Out[16]: Odometer (KM) Doors count 10.000000 10.000000 mean 78601.400000 4.000000 std 61983.471735 0.471405 min 11179.000000 3.000000 25% 35836.250000 4.000000 50% 57369.000000 4.000000 75% 96384.500000 4.000000 max 213095.000000 5.000000 <p><code>.info()</code> shows a handful of useful information about a <code>DataFrame</code> such as:</p> <ul> <li>How many entries (rows) there are</li> <li>Whether there are missing values (if a columns non-null value is less than the number of entries, it has missing values)</li> <li>The datatypes of each column</li> </ul> In\u00a0[17]: Copied! <pre>car_sales.info()\n</pre> car_sales.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10 entries, 0 to 9\nData columns (total 5 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   Make           10 non-null     object\n 1   Colour         10 non-null     object\n 2   Odometer (KM)  10 non-null     int64 \n 3   Doors          10 non-null     int64 \n 4   Price          10 non-null     object\ndtypes: int64(2), object(3)\nmemory usage: 528.0+ bytes\n</pre> <p>You can also call various statistical and mathematical methods such as <code>.mean()</code> or <code>.sum()</code> directly on a <code>DataFrame</code> or <code>Series</code>.</p> In\u00a0[18]: Copied! <pre># Calling .mean() on a DataFrame\ncar_sales.mean(numeric_only=True) # numeric_only = get mean values of numeric columnns only\n</pre> # Calling .mean() on a DataFrame car_sales.mean(numeric_only=True) # numeric_only = get mean values of numeric columnns only Out[18]: <pre>Odometer (KM)    78601.4\nDoors                4.0\ndtype: float64</pre> In\u00a0[19]: Copied! <pre># Calling .mean() on a Series\ncar_prices = pd.Series([3000, 3500, 11250])\ncar_prices.mean()\n</pre> # Calling .mean() on a Series car_prices = pd.Series([3000, 3500, 11250]) car_prices.mean() Out[19]: <pre>5916.666666666667</pre> In\u00a0[20]: Copied! <pre># Calling .sum() on a DataFrame with numeric_only=False (default)\ncar_sales.sum(numeric_only=False)\n</pre> # Calling .sum() on a DataFrame with numeric_only=False (default) car_sales.sum(numeric_only=False) Out[20]: <pre>Make             ToyotaHondaToyotaBMWNissanToyotaHondaHondaToyo...\nColour               WhiteRedBlueBlackWhiteGreenBlueBlueWhiteWhite\nOdometer (KM)                                               786014\nDoors                                                           40\nPrice            $4,000.00$5,000.00$7,000.00$22,000.00$3,500.00...\ndtype: object</pre> In\u00a0[21]: Copied! <pre># Calling .sum() on a DataFrame with numeric_only=True\ncar_sales.sum(numeric_only=True)\n</pre> # Calling .sum() on a DataFrame with numeric_only=True car_sales.sum(numeric_only=True) Out[21]: <pre>Odometer (KM)    786014\nDoors                40\ndtype: int64</pre> In\u00a0[22]: Copied! <pre># Calling .sum() on a Series\ncar_prices.sum()\n</pre> # Calling .sum() on a Series car_prices.sum() Out[22]: <pre>17750</pre> <p>Calling these on a whole <code>DataFrame</code> may not be as helpful as targeting an individual column. But it's helpful to know they're there.</p> <p><code>.columns</code> will show you all the columns of a <code>DataFrame</code>.</p> In\u00a0[23]: Copied! <pre>car_sales.columns\n</pre> car_sales.columns Out[23]: <pre>Index(['Make', 'Colour', 'Odometer (KM)', 'Doors', 'Price'], dtype='object')</pre> <p>You can save them to a list which you could use later.</p> In\u00a0[24]: Copied! <pre># Save car_sales columns to a list \ncar_columns = car_sales.columns\ncar_columns[0]\n</pre> # Save car_sales columns to a list  car_columns = car_sales.columns car_columns[0] Out[24]: <pre>'Make'</pre> <p><code>.index</code> will show you the values in a <code>DataFrame</code>'s index (the column on the far left).</p> In\u00a0[25]: Copied! <pre>car_sales.index\n</pre> car_sales.index Out[25]: <pre>RangeIndex(start=0, stop=10, step=1)</pre> <p>pandas <code>DataFrame</code>'s, like Python lists, are 0-indexed (unless otherwise changed). This means they start at 0.</p> <p></p> In\u00a0[26]: Copied! <pre># Show the length of a DataFrame\nlen(car_sales)\n</pre> # Show the length of a DataFrame len(car_sales) Out[26]: <pre>10</pre> <p>So even though the length of our <code>car_sales</code> dataframe is 10, this means the indexes go from 0-9.</p> In\u00a0[27]: Copied! <pre># Show the first 5 rows of car_sales\ncar_sales.head()\n</pre> # Show the first 5 rows of car_sales car_sales.head() Out[27]: Make Colour Odometer (KM) Doors Price 0 Toyota White 150043 4 $4,000.00 1 Honda Red 87899 4 $5,000.00 2 Toyota Blue 32549 3 $7,000.00 3 BMW Black 11179 5 $22,000.00 4 Nissan White 213095 4 $3,500.00 <p>Why 5 rows? Good question. I don't know the answer. But 5 seems like a good amount.</p> <p>Want more than 5?</p> <p>No worries, you can pass <code>.head()</code> an integer to display more than or less than 5 rows.</p> In\u00a0[28]: Copied! <pre># Show the first 7 rows of car_sales\ncar_sales.head(7)\n</pre> # Show the first 7 rows of car_sales car_sales.head(7) Out[28]: Make Colour Odometer (KM) Doors Price 0 Toyota White 150043 4 $4,000.00 1 Honda Red 87899 4 $5,000.00 2 Toyota Blue 32549 3 $7,000.00 3 BMW Black 11179 5 $22,000.00 4 Nissan White 213095 4 $3,500.00 5 Toyota Green 99213 4 $4,500.00 6 Honda Blue 45698 4 $7,500.00 <p><code>.tail()</code> allows you to see the bottom 5 rows of your <code>DataFrame</code>. This is helpful if your changes are influencing the bottom rows of your data.</p> In\u00a0[29]: Copied! <pre># Show bottom 5 rows of car_sales\ncar_sales.tail()\n</pre> # Show bottom 5 rows of car_sales car_sales.tail() Out[29]: Make Colour Odometer (KM) Doors Price 5 Toyota Green 99213 4 $4,500.00 6 Honda Blue 45698 4 $7,500.00 7 Honda Blue 54738 4 $7,000.00 8 Toyota White 60000 4 $6,250.00 9 Nissan White 31600 4 $9,700.00 <p>You can use <code>.loc[]</code> and <code>.iloc[]</code> to select data from your <code>Series</code> and <code>DataFrame</code>'s.</p> <p>Let's see.</p> In\u00a0[30]: Copied! <pre># Create a sample series\nanimals = pd.Series([\"cat\", \"dog\", \"bird\", \"snake\", \"ox\", \"lion\"], \n                    index=[0, 3, 9, 8, 67, 3])\nanimals\n</pre> # Create a sample series animals = pd.Series([\"cat\", \"dog\", \"bird\", \"snake\", \"ox\", \"lion\"],                      index=[0, 3, 9, 8, 67, 3]) animals Out[30]: <pre>0       cat\n3       dog\n9      bird\n8     snake\n67       ox\n3      lion\ndtype: object</pre> <p><code>.loc[]</code> takes an integer or label as input. And it chooses from your <code>Series</code> or <code>DataFrame</code> whichever index matches the number.</p> In\u00a0[31]: Copied! <pre># Select all indexes with 3\nanimals.loc[3]\n</pre> # Select all indexes with 3 animals.loc[3] Out[31]: <pre>3     dog\n3    lion\ndtype: object</pre> In\u00a0[32]: Copied! <pre># Select index 9\nanimals.loc[9]\n</pre> # Select index 9 animals.loc[9] Out[32]: <pre>'bird'</pre> <p>Let's try with our <code>car_sales</code> DataFrame.</p> In\u00a0[33]: Copied! <pre>car_sales\n</pre> car_sales Out[33]: Make Colour Odometer (KM) Doors Price 0 Toyota White 150043 4 $4,000.00 1 Honda Red 87899 4 $5,000.00 2 Toyota Blue 32549 3 $7,000.00 3 BMW Black 11179 5 $22,000.00 4 Nissan White 213095 4 $3,500.00 5 Toyota Green 99213 4 $4,500.00 6 Honda Blue 45698 4 $7,500.00 7 Honda Blue 54738 4 $7,000.00 8 Toyota White 60000 4 $6,250.00 9 Nissan White 31600 4 $9,700.00 In\u00a0[34]: Copied! <pre># Select row at index 3\ncar_sales.loc[3]\n</pre> # Select row at index 3 car_sales.loc[3] Out[34]: <pre>Make                    BMW\nColour                Black\nOdometer (KM)         11179\nDoors                     5\nPrice            $22,000.00\nName: 3, dtype: object</pre> <p><code>iloc[]</code> does a similar thing but works with exact positions.</p> In\u00a0[35]: Copied! <pre>animals\n</pre> animals Out[35]: <pre>0       cat\n3       dog\n9      bird\n8     snake\n67       ox\n3      lion\ndtype: object</pre> In\u00a0[36]: Copied! <pre># Select row at position 3\nanimals.iloc[3]\n</pre> # Select row at position 3 animals.iloc[3] Out[36]: <pre>'snake'</pre> <p>Even though <code>'snake'</code> appears at index 8 in the series, it's shown using <code>.iloc[3]</code> because it's at the 3rd (starting from 0) position.</p> <p>Let's try with the <code>car_sales</code> <code>DataFrame</code>.</p> In\u00a0[37]: Copied! <pre># Select row at position 3\ncar_sales.iloc[3]\n</pre> # Select row at position 3 car_sales.iloc[3] Out[37]: <pre>Make                    BMW\nColour                Black\nOdometer (KM)         11179\nDoors                     5\nPrice            $22,000.00\nName: 3, dtype: object</pre> <p>You can see it's the same as <code>.loc[]</code> because the index is in order, position 3 is the same as index 3.</p> <p>You can also use slicing with <code>.loc[]</code> and <code>.iloc[]</code>.</p> In\u00a0[38]: Copied! <pre># Get all rows up to position 3\nanimals.iloc[:3]\n</pre> # Get all rows up to position 3 animals.iloc[:3] Out[38]: <pre>0     cat\n3     dog\n9    bird\ndtype: object</pre> In\u00a0[39]: Copied! <pre># Get all rows up to (and including) index 3\ncar_sales.loc[:3]\n</pre> # Get all rows up to (and including) index 3 car_sales.loc[:3] Out[39]: Make Colour Odometer (KM) Doors Price 0 Toyota White 150043 4 $4,000.00 1 Honda Red 87899 4 $5,000.00 2 Toyota Blue 32549 3 $7,000.00 3 BMW Black 11179 5 $22,000.00 In\u00a0[40]: Copied! <pre># Get all rows of the \"Colour\" column\ncar_sales.loc[:, \"Colour\"] # note: \":\" stands for \"all\", e.g. \"all indices in the first axis\"\n</pre> # Get all rows of the \"Colour\" column car_sales.loc[:, \"Colour\"] # note: \":\" stands for \"all\", e.g. \"all indices in the first axis\" Out[40]: <pre>0    White\n1      Red\n2     Blue\n3    Black\n4    White\n5    Green\n6     Blue\n7     Blue\n8    White\n9    White\nName: Colour, dtype: object</pre> <p>When should you use <code>.loc[]</code> or <code>.iloc[]</code>?</p> <ul> <li>Use <code>.loc[]</code> when you're selecting rows and columns based on their lables or a condition (e.g. retrieving data for specific columns).</li> <li>Use <code>.iloc[]</code> when you're selecting rows and columns based on their integer index positions (e.g. extracting the first ten rows regardless of the labels).</li> </ul> <p>However, in saying this, it will often take a bit of practice with each of the methods before you figure out which you'd like to use.</p> <p>If you want to select a particular column, you can use <code>DataFrame.['COLUMN_NAME']</code>.</p> In\u00a0[41]: Copied! <pre># Select Make column\ncar_sales['Make']\n</pre> # Select Make column car_sales['Make'] Out[41]: <pre>0    Toyota\n1     Honda\n2    Toyota\n3       BMW\n4    Nissan\n5    Toyota\n6     Honda\n7     Honda\n8    Toyota\n9    Nissan\nName: Make, dtype: object</pre> In\u00a0[42]: Copied! <pre># Select Colour column\ncar_sales['Colour']\n</pre> # Select Colour column car_sales['Colour'] Out[42]: <pre>0    White\n1      Red\n2     Blue\n3    Black\n4    White\n5    Green\n6     Blue\n7     Blue\n8    White\n9    White\nName: Colour, dtype: object</pre> <p>Boolean indexing works with column selection too. Using it will select the rows which fulfill the condition in the brackets.</p> In\u00a0[43]: Copied! <pre># Select cars with over 100,000 on the Odometer\ncar_sales[car_sales[\"Odometer (KM)\"] &gt; 100000]\n</pre> # Select cars with over 100,000 on the Odometer car_sales[car_sales[\"Odometer (KM)\"] &gt; 100000] Out[43]: Make Colour Odometer (KM) Doors Price 0 Toyota White 150043 4 $4,000.00 4 Nissan White 213095 4 $3,500.00 In\u00a0[44]: Copied! <pre># Select cars which are made by Toyota\ncar_sales[car_sales[\"Make\"] == \"Toyota\"]\n</pre> # Select cars which are made by Toyota car_sales[car_sales[\"Make\"] == \"Toyota\"] Out[44]: Make Colour Odometer (KM) Doors Price 0 Toyota White 150043 4 $4,000.00 2 Toyota Blue 32549 3 $7,000.00 5 Toyota Green 99213 4 $4,500.00 8 Toyota White 60000 4 $6,250.00 <p><code>pd.crosstab()</code> is a great way to view two different columns together and compare them.</p> In\u00a0[45]: Copied! <pre># Compare car Make with number of Doors\npd.crosstab(car_sales[\"Make\"], car_sales[\"Doors\"])\n</pre> # Compare car Make with number of Doors pd.crosstab(car_sales[\"Make\"], car_sales[\"Doors\"]) Out[45]: Doors 3 4 5 Make BMW 0 0 1 Honda 0 3 0 Nissan 0 2 0 Toyota 1 3 0 <p>If you want to compare more columns in the context of another column, you can use <code>.groupby()</code>.</p> In\u00a0[46]: Copied! <pre>car_sales\n</pre> car_sales Out[46]: Make Colour Odometer (KM) Doors Price 0 Toyota White 150043 4 $4,000.00 1 Honda Red 87899 4 $5,000.00 2 Toyota Blue 32549 3 $7,000.00 3 BMW Black 11179 5 $22,000.00 4 Nissan White 213095 4 $3,500.00 5 Toyota Green 99213 4 $4,500.00 6 Honda Blue 45698 4 $7,500.00 7 Honda Blue 54738 4 $7,000.00 8 Toyota White 60000 4 $6,250.00 9 Nissan White 31600 4 $9,700.00 In\u00a0[47]: Copied! <pre># Group by the Make column and find the mean of the other columns \ncar_sales.groupby([\"Make\"]).mean(numeric_only=True)\n</pre> # Group by the Make column and find the mean of the other columns  car_sales.groupby([\"Make\"]).mean(numeric_only=True) Out[47]: Odometer (KM) Doors Make BMW 11179.000000 5.00 Honda 62778.333333 4.00 Nissan 122347.500000 4.00 Toyota 85451.250000 3.75 <p>pandas even allows for quick plotting of columns so you can see your data visualling. To plot, you'll have to import <code>matplotlib</code>. If your plots aren't showing, try running the two lines of code below.</p> <p><code>%matplotlib inline</code> is a special command which tells Jupyter to show your plots. Commands with <code>%</code> at the front are called magic commands.</p> In\u00a0[48]: Copied! <pre># Import matplotlib and tell Jupyter to show plots\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> # Import matplotlib and tell Jupyter to show plots import matplotlib.pyplot as plt %matplotlib inline <p>You can visualize a column by calling <code>.plot()</code> on it.</p> In\u00a0[49]: Copied! <pre>car_sales[\"Odometer (KM)\"].plot(); # tip: the \";\" on the end prevents matplotlib from outputing the plot class\n</pre> car_sales[\"Odometer (KM)\"].plot(); # tip: the \";\" on the end prevents matplotlib from outputing the plot class <p>Or compare two columns by passing them as <code>x</code> and <code>y</code> to <code>plot()</code>.</p> In\u00a0[50]: Copied! <pre>car_sales.plot(x=\"Make\", y=\"Odometer (KM)\");\n</pre> car_sales.plot(x=\"Make\", y=\"Odometer (KM)\");  <p>You can see the distribution of a column by calling <code>.hist()</code> on you.</p> <p>The distribution of something is a way of describing the spread of different values.</p> In\u00a0[51]: Copied! <pre>car_sales[\"Odometer (KM)\"].hist()\n</pre> car_sales[\"Odometer (KM)\"].hist() Out[51]: <pre>&lt;AxesSubplot: &gt;</pre> <p>In this case, the majority of the distribution (spread) of the <code>\"Odometer (KM)\"</code> column is more towards the left of the graph. And there are two values which are more to the right. These two values to the right could be considered outliers (not part of the majority).</p> <p>Now what if we wanted to plot our <code>\"Price\"</code> column?</p> <p>Let's try.</p> In\u00a0[52]: Copied! <pre>car_sales[\"Price\"].plot()\n</pre> car_sales[\"Price\"].plot() <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[52], line 1\n----&gt; 1 car_sales[\"Price\"].plot()\n\nFile ~/code/zero-to-mastery-ml/env/lib/python3.8/site-packages/pandas/plotting/_core.py:1000, in PlotAccessor.__call__(self, *args, **kwargs)\n    997             label_name = label_kw or data.columns\n    998             data.columns = label_name\n-&gt; 1000 return plot_backend.plot(data, kind=kind, **kwargs)\n\nFile ~/code/zero-to-mastery-ml/env/lib/python3.8/site-packages/pandas/plotting/_matplotlib/__init__.py:71, in plot(data, kind, **kwargs)\n     69         kwargs[\"ax\"] = getattr(ax, \"left_ax\", ax)\n     70 plot_obj = PLOT_CLASSES[kind](data, **kwargs)\n---&gt; 71 plot_obj.generate()\n     72 plot_obj.draw()\n     73 return plot_obj.result\n\nFile ~/code/zero-to-mastery-ml/env/lib/python3.8/site-packages/pandas/plotting/_matplotlib/core.py:450, in MPLPlot.generate(self)\n    448 def generate(self) -&gt; None:\n    449     self._args_adjust()\n--&gt; 450     self._compute_plot_data()\n    451     self._setup_subplots()\n    452     self._make_plot()\n\nFile ~/code/zero-to-mastery-ml/env/lib/python3.8/site-packages/pandas/plotting/_matplotlib/core.py:635, in MPLPlot._compute_plot_data(self)\n    633 # no non-numeric frames or series allowed\n    634 if is_empty:\n--&gt; 635     raise TypeError(\"no numeric data to plot\")\n    637 self.data = numeric_data.apply(self._convert_to_ndarray)\n\nTypeError: no numeric data to plot</pre> <p>Trying to run it leaves us with an error. This is because the <code>\"Price\"</code> column of <code>car_sales</code> isn't in numeric form. We can tell this because of the <code>TypeError: no numeric data to plot</code> at the bottom of the cell.</p> <p>We can check this with <code>.info()</code>.</p> In\u00a0[53]: Copied! <pre>car_sales.info()\n</pre> car_sales.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10 entries, 0 to 9\nData columns (total 5 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   Make           10 non-null     object\n 1   Colour         10 non-null     object\n 2   Odometer (KM)  10 non-null     int64 \n 3   Doors          10 non-null     int64 \n 4   Price          10 non-null     object\ndtypes: int64(2), object(3)\nmemory usage: 528.0+ bytes\n</pre> <p>So what can we do?</p> <p>We need to convert the <code>\"Price\"</code> column to a numeric type.</p> <p>How?</p> <p>We could try a few different things on our own. But let's practice researching.</p> <p>1. Open up a search engine and type in something like \"how to convert a pandas column price to integer\".</p> <p>In the first result, I found this Stack Overflow question and answer . Where someone has had the same problem as us and someone else has provided an answer.</p> <p>Note: Sometimes the answer you're looking for won't be in the first result, or the 2nd or the 3rd. You may have to combine a few different solutions. Or, if possible, you can try and ask ChatGPT to help you out.</p> <p>2. In practice, you'd read through this and see if it relates to your problem.</p> <p>3. If it does, you can adjust the code from what's given in the Stack Overflow answer(s) to your own problem.</p> <p>4. If you're still stuck, you can try and converse with ChatGPT to help you with your problem (as long as the data/problem you're working on is okay to share - never share private data with anyone on the internet, including AI chatbots).</p> <p></p> <p>What's important in the beginning is not to remember every single detail off by heart but to know where to look. Remember, if in doubt, write code, run it, see what happens.</p> <p>Let's copy the answer code here and see how it relates to our problem.</p> <p>Answer code: <code>dataframe['amount'] = dataframe['amount'].str.replace('[\\$\\,\\.]', '').astype(int)</code></p> <p>There's a lot going on here but what we can do is change the parts which aren't in our problem and keep the rest the same.</p> <p>Our <code>DataFrame</code> is called <code>car_sales</code> not <code>dataframe</code>.</p> <p><code>car_sales['amount'] = car_sales['amount'].str.replace('[\\$\\,\\.]', '').astype(int)</code></p> <p>And our <code>'amount'</code> column is called <code>\"Price\"</code>.</p> <p><code>car_sales[\"Price\"] = car_sales[\"Price\"].str.replace('[\\$\\,\\.]', '').astype(int)</code></p> <p>That looks better. What the code on the right of <code>car_sales[\"Price\"]</code> is saying is \"remove the $ sign and comma and change the type of the cell to int\".</p> <p>Let's see what happens.</p> In\u00a0[54]: Copied! <pre># Change Price column to integers\ncar_sales[\"Price\"] = car_sales[\"Price\"].str.replace('[\\$\\,\\.]', '', regex=True)\ncar_sales\n</pre> # Change Price column to integers car_sales[\"Price\"] = car_sales[\"Price\"].str.replace('[\\$\\,\\.]', '', regex=True) car_sales Out[54]: Make Colour Odometer (KM) Doors Price 0 Toyota White 150043 4 400000 1 Honda Red 87899 4 500000 2 Toyota Blue 32549 3 700000 3 BMW Black 11179 5 2200000 4 Nissan White 213095 4 350000 5 Toyota Green 99213 4 450000 6 Honda Blue 45698 4 750000 7 Honda Blue 54738 4 700000 8 Toyota White 60000 4 625000 9 Nissan White 31600 4 970000 <p>Cool! but there are extra zeros in the <code>Price</code> column.</p> <p>Let's remove it.</p> In\u00a0[55]: Copied! <pre># Remove extra zeros from the price column\ncar_sales[\"Price\"] = car_sales[\"Price\"].str[:-2].astype(int)\n</pre> # Remove extra zeros from the price column car_sales[\"Price\"] = car_sales[\"Price\"].str[:-2].astype(int) In\u00a0[56]: Copied! <pre>car_sales.dtypes\n</pre> car_sales.dtypes Out[56]: <pre>Make             object\nColour           object\nOdometer (KM)     int64\nDoors             int64\nPrice             int64\ndtype: object</pre> <p>Beautiful! Now let's try to plot it agian.</p> In\u00a0[57]: Copied! <pre>car_sales[\"Price\"].plot();\n</pre> car_sales[\"Price\"].plot(); <p>This is one of the many ways you can manipulate data using pandas.</p> <p>When you see a number of different functions in a row, it's referred to as chaining. This means you add together a series of functions all to do one overall task.</p> <p>Let's see a few more ways of manipulating data.</p> In\u00a0[58]: Copied! <pre># Lower the Make column\ncar_sales[\"Make\"].str.lower()\n</pre> # Lower the Make column car_sales[\"Make\"].str.lower() Out[58]: <pre>0    toyota\n1     honda\n2    toyota\n3       bmw\n4    nissan\n5    toyota\n6     honda\n7     honda\n8    toyota\n9    nissan\nName: Make, dtype: object</pre> <p>Notice how it doesn't change the values of the original <code>car_sales</code> <code>DataFrame</code> unless we set it equal to.</p> In\u00a0[59]: Copied! <pre># View top 5 rows, Make column not lowered\ncar_sales.head()\n</pre> # View top 5 rows, Make column not lowered car_sales.head() Out[59]: Make Colour Odometer (KM) Doors Price 0 Toyota White 150043 4 4000 1 Honda Red 87899 4 5000 2 Toyota Blue 32549 3 7000 3 BMW Black 11179 5 22000 4 Nissan White 213095 4 3500 In\u00a0[60]: Copied! <pre># Set Make column to be lowered\ncar_sales[\"Make\"] = car_sales[\"Make\"].str.lower()\ncar_sales.head()\n</pre> # Set Make column to be lowered car_sales[\"Make\"] = car_sales[\"Make\"].str.lower() car_sales.head() Out[60]: Make Colour Odometer (KM) Doors Price 0 toyota White 150043 4 4000 1 honda Red 87899 4 5000 2 toyota Blue 32549 3 7000 3 bmw Black 11179 5 22000 4 nissan White 213095 4 3500 <p>Reassigning the column changes it in the original <code>DataFrame</code>. This trend occurs throughout all kinds of data manipulation with pandas.</p> <p>Some functions have a parameter called <code>inplace</code> which means a <code>DataFrame</code> is updated in place without having to reassign it.</p> <p>Let's see what it looks like in combination with <code>.fillna()</code>, a function which fills missing data. But the thing is, our table isn't missing any data.</p> <p>In practice, it's likely you'll work with datasets which aren't complete. What this means is you'll have to decide whether how to fill the missing data or remove the rows which have data missing.</p> <p>Let's check out what a version of our <code>car_sales</code> <code>DataFrame</code> might look like with missing values.</p> In\u00a0[61]: Copied! <pre># Import car sales data with missing values\ncar_sales_missing = pd.read_csv(\"../data/car-sales-missing-data.csv\")\ncar_sales_missing\n</pre> # Import car sales data with missing values car_sales_missing = pd.read_csv(\"../data/car-sales-missing-data.csv\") car_sales_missing Out[61]: Make Colour Odometer Doors Price 0 Toyota White 150043.0 4.0 $4,000 1 Honda Red 87899.0 4.0 $5,000 2 Toyota Blue NaN 3.0 $7,000 3 BMW Black 11179.0 5.0 $22,000 4 Nissan White 213095.0 4.0 $3,500 5 Toyota Green NaN 4.0 $4,500 6 Honda NaN NaN 4.0 $7,500 7 Honda Blue NaN 4.0 NaN 8 Toyota White 60000.0 NaN NaN 9 NaN White 31600.0 4.0 $9,700 <p>Missing values are shown by <code>NaN</code> in pandas. This can be considered the equivalent of <code>None</code> in Python.</p> <p>Let's use the <code>.fillna()</code> function to fill the <code>Odometer</code> column with the average of the other values in the same column.</p> <p>We'll do it with and without <code>inplace</code>.</p> In\u00a0[62]: Copied! <pre># Fill Odometer column missing values with mean\ncar_sales_missing[\"Odometer\"].fillna(car_sales_missing[\"Odometer\"].mean(), \n                                     inplace=False) # inplace is set to False by default\n</pre> # Fill Odometer column missing values with mean car_sales_missing[\"Odometer\"].fillna(car_sales_missing[\"Odometer\"].mean(),                                       inplace=False) # inplace is set to False by default  Out[62]: <pre>0    150043.000000\n1     87899.000000\n2     92302.666667\n3     11179.000000\n4    213095.000000\n5     92302.666667\n6     92302.666667\n7     92302.666667\n8     60000.000000\n9     31600.000000\nName: Odometer, dtype: float64</pre> <p>Now let's check the original <code>car_sales_missing</code> <code>DataFrame</code>.</p> In\u00a0[63]: Copied! <pre>car_sales_missing\n</pre> car_sales_missing Out[63]: Make Colour Odometer Doors Price 0 Toyota White 150043.0 4.0 $4,000 1 Honda Red 87899.0 4.0 $5,000 2 Toyota Blue NaN 3.0 $7,000 3 BMW Black 11179.0 5.0 $22,000 4 Nissan White 213095.0 4.0 $3,500 5 Toyota Green NaN 4.0 $4,500 6 Honda NaN NaN 4.0 $7,500 7 Honda Blue NaN 4.0 NaN 8 Toyota White 60000.0 NaN NaN 9 NaN White 31600.0 4.0 $9,700 <p>Because <code>inplace</code> is set to <code>False</code> (default), there's still missing values in the <code>\"Odometer\"</code> column. Let's try setting <code>inplace</code> to <code>True</code>.</p> In\u00a0[64]: Copied! <pre># Fill the Odometer missing values to the mean with inplace=True\ncar_sales_missing[\"Odometer\"].fillna(car_sales_missing[\"Odometer\"].mean(),\n                                     inplace=True)\n</pre> # Fill the Odometer missing values to the mean with inplace=True car_sales_missing[\"Odometer\"].fillna(car_sales_missing[\"Odometer\"].mean(),                                      inplace=True) <p>Now let's check the <code>car_sales_missing</code> <code>DataFrame</code> again.</p> In\u00a0[65]: Copied! <pre>car_sales_missing\n</pre> car_sales_missing Out[65]: Make Colour Odometer Doors Price 0 Toyota White 150043.000000 4.0 $4,000 1 Honda Red 87899.000000 4.0 $5,000 2 Toyota Blue 92302.666667 3.0 $7,000 3 BMW Black 11179.000000 5.0 $22,000 4 Nissan White 213095.000000 4.0 $3,500 5 Toyota Green 92302.666667 4.0 $4,500 6 Honda NaN 92302.666667 4.0 $7,500 7 Honda Blue 92302.666667 4.0 NaN 8 Toyota White 60000.000000 NaN NaN 9 NaN White 31600.000000 4.0 $9,700 <p>The missing values in the <code>Odometer</code> column have been filled with the mean value of the same column.</p> <p>In practice, you might not want to fill a column's missing values with the mean, but this example was to show the difference between <code>inplace=False</code> (default) and <code>inplace=True</code>.</p> <p>Whichever you choose to use will depend on how you structure your code.</p> <p>All you have to remember is <code>inplace=False</code> returns a copy of the <code>DataFrame</code> you're working with.</p> <p>This is helpful if you want to make a duplicate of your current <code>DataFrame</code> and save it to another variable.</p> <p>Where as, <code>inplace=True</code> makes all the changes directly to the target <code>DataFrame</code>.</p> <p>We've filled some values but there's still missing values in <code>car_sales_missing</code>. Let's say you wanted to remove any rows which had missing data and only work with rows which had complete coverage.</p> <p>You can do this using <code>.dropna()</code>.</p> In\u00a0[66]: Copied! <pre># Remove missing data\ncar_sales_missing.dropna()\n</pre> # Remove missing data car_sales_missing.dropna() Out[66]: Make Colour Odometer Doors Price 0 Toyota White 150043.000000 4.0 $4,000 1 Honda Red 87899.000000 4.0 $5,000 2 Toyota Blue 92302.666667 3.0 $7,000 3 BMW Black 11179.000000 5.0 $22,000 4 Nissan White 213095.000000 4.0 $3,500 5 Toyota Green 92302.666667 4.0 $4,500 <p>It appears the rows with missing values have been removed, now let's check to make sure.</p> In\u00a0[67]: Copied! <pre>car_sales_missing\n</pre> car_sales_missing Out[67]: Make Colour Odometer Doors Price 0 Toyota White 150043.000000 4.0 $4,000 1 Honda Red 87899.000000 4.0 $5,000 2 Toyota Blue 92302.666667 3.0 $7,000 3 BMW Black 11179.000000 5.0 $22,000 4 Nissan White 213095.000000 4.0 $3,500 5 Toyota Green 92302.666667 4.0 $4,500 6 Honda NaN 92302.666667 4.0 $7,500 7 Honda Blue 92302.666667 4.0 NaN 8 Toyota White 60000.000000 NaN NaN 9 NaN White 31600.000000 4.0 $9,700 <p>Hmm, they're still there, can you guess why?</p> <p>It's because <code>.dropna()</code> has <code>inplace=False</code> as default. We can either set <code>inplace=True</code> or reassign the <code>car_sales_missing</code> <code>DataFrame</code>.</p> In\u00a0[68]: Copied! <pre># The following two lines do the same thing\ncar_sales_missing.dropna(inplace=True) # Operation happens inplace without reassignment\ncar_sales_missing = car_sales_missing.dropna() # car_sales_missing gets reassigned to same DataFrame but with dropped values\n</pre> # The following two lines do the same thing car_sales_missing.dropna(inplace=True) # Operation happens inplace without reassignment car_sales_missing = car_sales_missing.dropna() # car_sales_missing gets reassigned to same DataFrame but with dropped values   <p>Now if check again, the rows with missing values are gone and the index numbers have been updated.</p> In\u00a0[69]: Copied! <pre>car_sales_missing\n</pre> car_sales_missing Out[69]: Make Colour Odometer Doors Price 0 Toyota White 150043.000000 4.0 $4,000 1 Honda Red 87899.000000 4.0 $5,000 2 Toyota Blue 92302.666667 3.0 $7,000 3 BMW Black 11179.000000 5.0 $22,000 4 Nissan White 213095.000000 4.0 $3,500 5 Toyota Green 92302.666667 4.0 $4,500 <p>Instead of removing or filling data, what if you wanted to create it?</p> <p>For example, creating a column called <code>Seats</code> for number of seats.</p> <p>pandas allows for simple extra column creation on <code>DataFrame</code>'s.</p> <p>Three common ways are:</p> <ol> <li>Adding a <code>pandas.Series</code> as a column.</li> <li>Adding a Python list as a column.</li> <li>By using existing columns to create a new column.</li> </ol> In\u00a0[70]: Copied! <pre># Create a column from a pandas Series\nseats_column = pd.Series([5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\ncar_sales[\"Seats\"] = seats_column\ncar_sales\n</pre> # Create a column from a pandas Series seats_column = pd.Series([5, 5, 5, 5, 5, 5, 5, 5, 5, 5]) car_sales[\"Seats\"] = seats_column car_sales Out[70]: Make Colour Odometer (KM) Doors Price Seats 0 toyota White 150043 4 4000 5 1 honda Red 87899 4 5000 5 2 toyota Blue 32549 3 7000 5 3 bmw Black 11179 5 22000 5 4 nissan White 213095 4 3500 5 5 toyota Green 99213 4 4500 5 6 honda Blue 45698 4 7500 5 7 honda Blue 54738 4 7000 5 8 toyota White 60000 4 6250 5 9 nissan White 31600 4 9700 5 <p>Creating a column is similar to selecting a column, you pass the target <code>DataFrame</code> along with a new column name in brackets.</p> In\u00a0[71]: Copied! <pre># Create a column from a Python list\nengine_sizes = [1.3, 2.0, 3.0, 4.2, 1.6, 1, 2.0, 2.3, 2.0, 3.0]\ncar_sales[\"Engine Size\"] = engine_sizes\ncar_sales\n</pre> # Create a column from a Python list engine_sizes = [1.3, 2.0, 3.0, 4.2, 1.6, 1, 2.0, 2.3, 2.0, 3.0] car_sales[\"Engine Size\"] = engine_sizes car_sales Out[71]: Make Colour Odometer (KM) Doors Price Seats Engine Size 0 toyota White 150043 4 4000 5 1.3 1 honda Red 87899 4 5000 5 2.0 2 toyota Blue 32549 3 7000 5 3.0 3 bmw Black 11179 5 22000 5 4.2 4 nissan White 213095 4 3500 5 1.6 5 toyota Green 99213 4 4500 5 1.0 6 honda Blue 45698 4 7500 5 2.0 7 honda Blue 54738 4 7000 5 2.3 8 toyota White 60000 4 6250 5 2.0 9 nissan White 31600 4 9700 5 3.0 <p>You can also make a column by directly combining the values of other columns. Such as, price per kilometre on the Odometer.</p> In\u00a0[72]: Copied! <pre># Column from other columns\ncar_sales[\"Price per KM\"] = car_sales[\"Price\"] / car_sales[\"Odometer (KM)\"]\ncar_sales\n</pre> # Column from other columns car_sales[\"Price per KM\"] = car_sales[\"Price\"] / car_sales[\"Odometer (KM)\"] car_sales Out[72]: Make Colour Odometer (KM) Doors Price Seats Engine Size Price per KM 0 toyota White 150043 4 4000 5 1.3 0.026659 1 honda Red 87899 4 5000 5 2.0 0.056883 2 toyota Blue 32549 3 7000 5 3.0 0.215060 3 bmw Black 11179 5 22000 5 4.2 1.967976 4 nissan White 213095 4 3500 5 1.6 0.016425 5 toyota Green 99213 4 4500 5 1.0 0.045357 6 honda Blue 45698 4 7500 5 2.0 0.164121 7 honda Blue 54738 4 7000 5 2.3 0.127882 8 toyota White 60000 4 6250 5 2.0 0.104167 9 nissan White 31600 4 9700 5 3.0 0.306962 <p>Now can you think why this might not be a great column to add?</p> <p>It could be confusing when a car with less kilometers on the odometer looks to cost more per kilometre than one with more.</p> <p>When buying a car, usually less kilometres on the odometer is better.</p> <p>This kind of column creation is called feature engineering, the practice of enriching your dataset with more information (either from it directly or elsewhere).</p> <p>If <code>Make</code>, <code>Colour</code>, <code>Doors</code> are features of the data, creating <code>Price per KM</code> could be another. But in this case, not a very good one.</p> <p>As for column creation, you can also create a new column setting all values to a one standard value.</p> In\u00a0[73]: Copied! <pre># Column to all 1 value (number of wheels)\ncar_sales[\"Number of wheels\"] = 4\ncar_sales\n</pre> # Column to all 1 value (number of wheels) car_sales[\"Number of wheels\"] = 4 car_sales Out[73]: Make Colour Odometer (KM) Doors Price Seats Engine Size Price per KM Number of wheels 0 toyota White 150043 4 4000 5 1.3 0.026659 4 1 honda Red 87899 4 5000 5 2.0 0.056883 4 2 toyota Blue 32549 3 7000 5 3.0 0.215060 4 3 bmw Black 11179 5 22000 5 4.2 1.967976 4 4 nissan White 213095 4 3500 5 1.6 0.016425 4 5 toyota Green 99213 4 4500 5 1.0 0.045357 4 6 honda Blue 45698 4 7500 5 2.0 0.164121 4 7 honda Blue 54738 4 7000 5 2.3 0.127882 4 8 toyota White 60000 4 6250 5 2.0 0.104167 4 9 nissan White 31600 4 9700 5 3.0 0.306962 4 In\u00a0[74]: Copied! <pre>car_sales[\"Passed road safety\"] = True\ncar_sales\n</pre> car_sales[\"Passed road safety\"] = True car_sales Out[74]: Make Colour Odometer (KM) Doors Price Seats Engine Size Price per KM Number of wheels Passed road safety 0 toyota White 150043 4 4000 5 1.3 0.026659 4 True 1 honda Red 87899 4 5000 5 2.0 0.056883 4 True 2 toyota Blue 32549 3 7000 5 3.0 0.215060 4 True 3 bmw Black 11179 5 22000 5 4.2 1.967976 4 True 4 nissan White 213095 4 3500 5 1.6 0.016425 4 True 5 toyota Green 99213 4 4500 5 1.0 0.045357 4 True 6 honda Blue 45698 4 7500 5 2.0 0.164121 4 True 7 honda Blue 54738 4 7000 5 2.3 0.127882 4 True 8 toyota White 60000 4 6250 5 2.0 0.104167 4 True 9 nissan White 31600 4 9700 5 3.0 0.306962 4 True <p>Now you've created some columns, you decide to show your colleague what you've done. When they ask about the <code>Price per KM</code> column, you tell them you're not really sure why it's there.</p> <p>You decide you better remove it to prevent confusion.</p> <p>You can remove a column using <code>.drop('COLUMN_NAME', axis=1)</code>.</p> In\u00a0[75]: Copied! <pre># Drop the Price per KM column\ncar_sales = car_sales.drop(\"Price per KM\", axis=1) # columns live on axis 1\ncar_sales\n</pre> # Drop the Price per KM column car_sales = car_sales.drop(\"Price per KM\", axis=1) # columns live on axis 1 car_sales Out[75]: Make Colour Odometer (KM) Doors Price Seats Engine Size Number of wheels Passed road safety 0 toyota White 150043 4 4000 5 1.3 4 True 1 honda Red 87899 4 5000 5 2.0 4 True 2 toyota Blue 32549 3 7000 5 3.0 4 True 3 bmw Black 11179 5 22000 5 4.2 4 True 4 nissan White 213095 4 3500 5 1.6 4 True 5 toyota Green 99213 4 4500 5 1.0 4 True 6 honda Blue 45698 4 7500 5 2.0 4 True 7 honda Blue 54738 4 7000 5 2.3 4 True 8 toyota White 60000 4 6250 5 2.0 4 True 9 nissan White 31600 4 9700 5 3.0 4 True <p>Why <code>axis=1</code>? Because that's the axis columns live on. Rows live on <code>axis=0</code>.</p> <p>Let's say you wanted to shuffle the order of your <code>DataFrame</code> so you could split it into train, validation and test sets. And even though the order of your samples was random, you wanted to make sure.</p> <p>To do so you could use <code>.sample(frac=1)</code>.</p> <p><code>.sample()</code> randomly samples different rows from a <code>DataFrame</code>.</p> <p>The <code>frac</code> parameter dictates the fraction, where 1 = 100% of rows, 0.5 = 50% of rows, 0.01 = 1% of rows.</p> <p>You can also use <code>.sample(n=1)</code> where <code>n</code> is the number of rows to sample.</p> In\u00a0[76]: Copied! <pre># Sample car_sales\ncar_sales_sampled = car_sales.sample(frac=1)\ncar_sales_sampled\n</pre> # Sample car_sales car_sales_sampled = car_sales.sample(frac=1) car_sales_sampled Out[76]: Make Colour Odometer (KM) Doors Price Seats Engine Size Number of wheels Passed road safety 4 nissan White 213095 4 3500 5 1.6 4 True 6 honda Blue 45698 4 7500 5 2.0 4 True 3 bmw Black 11179 5 22000 5 4.2 4 True 5 toyota Green 99213 4 4500 5 1.0 4 True 1 honda Red 87899 4 5000 5 2.0 4 True 9 nissan White 31600 4 9700 5 3.0 4 True 8 toyota White 60000 4 6250 5 2.0 4 True 7 honda Blue 54738 4 7000 5 2.3 4 True 2 toyota Blue 32549 3 7000 5 3.0 4 True 0 toyota White 150043 4 4000 5 1.3 4 True <p>Notice how the rows remain intact but their order is mixed (check the indexes).</p> <p><code>.sample(frac=X)</code> is also helpful when you're working with a large <code>DataFrame</code>.</p> <p>Say you had 2,000,000 rows.</p> <p>Running tests, analysis and machine learning algorithms on 2,000,000 rows could take a long time. And since being a data scientist or machine learning engineer is about reducing the time between experiments, you might begin with a sample of rows first.</p> <p>For example, you could use <code>40k_rows = 2_mil_rows.sample(frac=0.05)</code> to work on 40,000 rows from a <code>DataFrame</code> called <code>2_mil_rows</code> containing 2,000,000 rows.</p> <p>What if you wanted to get the indexes back in order?</p> <p>You could do so using <code>.reset_index()</code>.</p> In\u00a0[77]: Copied! <pre># Reset the indexes of car_sales_sampled\ncar_sales_sampled.reset_index()\n</pre> # Reset the indexes of car_sales_sampled car_sales_sampled.reset_index() Out[77]: index Make Colour Odometer (KM) Doors Price Seats Engine Size Number of wheels Passed road safety 0 4 nissan White 213095 4 3500 5 1.6 4 True 1 6 honda Blue 45698 4 7500 5 2.0 4 True 2 3 bmw Black 11179 5 22000 5 4.2 4 True 3 5 toyota Green 99213 4 4500 5 1.0 4 True 4 1 honda Red 87899 4 5000 5 2.0 4 True 5 9 nissan White 31600 4 9700 5 3.0 4 True 6 8 toyota White 60000 4 6250 5 2.0 4 True 7 7 honda Blue 54738 4 7000 5 2.3 4 True 8 2 toyota Blue 32549 3 7000 5 3.0 4 True 9 0 toyota White 150043 4 4000 5 1.3 4 True <p>Calling <code>.reset_index()</code> on a <code>DataFrame</code> resets the index numbers to their defaults. It also creates a new <code>Index</code> column by default which contains the previous index values.</p> <p>Finally, what if you wanted to apply a function to a column. Such as, converting the <code>Odometer</code> column from kilometers to miles.</p> <p>You can do so using the <code>.apply()</code> function and passing it a Python lambda function. We know there's about 1.6 kilometers in a mile, so if you divide the value in the <code>Odometer</code> column by 1.6, it should convert it to miles.</p> In\u00a0[78]: Copied! <pre># Change the Odometer values from kilometres to miles\ncar_sales[\"Odometer (KM)\"].apply(lambda x: x / 1.6)\n</pre> # Change the Odometer values from kilometres to miles car_sales[\"Odometer (KM)\"].apply(lambda x: x / 1.6) Out[78]: <pre>0     93776.875\n1     54936.875\n2     20343.125\n3      6986.875\n4    133184.375\n5     62008.125\n6     28561.250\n7     34211.250\n8     37500.000\n9     19750.000\nName: Odometer (KM), dtype: float64</pre> <p>Now let's check our <code>car_sales</code> <code>DataFrame</code>.</p> In\u00a0[79]: Copied! <pre>car_sales\n</pre> car_sales Out[79]: Make Colour Odometer (KM) Doors Price Seats Engine Size Number of wheels Passed road safety 0 toyota White 150043 4 4000 5 1.3 4 True 1 honda Red 87899 4 5000 5 2.0 4 True 2 toyota Blue 32549 3 7000 5 3.0 4 True 3 bmw Black 11179 5 22000 5 4.2 4 True 4 nissan White 213095 4 3500 5 1.6 4 True 5 toyota Green 99213 4 4500 5 1.0 4 True 6 honda Blue 45698 4 7500 5 2.0 4 True 7 honda Blue 54738 4 7000 5 2.3 4 True 8 toyota White 60000 4 6250 5 2.0 4 True 9 nissan White 31600 4 9700 5 3.0 4 True <p>The <code>Odometer</code> column didn't change. Can you guess why?</p> <p>We didn't reassign it.</p> In\u00a0[80]: Copied! <pre># Reassign the Odometer column to be miles instead of kilometers\ncar_sales[\"Odometer (KM)\"] = car_sales[\"Odometer (KM)\"].apply(lambda x: x / 1.6)\ncar_sales\n</pre> # Reassign the Odometer column to be miles instead of kilometers car_sales[\"Odometer (KM)\"] = car_sales[\"Odometer (KM)\"].apply(lambda x: x / 1.6) car_sales Out[80]: Make Colour Odometer (KM) Doors Price Seats Engine Size Number of wheels Passed road safety 0 toyota White 93776.875 4 4000 5 1.3 4 True 1 honda Red 54936.875 4 5000 5 2.0 4 True 2 toyota Blue 20343.125 3 7000 5 3.0 4 True 3 bmw Black 6986.875 5 22000 5 4.2 4 True 4 nissan White 133184.375 4 3500 5 1.6 4 True 5 toyota Green 62008.125 4 4500 5 1.0 4 True 6 honda Blue 28561.250 4 7500 5 2.0 4 True 7 honda Blue 34211.250 4 7000 5 2.3 4 True 8 toyota White 37500.000 4 6250 5 2.0 4 True 9 nissan White 19750.000 4 9700 5 3.0 4 True <p>If you've never seen a lambda function they can be tricky. What the line above is saying is \"take the value in the <code>Odometer (KM)</code> column (<code>x</code>) and set it to be itself divided by 1.6\".</p>"},{"location":"introduction-to-pandas/#a-quick-introduction-to-data-analysis-and-manipulation-with-python-and-pandas","title":"A Quick Introduction to Data Analysis and Manipulation with Python and pandas\u00b6","text":""},{"location":"introduction-to-pandas/#what-is-pandas","title":"What is pandas?\u00b6","text":"<p>If you're getting into machine learning and data science and you're using Python, you're going to use pandas.</p> <p>pandas is an open source library which helps you analyse and manipulate data.</p> <p></p>"},{"location":"introduction-to-pandas/#why-pandas","title":"Why pandas?\u00b6","text":"<p>pandas provides a simple to use but very capable set of functions you can use to on your data.</p> <p>It's integrated with many other data science and machine learning tools which use Python so having an understanding of it will be helpful throughout your journey.</p> <p>One of the main use cases you'll come across is using pandas to transform your data in a way which makes it usable with machine learning algorithms.</p>"},{"location":"introduction-to-pandas/#what-does-this-notebook-cover","title":"What does this notebook cover?\u00b6","text":"<p>Because the pandas library is vast, there's often many ways to do the same thing. This notebook covers some of the most fundamental functions of the library, which are more than enough to get started.</p>"},{"location":"introduction-to-pandas/#where-can-i-get-help","title":"Where can I get help?\u00b6","text":"<p>If you get stuck or think of something you'd like to do which this notebook doesn't cover, don't fear!</p> <p>The recommended steps you take are:</p> <ol> <li>Try it - Since pandas is very friendly, your first step should be to use what you know and try figure out the answer to your own question (getting it wrong is part of the process). If in doubt, run your code.</li> <li>Search for it - If trying it on your own doesn't work, since someone else has probably tried to do something similar, try searching for your problem in the following places (either via a search engine or direct):<ul> <li>pandas documentation - the best place for learning pandas, this resource covers all of the pandas functionality.</li> <li>Stack Overflow - this is the developers Q&amp;A hub, it's full of questions and answers of different problems across a wide range of software development topics and chances are, there's one related to your problem.</li> <li>ChatGPT - ChatGPT is very good at explaining code, however, it can make mistakes. Best to verify the code it writes first before using it. Try asking \"Can you explain the following code for me? {your code here}\" and then continue with follow up questions from there.</li> </ul> </li> </ol> <p>An example of searching for a pandas function might be:</p> <p>\"how to fill all the missing values of two columns using pandas\"</p> <p>Searching this on Google leads to this post on Stack Overflow: https://stackoverflow.com/questions/36556256/how-do-i-fill-na-values-in-multiple-columns-in-pandas</p> <p>The next steps here are to read through the post and see if it relates to your problem. If it does, great, take the code/information you need and rewrite it to suit your own problem.</p> <ol> <li>Ask for help - If you've been through the above 2 steps and you're still stuck, you might want to ask your question on Stack Overflow. Remember to be specific as possible and provide details on what you've tried.</li> </ol> <p>Remember, you don't have to learn all of these functions off by heart to begin with.</p> <p>What's most important is remembering to continually ask yourself, \"what am I trying to do with the data?\".</p> <p>Start by answering that question and then practicing finding the code which does it.</p> <p>Let's get started.</p>"},{"location":"introduction-to-pandas/#0-importing-pandas","title":"0. Importing pandas\u00b6","text":"<p>To get started using pandas, the first step is to import it.</p> <p>The most common way (and method you should use) is to import pandas as the abbreviation <code>pd</code> (e.g. <code>pandas</code> -&gt; <code>pd</code>).</p> <p>If you see the letters <code>pd</code> used anywhere in machine learning or data science, it's probably referring to the pandas library.</p>"},{"location":"introduction-to-pandas/#1-datatypes","title":"1. Datatypes\u00b6","text":"<p>pandas has two main datatypes, <code>Series</code> and <code>DataFrame</code>.</p> <ul> <li><code>pandas.Series</code> - a 1-dimensional column of data.</li> <li><code>pandas.DataFrame</code> (most common) - a 2-dimesional table of data with rows and columns.</li> </ul> <p>You can create a <code>Series</code> using <code>pd.Series()</code> and passing it a Python list.</p>"},{"location":"introduction-to-pandas/#exercises","title":"Exercises\u00b6","text":"<ol> <li>Make a <code>Series</code> of different foods.</li> <li>Make a <code>Series</code> of different dollar values (these can be integers).</li> <li>Combine your <code>Series</code>'s of foods and dollar values into a <code>DataFrame</code>.</li> </ol> <p>Try it out for yourself first, then see how your code goes against the solution.</p> <p>Note: Make sure your two <code>Series</code> are the same size before combining them in a DataFrame.</p>"},{"location":"introduction-to-pandas/#2-importing-data","title":"2. Importing data\u00b6","text":"<p>Creating <code>Series</code> and <code>DataFrame</code>'s from scratch is nice but what you'll usually be doing is importing your data in the form of a <code>.csv</code> (comma separated value), spreadsheet file or something similar such as an SQL database.</p> <p>pandas allows for easy importing of data like this through functions such as <code>pd.read_csv()</code> and <code>pd.read_excel()</code> (for Microsoft Excel files).</p> <p>Say you wanted to get this information from this Google Sheet document into a pandas <code>DataFrame</code>.</p> <p>You could export it as a <code>.csv</code> file and then import it using <code>pd.read_csv()</code>.</p> <p>Tip: If the Google Sheet is public, <code>pd.read_csv()</code> can read it via URL, try searching for \"pandas read Google Sheet with URL\".</p> <p>In this case, the exported <code>.csv</code> file is called <code>car-sales.csv</code>.</p>"},{"location":"introduction-to-pandas/#anatomy-of-a-dataframe","title":"Anatomy of a DataFrame\u00b6","text":"<p>Different functions use different labels for different things. This graphic sums up some of the main components of <code>DataFrame</code>'s and their different names.</p> <p></p>"},{"location":"introduction-to-pandas/#3-exporting-data","title":"3. Exporting data\u00b6","text":"<p>After you've made a few changes to your data, you might want to export it and save it so someone else can access the changes.</p> <p>pandas allows you to export <code>DataFrame</code>'s to <code>.csv</code> format using <code>.to_csv()</code> or spreadsheet format using <code>.to_excel()</code>.</p> <p>We haven't made any changes yet to the <code>car_sales</code> <code>DataFrame</code> but let's try export it.</p>"},{"location":"introduction-to-pandas/#exercises","title":"Exercises\u00b6","text":"<ol> <li>Practice importing a <code>.csv</code> file using <code>pd.read_csv()</code>, you can download <code>heart-disease.csv</code>. This file contains annonymous patient medical records and whether or not they have heart disease.</li> <li>Practice exporting a <code>DataFrame</code> using <code>.to_csv()</code>. You could export the heart disease <code>DataFrame</code> after you've imported it.</li> </ol> <p>Note:</p> <ul> <li>Make sure the <code>heart-disease.csv</code> file is in the same folder as your notebook orbe sure to use the filepath where the file is.</li> <li>You can name the variables and exported files whatever you like but make sure they're readable.</li> </ul>"},{"location":"introduction-to-pandas/#example-solution","title":"Example solution\u00b6","text":""},{"location":"introduction-to-pandas/#4-describing-data","title":"4. Describing data\u00b6","text":"<p>One of the first things you'll want to do after you import some data into a pandas <code>DataFrame</code> is to start exploring it.</p> <p>pandas has many built in functions which allow you to quickly get information about a <code>DataFrame</code>.</p> <p>Let's explore some using the <code>car_sales</code> <code>DataFrame</code>.</p>"},{"location":"introduction-to-pandas/#5-viewing-and-selecting-data","title":"5. Viewing and selecting data\u00b6","text":"<p>Some common methods for viewing and selecting data in a pandas DataFrame include:</p> <ul> <li><code>DataFrame.head(n=5)</code> - Displays the first <code>n</code> rows of a DataFrame (e.g. <code>car_sales.head()</code> will show the first 5 rows of the <code>car_sales</code> DataFrame).</li> <li><code>DataFrame.tail(n=5)</code> - Displays the last <code>n</code> rows of a DataFrame.</li> <li><code>DataFrame.loc[]</code> - Accesses a group of rows and columns by labels or a boolean array.</li> <li><code>DataFrame.iloc[]</code> - Accesses a group of rows and columns by integer indices (e.g. <code>car_sales.iloc[0]</code> shows all the columns from index <code>0</code>.</li> <li><code>DataFrame.columns</code> -  Lists the column labels of the DataFrame.</li> <li><code>DataFrame['A']</code> - Selects the column named <code>'A'</code> from the DataFrame.</li> <li><code>DataFrame[DataFrame['A'] &gt; 5]</code> - Boolean indexing filters rows based on column values meeting a condition (e.g. all rows from column <code>'A'</code> greater than <code>5</code>.</li> <li><code>DataFrame.plot()</code> - Creates a line plot of a DataFrame's columns (e.g. plot <code>Make</code> vs. <code>Odometer (KM)</code> columns with <code>car_sales[[\"Make\", \"Odometer (KM)\"]].plot();</code>).</li> <li><code>DataFrame.hist()</code> - Generates histograms for columns in a DataFrame.</li> <li><code>pandas.crosstab()</code> - Computes a cross-tabulation of two or more factors.</li> </ul> <p>In practice, you'll constantly be making changes to your data, and viewing it. Changing it, viewing it, changing it, viewing it.</p> <p>You won't always want to change all of the data in your <code>DataFrame</code>'s either. So there are just as many different ways to select data as there is to view it.</p> <p><code>.head()</code> allows you to view the first 5 rows of your <code>DataFrame</code>. You'll likely be using this one a lot.</p>"},{"location":"introduction-to-pandas/#6-manipulating-data","title":"6. Manipulating data\u00b6","text":"<p>You've seen an example of one way to manipulate data but pandas has many more.</p> <p>How many more?</p> <p>Put it this way, if you can imagine it, chances are, pandas can do it.</p> <p>Let's start with string methods. Because pandas is based on Python, however you can manipulate strings in Python, you can do the same in pandas.</p> <p>You can access the string value of a column using <code>.str</code>. Knowing this, how do you think you'd set a column to lowercase?</p>"},{"location":"introduction-to-pandas/#summary","title":"Summary\u00b6","text":""},{"location":"introduction-to-pandas/#main-topics-we-covered","title":"Main topics we covered\u00b6","text":"<ul> <li>Series - a single column (can be multiple rows) of values.</li> <li>DataFrame - multiple columns/rows of values (a DataFrame is comprised of multiple Series).</li> <li>Importing data - we used <code>pd.read_csv()</code> to read in a CSV (comma-separated values) file but there are multiple options for reading data.</li> <li>Exporting data - we exported our data using <code>to_csv()</code>, however there are multiple methods of exporting data.</li> <li>Describing data<ul> <li><code>df.dtypes</code> - find the datatypes present in a dataframe.</li> <li><code>df.describe()</code> - find various numerical features of a dataframe.</li> <li><code>df.info()</code> - find the number of rows and whether or not any of them are empty.</li> </ul> </li> <li>Viewing and selecting data<ul> <li><code>df.head()</code> - view the first 5 rows of <code>df</code>.</li> <li><code>df.loc</code> &amp; <code>df.iloc</code> - select specific parts of a dataframe.</li> <li><code>df['A']</code> - select column <code>A</code> of <code>df</code>.</li> <li><code>df[df['A'] &gt; 1000]</code> - selection column <code>A</code> rows with values over 1000 of <code>df</code>.</li> <li><code>df['A']</code> - plot values from column <code>A</code> using matplotlib (defaults to line graph).</li> </ul> </li> <li>Manipulating data and performing operations - pandas has many built-in functions you can use to manipulate data, also many of the Python operators (e.g. <code>+</code>, <code>-</code>, <code>&gt;</code>, <code>==</code>) work with pandas.</li> </ul>"},{"location":"introduction-to-pandas/#further-reading","title":"Further reading\u00b6","text":"<p>Since pandas is such a large library, it would be impossible to cover it all in one go.</p> <p>The following are some resources you might want to look into for more.</p> <ul> <li>Python for Data Analysis by Wes McKinney - possibly the most complete text of the pandas library (apart from the documentation itself) written by the creator of pandas.</li> <li>Data Manipulation with Pandas (section of Python Data Science Handbook by Jake VanderPlas) - a very hands-on approach to many of the main functions in the pandas library.</li> </ul>"},{"location":"introduction-to-pandas/#exercises","title":"Exercises\u00b6","text":"<p>After completing this notebook, you next thing should be to try out some more pandas code of your own.</p> <p>I'd suggest at least going through number 1 (write out all the code yourself), a couple from number 2 (again, write out the code yourself) and spend an hour reading number 3 (this is vast but keep it in mind).</p> <ol> <li>10-minute introduction to pandas - go through all the functions here and be sure to write out the code yourself.</li> <li>Pandas getting started tutorial - pick a couple from here which spark your interest and go through them both writing out the code for your self.</li> <li>Pandas essential basic functionality - spend an hour reading this and bookmark it for whenever you need to come back for an overview of pandas.</li> </ol>"},{"location":"introduction-to-scikit-learn/","title":"Introduction to Scikit-Learn","text":"In\u00a0[1]: Copied! <pre>import datetime\nprint(f\"Last updated: {datetime.datetime.now()}\")\n</pre> import datetime print(f\"Last updated: {datetime.datetime.now()}\") <pre>Last updated: 2023-10-13 09:47:50.863667\n</pre> In\u00a0[2]: Copied! <pre># Standard imports\n# %matplotlib inline # No longer required in newer versions of Jupyter (2022+)\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport sklearn\nprint(f\"Using Scikit-Learn version: {sklearn.__version__} (materials in this notebook require this version or newer).\")\n</pre> # Standard imports # %matplotlib inline # No longer required in newer versions of Jupyter (2022+) import matplotlib.pyplot as plt import numpy as np import pandas as pd  import sklearn print(f\"Using Scikit-Learn version: {sklearn.__version__} (materials in this notebook require this version or newer).\") <pre>Using Scikit-Learn version: 1.3.1 (materials in this notebook require this version or newer).\n</pre> In\u00a0[3]: Copied! <pre>import pandas as pd\nheart_disease = pd.read_csv('../data/heart-disease.csv')\nheart_disease.head()\n</pre> import pandas as pd heart_disease = pd.read_csv('../data/heart-disease.csv') heart_disease.head() Out[3]: age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target 0 63 1 3 145 233 1 0 150 0 2.3 0 0 1 1 1 37 1 2 130 250 0 1 187 0 3.5 0 0 2 1 2 41 0 1 130 204 0 0 172 0 1.4 2 0 2 1 3 56 1 1 120 236 0 1 178 0 0.8 2 0 2 1 4 57 0 0 120 354 0 1 163 1 0.6 2 0 2 1 <p>Here, each row is a different patient and all columns except <code>target</code> are different patient characteristics.</p> <p>The <code>target</code> column indicates whether the patient has heart disease (<code>target=1</code>) or not (<code>target=0</code>), this is our \"label\" columnm, the variable we're going to try and predict.</p> <p>The rest of the columns (often called features) are what we'll be using to predict the <code>target</code> value.</p> <p>Note: It's a common custom to save features to a varialbe <code>X</code> and labels to a variable <code>y</code>. In practice, we'd like to use the <code>X</code> (features) to build a predictive algorithm to predict the <code>y</code> (labels).</p> In\u00a0[4]: Copied! <pre># Create X (all the feature columns)\nX = heart_disease.drop(\"target\", axis=1)\n\n# Create y (the target column)\ny = heart_disease[\"target\"]\n\n# Check the head of the features DataFrame\nX.head()\n</pre> # Create X (all the feature columns) X = heart_disease.drop(\"target\", axis=1)  # Create y (the target column) y = heart_disease[\"target\"]  # Check the head of the features DataFrame X.head() Out[4]: age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal 0 63 1 3 145 233 1 0 150 0 2.3 0 0 1 1 37 1 2 130 250 0 1 187 0 3.5 0 0 2 2 41 0 1 130 204 0 0 172 0 1.4 2 0 2 3 56 1 1 120 236 0 1 178 0 0.8 2 0 2 4 57 0 0 120 354 0 1 163 1 0.6 2 0 2 In\u00a0[5]: Copied! <pre># Check the head and the value counts of the labels \ny.head(), y.value_counts()\n</pre> # Check the head and the value counts of the labels  y.head(), y.value_counts() Out[5]: <pre>(0    1\n 1    1\n 2    1\n 3    1\n 4    1\n Name: target, dtype: int64,\n target\n 1    165\n 0    138\n Name: count, dtype: int64)</pre> <p>One of the most important practices in machine learning is to split datasets into training and test sets.</p> <p>As in, a model will train on the training set to learn patterns and then those patterns can be evaluated on the test set.</p> <p>Crucially, a model should never see testing data during training.</p> <p>This is equivalent to a student studying course materials during the semester (training set) and then testing their abilities on the following exam (testing set).</p> <p>Scikit-learn provides the <code>sklearn.model_selection.train_test_split</code> method to split datasets in training and test sets.</p> <p>Note: A common practice to use an 80/20 or 70/30 or 75/25 split for training/testing data. There is also a third set, known as a validation set (e.g. 70/15/15 for training/validation/test) for hyperparamter tuning on but for now we'll focus on training and test sets.</p> In\u00a0[6]: Copied! <pre># Split the data into training and test sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y,\n                                                    test_size=0.25) # by default train_test_split uses 25% of the data for the test set\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n</pre> # Split the data into training and test sets from sklearn.model_selection import train_test_split  X_train, X_test, y_train, y_test = train_test_split(X,                                                      y,                                                     test_size=0.25) # by default train_test_split uses 25% of the data for the test set  X_train.shape, X_test.shape, y_train.shape, y_test.shape Out[6]: <pre>((227, 13), (76, 13), (227,), (76,))</pre> In\u00a0[7]: Copied! <pre># Since we're working on a classification problem, we'll start with a RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()\n</pre> # Since we're working on a classification problem, we'll start with a RandomForestClassifier from sklearn.ensemble import RandomForestClassifier  clf = RandomForestClassifier() <p>We can see the current hyperparameters of a model with the <code>get_params()</code> method.</p> In\u00a0[8]: Copied! <pre># View the current hyperparameters\nclf.get_params()\n</pre> # View the current hyperparameters clf.get_params() Out[8]: <pre>{'bootstrap': True,\n 'ccp_alpha': 0.0,\n 'class_weight': None,\n 'criterion': 'gini',\n 'max_depth': None,\n 'max_features': 'sqrt',\n 'max_leaf_nodes': None,\n 'max_samples': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'n_estimators': 100,\n 'n_jobs': None,\n 'oob_score': False,\n 'random_state': None,\n 'verbose': 0,\n 'warm_start': False}</pre> <p>We'll leave this as is for now, as Scikit-Learn models generally have good default settings.</p> In\u00a0[9]: Copied! <pre>clf.fit(X=X_train, y=y_train)\n</pre> clf.fit(X=X_train, y=y_train) Out[9]: <pre>RandomForestClassifier()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier<pre>RandomForestClassifier()</pre> In\u00a0[10]: Copied! <pre># This doesn't work... incorrect shapes\ny_label = clf.predict(np.array([0, 2, 3, 4]))\n</pre> # This doesn't work... incorrect shapes y_label = clf.predict(np.array([0, 2, 3, 4])) <pre>/Users/daniel/code/zero-to-mastery-ml/env/lib/python3.10/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n  warnings.warn(\n</pre> <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/Users/daniel/code/zero-to-mastery-ml/section-2-data-science-and-ml-tools/introduction-to-scikit-learn.ipynb Cell 24 line 2\n      &lt;a href='vscode-notebook-cell:/Users/daniel/code/zero-to-mastery-ml/section-2-data-science-and-ml-tools/introduction-to-scikit-learn.ipynb#X32sZmlsZQ%3D%3D?line=0'&gt;1&lt;/a&gt; # This doesn't work... incorrect shapes\n----&gt; &lt;a href='vscode-notebook-cell:/Users/daniel/code/zero-to-mastery-ml/section-2-data-science-and-ml-tools/introduction-to-scikit-learn.ipynb#X32sZmlsZQ%3D%3D?line=1'&gt;2&lt;/a&gt; y_label = clf.predict(np.array([0, 2, 3, 4]))\n\nFile ~/code/zero-to-mastery-ml/env/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:823, in ForestClassifier.predict(self, X)\n    802 def predict(self, X):\n    803     \"\"\"\n    804     Predict class for X.\n    805 \n   (...)\n    821         The predicted classes.\n    822     \"\"\"\n--&gt; 823     proba = self.predict_proba(X)\n    825     if self.n_outputs_ == 1:\n    826         return self.classes_.take(np.argmax(proba, axis=1), axis=0)\n\nFile ~/code/zero-to-mastery-ml/env/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:865, in ForestClassifier.predict_proba(self, X)\n    863 check_is_fitted(self)\n    864 # Check data\n--&gt; 865 X = self._validate_X_predict(X)\n    867 # Assign chunk of trees to jobs\n    868 n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)\n\nFile ~/code/zero-to-mastery-ml/env/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:599, in BaseForest._validate_X_predict(self, X)\n    596 \"\"\"\n    597 Validate X whenever one tries to predict, apply, predict_proba.\"\"\"\n    598 check_is_fitted(self)\n--&gt; 599 X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n    600 if issparse(X) and (X.indices.dtype != np.intc or X.indptr.dtype != np.intc):\n    601     raise ValueError(\"No support for np.int64 index based sparse matrices\")\n\nFile ~/code/zero-to-mastery-ml/env/lib/python3.10/site-packages/sklearn/base.py:605, in BaseEstimator._validate_data(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\n    603         out = X, y\n    604 elif not no_val_X and no_val_y:\n--&gt; 605     out = check_array(X, input_name=\"X\", **check_params)\n    606 elif no_val_X and not no_val_y:\n    607     out = _check_y(y, **check_params)\n\nFile ~/code/zero-to-mastery-ml/env/lib/python3.10/site-packages/sklearn/utils/validation.py:938, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\n    936     # If input is 1D raise error\n    937     if array.ndim == 1:\n--&gt; 938         raise ValueError(\n    939             \"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\n    940             \"Reshape your data either using array.reshape(-1, 1) if \"\n    941             \"your data has a single feature or array.reshape(1, -1) \"\n    942             \"if it contains a single sample.\".format(array)\n    943         )\n    945 if dtype_numeric and hasattr(array.dtype, \"kind\") and array.dtype.kind in \"USV\":\n    946     raise ValueError(\n    947         \"dtype='numeric' is not compatible with arrays of bytes/strings.\"\n    948         \"Convert your data to numeric values explicitly instead.\"\n    949     )\n\nValueError: Expected 2D array, got 1D array instead:\narray=[0. 2. 3. 4.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.</pre> <p>Since our model was trained on data from <code>X_train</code>, predictions should be made on data in the same format and shape as <code>X_train</code>.</p> <p>Our goal in many machine learning problems is to use patterns learned from the training data to make predictions on the test data (or future unseen data).</p> In\u00a0[11]: Copied! <pre># In order to predict a label, data has to be in the same shape as X_train\nX_test.head()\n</pre> # In order to predict a label, data has to be in the same shape as X_train X_test.head() Out[11]: age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal 298 57 0 0 140 241 0 1 123 1 0.2 1 0 3 102 63 0 1 140 195 0 1 179 0 0.0 2 2 2 254 59 1 3 160 273 0 0 125 0 0.0 2 0 2 171 48 1 1 110 229 0 1 168 0 1.0 0 0 3 163 38 1 2 138 175 0 1 173 0 0.0 2 4 2 In\u00a0[12]: Copied! <pre># Use the model to make a prediction on the test data (further evaluation)\ny_preds = clf.predict(X=X_test)\n</pre> # Use the model to make a prediction on the test data (further evaluation) y_preds = clf.predict(X=X_test) In\u00a0[13]: Copied! <pre># Evaluate the model on the training set\ntrain_acc = clf.score(X=X_train, y=y_train)\nprint(f\"The model's accuracy on the training dataset is: {train_acc*100}%\")\n</pre> # Evaluate the model on the training set train_acc = clf.score(X=X_train, y=y_train) print(f\"The model's accuracy on the training dataset is: {train_acc*100}%\") <pre>The model's accuracy on the training dataset is: 100.0%\n</pre> <p>Woah! Looks like our model does pretty well on the training datset.</p> <p>This is because it has a chance to see both data and labels.</p> <p>How about the test dataset?</p> In\u00a0[14]: Copied! <pre># Evaluate the model on the test set\ntest_acc = clf.score(X=X_test, y=y_test)\nprint(f\"The model's accuracy on the testing dataset is: {test_acc*100:.2f}%\")\n</pre> # Evaluate the model on the test set test_acc = clf.score(X=X_test, y=y_test) print(f\"The model's accuracy on the testing dataset is: {test_acc*100:.2f}%\") <pre>The model's accuracy on the testing dataset is: 75.00%\n</pre> <p>Hmm, looks like our model's accuracy is a bit less on the test dataset than the training dataset.</p> <p>This is quite often the case, because remember, a model has never seen the testing examples before.</p> <p>There are also a number of other evaluation methods we can use for our classification models.</p> <p>All of the following classification metrics come from the <code>sklearn.metrics</code> module:</p> <ul> <li><code>classification_report(y_true, y_true)</code> - Builds a text report showing various classification metrics such as precision, recall and F1-score.</li> <li><code>confusion_matrix(y_true, y_pred)</code> - Create a confusion matrix to compare predictions to truth labels.</li> <li><code>accuracy_score(y_true, y_pred)</code> - Find the accuracy score (the default metric) for a classifier.</li> </ul> <p>All metrics have the following in common: they compare a model's predictions (<code>y_pred</code>) to truth labels (<code>y_true</code>).</p> In\u00a0[15]: Copied! <pre>from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Create a classification report\nprint(classification_report(y_test, y_preds))\n</pre> from sklearn.metrics import classification_report, confusion_matrix, accuracy_score  # Create a classification report print(classification_report(y_test, y_preds)) <pre>              precision    recall  f1-score   support\n\n           0       0.81      0.60      0.69        35\n           1       0.72      0.88      0.79        41\n\n    accuracy                           0.75        76\n   macro avg       0.76      0.74      0.74        76\nweighted avg       0.76      0.75      0.74        76\n\n</pre> In\u00a0[16]: Copied! <pre># Create a confusion matrix\nconf_mat = confusion_matrix(y_test, y_preds)\nconf_mat\n</pre> # Create a confusion matrix conf_mat = confusion_matrix(y_test, y_preds) conf_mat Out[16]: <pre>array([[21, 14],\n       [ 5, 36]])</pre> In\u00a0[17]: Copied! <pre># Compute the accuracy score (same as the score() method for classifiers) \naccuracy_score(y_test, y_preds)\n</pre> # Compute the accuracy score (same as the score() method for classifiers)  accuracy_score(y_test, y_preds) Out[17]: <pre>0.75</pre> In\u00a0[18]: Copied! <pre># Try different numbers of estimators (trees)... (no cross-validation)\nnp.random.seed(42)\nfor i in range(100, 200, 10):\n    print(f\"Trying model with {i} estimators...\")\n    model = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)\n    print(f\"Model accuracy on test set: {model.score(X_test, y_test) * 100:.2f}%\")\n    print(\"\")\n</pre> # Try different numbers of estimators (trees)... (no cross-validation) np.random.seed(42) for i in range(100, 200, 10):     print(f\"Trying model with {i} estimators...\")     model = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)     print(f\"Model accuracy on test set: {model.score(X_test, y_test) * 100:.2f}%\")     print(\"\") <pre>Trying model with 100 estimators...\nModel accuracy on test set: 73.68%\n\nTrying model with 110 estimators...\nModel accuracy on test set: 73.68%\n\nTrying model with 120 estimators...\nModel accuracy on test set: 75.00%\n\nTrying model with 130 estimators...\nModel accuracy on test set: 72.37%\n\nTrying model with 140 estimators...\nModel accuracy on test set: 73.68%\n\nTrying model with 150 estimators...\nModel accuracy on test set: 73.68%\n\nTrying model with 160 estimators...\nModel accuracy on test set: 73.68%\n\nTrying model with 170 estimators...\nModel accuracy on test set: 75.00%\n\nTrying model with 180 estimators...\nModel accuracy on test set: 73.68%\n\nTrying model with 190 estimators...\nModel accuracy on test set: 75.00%\n\n</pre> <p>The metrics above were measured on a single train and test split.</p> <p>Let's use <code>sklearn.model_selection.cross_val_score</code> to measure the results across 5 different train and test sets.</p> <p>We can achieve this by setting <code>cross_val_score(X, y, cv=5)</code>.</p> <p>Where <code>X</code> is the full feature set and <code>y</code> is the full label set and <code>cv</code> is the number of train and test splits <code>cross_val_score</code> will automatically create from the data (in our case, <code>5</code> different splits, this is known as 5-fold cross-validation).</p> In\u00a0[19]: Copied! <pre>from sklearn.model_selection import cross_val_score\n\n# With cross-validation\nnp.random.seed(42)\nfor i in range(100, 200, 10):\n    print(f\"Trying model with {i} estimators...\")\n    model = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)\n\n    # Measure the model score on a single train/test split\n    model_score = model.score(X_test, y_test)\n    print(f\"Model accuracy on single test set split: {model_score * 100:.2f}%\")\n    \n    # Measure the mean cross-validation score across 5 different train and test splits\n    cross_val_mean = np.mean(cross_val_score(model, X, y, cv=5))\n    print(f\"5-fold cross-validation score: {cross_val_mean * 100:.2f}%\")\n    \n    print(\"\")\n</pre> from sklearn.model_selection import cross_val_score  # With cross-validation np.random.seed(42) for i in range(100, 200, 10):     print(f\"Trying model with {i} estimators...\")     model = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)      # Measure the model score on a single train/test split     model_score = model.score(X_test, y_test)     print(f\"Model accuracy on single test set split: {model_score * 100:.2f}%\")          # Measure the mean cross-validation score across 5 different train and test splits     cross_val_mean = np.mean(cross_val_score(model, X, y, cv=5))     print(f\"5-fold cross-validation score: {cross_val_mean * 100:.2f}%\")          print(\"\") <pre>Trying model with 100 estimators...\nModel accuracy on single test set split: 73.68%\n5-fold cross-validation score: 82.15%\n\nTrying model with 110 estimators...\nModel accuracy on single test set split: 73.68%\n5-fold cross-validation score: 81.17%\n\nTrying model with 120 estimators...\nModel accuracy on single test set split: 75.00%\n5-fold cross-validation score: 83.49%\n\nTrying model with 130 estimators...\nModel accuracy on single test set split: 72.37%\n5-fold cross-validation score: 83.14%\n\nTrying model with 140 estimators...\nModel accuracy on single test set split: 73.68%\n5-fold cross-validation score: 82.48%\n\nTrying model with 150 estimators...\nModel accuracy on single test set split: 73.68%\n5-fold cross-validation score: 80.17%\n\nTrying model with 160 estimators...\nModel accuracy on single test set split: 71.05%\n5-fold cross-validation score: 80.83%\n\nTrying model with 170 estimators...\nModel accuracy on single test set split: 73.68%\n5-fold cross-validation score: 82.16%\n\nTrying model with 180 estimators...\nModel accuracy on single test set split: 72.37%\n5-fold cross-validation score: 81.50%\n\nTrying model with 190 estimators...\nModel accuracy on single test set split: 72.37%\n5-fold cross-validation score: 81.83%\n\n</pre> <p>Which model had the best cross-validation score?</p> <p>This is usually a better indicator of a quality model than a single split accuracy score.</p> <p>Rather than set up and track the results of these experiments manually, we can get Scikit-Learn to do the exploration for us.</p> <p>Scikit-Learn's <code>sklearn.model_selection.GridSearchCV</code> is a way to search over a set of different hyperparameter values and automatically track which perform the best.</p> <p>Let's test it!</p> In\u00a0[20]: Copied! <pre># Another way to do it with GridSearchCV...\nnp.random.seed(42)\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the parameters to search over in dictionary form \n# (these can be any of your target model's hyperparameters) \nparam_grid = {'n_estimators': [i for i in range(100, 200, 10)]}\n\n# Setup the grid search\ngrid = GridSearchCV(estimator=RandomForestClassifier(),\n                    param_grid=param_grid,\n                    cv=5,\n                    verbose=1) \n\n# Fit the grid search to the data\ngrid.fit(X, y)\n\n# Find the best parameters\nprint(f\"The best parameter values are: {grid.best_params_}\")\nprint(f\"With a score of: {grid.best_score_*100:.2f}%\")\n</pre> # Another way to do it with GridSearchCV... np.random.seed(42) from sklearn.model_selection import GridSearchCV  # Define the parameters to search over in dictionary form  # (these can be any of your target model's hyperparameters)  param_grid = {'n_estimators': [i for i in range(100, 200, 10)]}  # Setup the grid search grid = GridSearchCV(estimator=RandomForestClassifier(),                     param_grid=param_grid,                     cv=5,                     verbose=1)   # Fit the grid search to the data grid.fit(X, y)  # Find the best parameters print(f\"The best parameter values are: {grid.best_params_}\") print(f\"With a score of: {grid.best_score_*100:.2f}%\") <pre>Fitting 5 folds for each of 10 candidates, totalling 50 fits\nThe best parameter values are: {'n_estimators': 120}\nWith a score of: 82.82%\n</pre> <p>We can extract the best model/estimator with the <code>best_estimator_</code> attribute.</p> In\u00a0[21]: Copied! <pre># Set the model to be the best estimator\nclf = grid.best_estimator_\nclf\n</pre> # Set the model to be the best estimator clf = grid.best_estimator_ clf Out[21]: <pre>RandomForestClassifier(n_estimators=120)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier<pre>RandomForestClassifier(n_estimators=120)</pre> <p>And now we've got the best cross-validated model, we can fit and score it on our original single train/test split of the data.</p> In\u00a0[22]: Copied! <pre># Fit the best model\nclf = clf.fit(X_train, y_train)\n\n# Find the best model scores on our single test split\n# (note: this may be lower than the cross-validation score since it's only on one split of the data)\nprint(f\"Best model score on single split of the data: {clf.score(X_test, y_test)*100:.2f}%\")\n</pre> # Fit the best model clf = clf.fit(X_train, y_train)  # Find the best model scores on our single test split # (note: this may be lower than the cross-validation score since it's only on one split of the data) print(f\"Best model score on single split of the data: {clf.score(X_test, y_test)*100:.2f}%\") <pre>Best model score on single split of the data: 75.00%\n</pre> In\u00a0[23]: Copied! <pre>import pickle\n\n# Save an existing model to file\npickle.dump(model, open(\"random_forest_model_1.pkl\", \"wb\"))\n</pre> import pickle  # Save an existing model to file pickle.dump(model, open(\"random_forest_model_1.pkl\", \"wb\")) In\u00a0[24]: Copied! <pre># Load a saved pickle model and evaluate it\nloaded_pickle_model = pickle.load(open(\"random_forest_model_1.pkl\", \"rb\"))\nprint(f\"Loaded pickle model prediction score: {loaded_pickle_model.score(X_test, y_test) * 100:.2f}%\")\n</pre> # Load a saved pickle model and evaluate it loaded_pickle_model = pickle.load(open(\"random_forest_model_1.pkl\", \"rb\")) print(f\"Loaded pickle model prediction score: {loaded_pickle_model.score(X_test, y_test) * 100:.2f}%\") <pre>Loaded pickle model prediction score: 72.37%\n</pre> <p>For larger models, it may be more efficient to use Joblib.</p> In\u00a0[25]: Copied! <pre>from joblib import dump, load\n\n# Save a model using joblib\ndump(model, \"random_forest_model_1.joblib\")\n</pre> from joblib import dump, load  # Save a model using joblib dump(model, \"random_forest_model_1.joblib\") Out[25]: <pre>['random_forest_model_1.joblib']</pre> In\u00a0[26]: Copied! <pre># Load a saved joblib model and evaluate it\nloaded_joblib_model = load(\"random_forest_model_1.joblib\")\nprint(f\"Loaded joblib model prediction score: {loaded_joblib_model.score(X_test, y_test) * 100:.2f}%\")\n</pre> # Load a saved joblib model and evaluate it loaded_joblib_model = load(\"random_forest_model_1.joblib\") print(f\"Loaded joblib model prediction score: {loaded_joblib_model.score(X_test, y_test) * 100:.2f}%\") <pre>Loaded joblib model prediction score: 72.37%\n</pre> <p>Woah!</p> <p>We've covered a lot of ground fast...</p> <p>Let's break things down a bit more by revisting each section.</p> In\u00a0[27]: Copied! <pre># Splitting the data into X &amp; y\nheart_disease.head()\n</pre> # Splitting the data into X &amp; y heart_disease.head() Out[27]: age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target 0 63 1 3 145 233 1 0 150 0 2.3 0 0 1 1 1 37 1 2 130 250 0 1 187 0 3.5 0 0 2 1 2 41 0 1 130 204 0 0 172 0 1.4 2 0 2 1 3 56 1 1 120 236 0 1 178 0 0.8 2 0 2 1 4 57 0 0 120 354 0 1 163 1 0.6 2 0 2 1 In\u00a0[28]: Copied! <pre># Splitting the data into features (X) and labels (y)\nX = heart_disease.drop('target', axis=1)\nX\n</pre> # Splitting the data into features (X) and labels (y) X = heart_disease.drop('target', axis=1) X Out[28]: age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal 0 63 1 3 145 233 1 0 150 0 2.3 0 0 1 1 37 1 2 130 250 0 1 187 0 3.5 0 0 2 2 41 0 1 130 204 0 0 172 0 1.4 2 0 2 3 56 1 1 120 236 0 1 178 0 0.8 2 0 2 4 57 0 0 120 354 0 1 163 1 0.6 2 0 2 ... ... ... ... ... ... ... ... ... ... ... ... ... ... 298 57 0 0 140 241 0 1 123 1 0.2 1 0 3 299 45 1 3 110 264 0 1 132 0 1.2 1 0 3 300 68 1 0 144 193 1 1 141 0 3.4 1 2 3 301 57 1 0 130 131 0 1 115 1 1.2 1 1 3 302 57 0 1 130 236 0 0 174 0 0.0 1 1 2 <p>303 rows \u00d7 13 columns</p> <p>Nice! Looks like our dataset has 303 samples with 13 features (13 columns).</p> <p>Let's check out the labels.</p> In\u00a0[29]: Copied! <pre>y = heart_disease['target']\ny\n</pre> y = heart_disease['target'] y Out[29]: <pre>0      1\n1      1\n2      1\n3      1\n4      1\n      ..\n298    0\n299    0\n300    0\n301    0\n302    0\nName: target, Length: 303, dtype: int64</pre> <p>Beautiful, 303 labels with values of <code>0</code> (no heart disease) and <code>1</code> (heart disease).</p> <p>Now let's split our data into training and test sets, we'll use an 80/20 split (80% of samples for training and 20% of samples for testing).</p> In\u00a0[30]: Copied! <pre># Splitting the data into training and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.2) # you can change the test size\n\n# Check the shapes of different data splits\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n</pre> # Splitting the data into training and test sets from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X,                                                      y,                                                      test_size=0.2) # you can change the test size  # Check the shapes of different data splits X_train.shape, X_test.shape, y_train.shape, y_test.shape Out[30]: <pre>((242, 13), (61, 13), (242,), (61,))</pre> In\u00a0[31]: Copied! <pre># 80% of data is being used for the training set (the model will learn patterns on these samples)\nX.shape[0] * 0.8\n</pre> # 80% of data is being used for the training set (the model will learn patterns on these samples) X.shape[0] * 0.8 Out[31]: <pre>242.4</pre> In\u00a0[32]: Copied! <pre># And 20% of the data is being used for the testing set (the model will be evaluated on these samples)\nX.shape[0] * 0.2\n</pre> # And 20% of the data is being used for the testing set (the model will be evaluated on these samples) X.shape[0] * 0.2 Out[32]: <pre>60.6</pre> In\u00a0[33]: Copied! <pre># Import car-sales-extended.csv\ncar_sales = pd.read_csv(\"../data/car-sales-extended.csv\")\ncar_sales\n</pre> # Import car-sales-extended.csv car_sales = pd.read_csv(\"../data/car-sales-extended.csv\") car_sales Out[33]: Make Colour Odometer (KM) Doors Price 0 Honda White 35431 4 15323 1 BMW Blue 192714 5 19943 2 Honda White 84714 4 28343 3 Toyota White 154365 4 13434 4 Nissan Blue 181577 3 14043 ... ... ... ... ... ... 995 Toyota Black 35820 4 32042 996 Nissan White 155144 3 5716 997 Nissan Blue 66604 4 31570 998 Honda White 215883 4 4001 999 Toyota Blue 248360 4 12732 <p>1000 rows \u00d7 5 columns</p> <p>We can check the dataset types with <code>.dtypes</code>.</p> In\u00a0[34]: Copied! <pre>car_sales.dtypes\n</pre> car_sales.dtypes Out[34]: <pre>Make             object\nColour           object\nOdometer (KM)     int64\nDoors             int64\nPrice             int64\ndtype: object</pre> <p>Notice the <code>Make</code> and <code>Colour</code> features are of <code>dtype=object</code> (they're strings) where as the rest of the columns are of <code>dtype=int64</code>.</p> <p>If we want to use the <code>Make</code> and <code>Colour</code> features in our model, we'll need to figure out how to turn them into numerical form.</p> In\u00a0[35]: Copied! <pre># Split into X &amp; y and train/test\nX = car_sales.drop(\"Price\", axis=1)\ny = car_sales[\"Price\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n</pre> # Split into X &amp; y and train/test X = car_sales.drop(\"Price\", axis=1) y = car_sales[\"Price\"]  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) <p>Now let's try and build a model on our <code>car_sales</code> data.</p> In\u00a0[36]: Copied! <pre># Try to predict with random forest on price column (doesn't work)\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\n</pre> # Try to predict with random forest on price column (doesn't work) from sklearn.ensemble import RandomForestRegressor  model = RandomForestRegressor() model.fit(X_train, y_train) model.score(X_test, y_test) <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/var/folders/c4/qj4gdk190td18bqvjjh0p3p00000gn/T/ipykernel_30502/1044518071.py in ?()\n      1 # Try to predict with random forest on price column (doesn't work)\n      2 from sklearn.ensemble import RandomForestRegressor\n      3 \n      4 model = RandomForestRegressor()\n----&gt; 5 model.fit(X_train, y_train)\n      6 model.score(X_test, y_test)\n\n~/code/zero-to-mastery-ml/env/lib/python3.10/site-packages/sklearn/base.py in ?(estimator, *args, **kwargs)\n   1148                 skip_parameter_validation=(\n   1149                     prefer_skip_nested_validation or global_skip_validation\n   1150                 )\n   1151             ):\n-&gt; 1152                 return fit_method(estimator, *args, **kwargs)\n\n~/code/zero-to-mastery-ml/env/lib/python3.10/site-packages/sklearn/ensemble/_forest.py in ?(self, X, y, sample_weight)\n    344         \"\"\"\n    345         # Validate or convert input data\n    346         if issparse(y):\n    347             raise ValueError(\"sparse multilabel-indicator for y is not supported.\")\n--&gt; 348         X, y = self._validate_data(\n    349             X, y, multi_output=True, accept_sparse=\"csc\", dtype=DTYPE\n    350         )\n    351         if sample_weight is not None:\n\n~/code/zero-to-mastery-ml/env/lib/python3.10/site-packages/sklearn/base.py in ?(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\n    618                 if \"estimator\" not in check_y_params:\n    619                     check_y_params = {**default_check_params, **check_y_params}\n    620                 y = check_array(y, input_name=\"y\", **check_y_params)\n    621             else:\n--&gt; 622                 X, y = check_X_y(X, y, **check_params)\n    623             out = X, y\n    624 \n    625         if not no_val_X and check_params.get(\"ensure_2d\", True):\n\n~/code/zero-to-mastery-ml/env/lib/python3.10/site-packages/sklearn/utils/validation.py in ?(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\n   1142         raise ValueError(\n   1143             f\"{estimator_name} requires y to be passed, but the target y is None\"\n   1144         )\n   1145 \n-&gt; 1146     X = check_array(\n   1147         X,\n   1148         accept_sparse=accept_sparse,\n   1149         accept_large_sparse=accept_large_sparse,\n\n~/code/zero-to-mastery-ml/env/lib/python3.10/site-packages/sklearn/utils/validation.py in ?(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\n    912                         )\n    913                     array = xp.astype(array, dtype, copy=False)\n    914                 else:\n    915                     array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n--&gt; 916             except ComplexWarning as complex_warning:\n    917                 raise ValueError(\n    918                     \"Complex data not supported\\n{}\\n\".format(array)\n    919                 ) from complex_warning\n\n~/code/zero-to-mastery-ml/env/lib/python3.10/site-packages/sklearn/utils/_array_api.py in ?(array, dtype, order, copy, xp)\n    376         # Use NumPy API to support order\n    377         if copy is True:\n    378             array = numpy.array(array, order=order, dtype=dtype)\n    379         else:\n--&gt; 380             array = numpy.asarray(array, order=order, dtype=dtype)\n    381 \n    382         # At this point array is a NumPy ndarray. We convert it to an array\n    383         # container that is consistent with the input's namespace.\n\n~/code/zero-to-mastery-ml/env/lib/python3.10/site-packages/pandas/core/generic.py in ?(self, dtype)\n   2082     def __array__(self, dtype: npt.DTypeLike | None = None) -&gt; np.ndarray:\n   2083         values = self._values\n-&gt; 2084         arr = np.asarray(values, dtype=dtype)\n   2085         if (\n   2086             astype_is_view(values.dtype, arr.dtype)\n   2087             and using_copy_on_write()\n\nValueError: could not convert string to float: 'Honda'</pre> <p>Oops... this doesn't work, we'll have to convert the non-numerical features into numbers first.</p> <p>The process of turning categorical features into numbers is often referred to as encoding.</p> <p>Scikit-Learn has a fantastic in-depth guide on Encoding categorical features.</p> <p>But let's look at one of the most straightforward ways to turn categorical features into numbers, one-hot encoding.</p> <p>In machine learning, one-hot encoding gives a value of <code>1</code> to the target value and a value of <code>0</code> to the other values.</p> <p>For example, let's say we had five samples and three car make options, Honda, Toyota, BMW.</p> <p>And our samples were:</p> <ol> <li>Honda</li> <li>BMW</li> <li>BMW</li> <li>Toyota</li> <li>Toyota</li> </ol> <p>If we were to one-hot encode these, it would look like:</p> Sample Honda Toyota BMW 1 1 0 0 2 0 0 1 3 0 0 1 4 0 1 0 5 0 1 0 <p>Notice how there's a 1 for each target value but a 0 for each other value.</p> <p>We can use the following steps to one-hot encode our dataset:</p> <ol> <li>Import <code>sklearn.preprocessing.OneHotEncoder</code> to one-hot encode our features and <code>sklearn.compose.ColumnTransformer</code> to target the specific columns of our DataFrame to transform.</li> <li>Define the categorical features we'd like to transform.</li> <li>Create an instance of the <code>OneHotEncoder</code>.</li> <li>Create an instance of <code>ColumnTransformer</code> and feed it the transforms we'd like to make.</li> <li>Fit the instance of the <code>ColumnTransformer</code> to our data and transform it with the <code>fit_transform(X)</code> method.</li> </ol> <p>Note: In Scikit-Learn, the term \"transformer\" is often used to refer to something that transforms data.</p> In\u00a0[37]: Copied! <pre># 1. Import OneHotEncoder and ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n# 2. Define the categorical features to transform\ncategorical_features = [\"Make\", \"Colour\", \"Doors\"]\n\n# 3. Create an instance of OneHotEncoder\none_hot = OneHotEncoder()\n\n# 4. Create an instance of ColumnTransformer\ntransformer = ColumnTransformer([(\"one_hot\", # name\n                                  one_hot, # transformer\n                                  categorical_features)], # columns to transform\n                                  remainder=\"passthrough\") # what to do with the rest of the columns? (\"passthrough\" = leave unchanged) \n\n# 5. Turn the categorical features into numbers (this will return an array-like sparse matrix, not a DataFrame)\ntransformed_X = transformer.fit_transform(X)\ntransformed_X\n</pre> # 1. Import OneHotEncoder and ColumnTransformer from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer  # 2. Define the categorical features to transform categorical_features = [\"Make\", \"Colour\", \"Doors\"]  # 3. Create an instance of OneHotEncoder one_hot = OneHotEncoder()  # 4. Create an instance of ColumnTransformer transformer = ColumnTransformer([(\"one_hot\", # name                                   one_hot, # transformer                                   categorical_features)], # columns to transform                                   remainder=\"passthrough\") # what to do with the rest of the columns? (\"passthrough\" = leave unchanged)   # 5. Turn the categorical features into numbers (this will return an array-like sparse matrix, not a DataFrame) transformed_X = transformer.fit_transform(X) transformed_X Out[37]: <pre>array([[0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n        0.00000e+00, 3.54310e+04],\n       [1.00000e+00, 0.00000e+00, 0.00000e+00, ..., 0.00000e+00,\n        1.00000e+00, 1.92714e+05],\n       [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n        0.00000e+00, 8.47140e+04],\n       ...,\n       [0.00000e+00, 0.00000e+00, 1.00000e+00, ..., 1.00000e+00,\n        0.00000e+00, 6.66040e+04],\n       [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n        0.00000e+00, 2.15883e+05],\n       [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n        0.00000e+00, 2.48360e+05]])</pre> <p>Note: You might be thinking why we considered <code>Doors</code> as a categorical variable. Which is a good question considering <code>Doors</code> is already numerical. Well, the answer is that <code>Doors</code> could be either numerical or categorical. However, I've decided to go with categorical, since where I'm from, number of doors is often a different category of car. For example, you can shop for 4-door cars or shop for 5-door cars (which always confused me since where's the 5th door?). However, you could experiment with treating this value as numerical or categorical, training a model on each, and then see how each model performs.</p> <p>Woah! Looks like our samples are all numerical, what did our data look like previously?</p> In\u00a0[38]: Copied! <pre>X.head()\n</pre> X.head() Out[38]: Make Colour Odometer (KM) Doors 0 Honda White 35431 4 1 BMW Blue 192714 5 2 Honda White 84714 4 3 Toyota White 154365 4 4 Nissan Blue 181577 3 <p>It seems <code>OneHotEncoder</code> and <code>ColumnTransformer</code> have turned all of our data samples into numbers.</p> <p>Let's check out the first transformed sample.</p> In\u00a0[39]: Copied! <pre># View first transformed sample\ntransformed_X[0]\n</pre> # View first transformed sample transformed_X[0] Out[39]: <pre>array([0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n       0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n       1.0000e+00, 0.0000e+00, 3.5431e+04])</pre> <p>And what were these values originally?</p> In\u00a0[40]: Copied! <pre># View original first sample\nX.iloc[0]\n</pre> # View original first sample X.iloc[0] Out[40]: <pre>Make             Honda\nColour           White\nOdometer (KM)    35431\nDoors                4\nName: 0, dtype: object</pre> In\u00a0[41]: Copied! <pre># View head of original DataFrame\ncar_sales.head()\n</pre> # View head of original DataFrame car_sales.head() Out[41]: Make Colour Odometer (KM) Doors Price 0 Honda White 35431 4 15323 1 BMW Blue 192714 5 19943 2 Honda White 84714 4 28343 3 Toyota White 154365 4 13434 4 Nissan Blue 181577 3 14043 <p>Wonderful, now let's use <code>pd.get_dummies()</code> to turn our categorical variables into one-hot encoded variables.</p> In\u00a0[42]: Copied! <pre># One-hot encode categorical variables\ncategorical_variables = [\"Make\", \"Colour\", \"Doors\"]\ndummies = pd.get_dummies(data=car_sales[categorical_variables])\ndummies\n</pre> # One-hot encode categorical variables categorical_variables = [\"Make\", \"Colour\", \"Doors\"] dummies = pd.get_dummies(data=car_sales[categorical_variables]) dummies Out[42]: Doors Make_BMW Make_Honda Make_Nissan Make_Toyota Colour_Black Colour_Blue Colour_Green Colour_Red Colour_White 0 4 False True False False False False False False True 1 5 True False False False False True False False False 2 4 False True False False False False False False True 3 4 False False False True False False False False True 4 3 False False True False False True False False False ... ... ... ... ... ... ... ... ... ... ... 995 4 False False False True True False False False False 996 3 False False True False False False False False True 997 4 False False True False False True False False False 998 4 False True False False False False False False True 999 4 False False False True False True False False False <p>1000 rows \u00d7 10 columns</p> <p>Nice!</p> <p>Notice how there's a new column for each categorical option (e.g. <code>Make_BMW</code>, <code>Make_Honda</code>, etc).</p> <p>But also notice how it also missed the <code>Doors</code> column?</p> <p>This is because <code>Doors</code> is already numeric, so for <code>pd.get_dummies()</code> to work on it, we can change it to type <code>object</code>.</p> <p>By default, <code>pd.get_dummies()</code> also turns all of the values to bools (<code>True</code> or <code>False</code>).</p> <p>We can get the returned values as <code>0</code> or <code>1</code> by setting <code>dtype=float</code>.</p> In\u00a0[43]: Copied! <pre># Have to convert doors to object for dummies to work on it...\ncar_sales[\"Doors\"] = car_sales[\"Doors\"].astype(object)\ndummies = pd.get_dummies(data=car_sales[[\"Make\", \"Colour\", \"Doors\"]],\n                         dtype=float)\ndummies\n</pre> # Have to convert doors to object for dummies to work on it... car_sales[\"Doors\"] = car_sales[\"Doors\"].astype(object) dummies = pd.get_dummies(data=car_sales[[\"Make\", \"Colour\", \"Doors\"]],                          dtype=float) dummies Out[43]: Make_BMW Make_Honda Make_Nissan Make_Toyota Colour_Black Colour_Blue Colour_Green Colour_Red Colour_White Doors_3 Doors_4 Doors_5 0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 2 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 3 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 4 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 ... ... ... ... ... ... ... ... ... ... ... ... ... 995 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 996 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 997 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 998 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 999 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 <p>1000 rows \u00d7 12 columns</p> <p>Woohoo!</p> <p>We've now turned our data into fully numeric form using Scikit-Learn and pandas.</p> <p>Now you might be wondering...</p> <p>Should you use Scikit-Learn or pandas for turning data into numerical form?</p> <p>And the answer is either.</p> <p>But as a rule of thumb:</p> <ul> <li>If you're performing quick data analysis and running small modelling experiments, use <code>pandas</code> as it's generally quite fast to get up and running.</li> <li>If you're performing a larger scale modelling experiment or would like to put your data processing steps into a production pipeline, I'd recommend leaning towards Scikit-Learn, specifically a Scikit-Learn Pipeline (chaining together multiple estimator/modelling steps).</li> </ul> <p>Since we've turned our data into numerical form, how about we try and fit our model again?</p> <p>Let's recreate a train/test split except this time we'll use <code>transformed_X</code> instead of <code>X</code>.</p> In\u00a0[44]: Copied! <pre>np.random.seed(42)\n\n# Create train and test splits with transformed_X\nX_train, X_test, y_train, y_test = train_test_split(transformed_X,\n                                                    y,\n                                                    test_size=0.2)\n\n# Create the model instance\nmodel = RandomForestRegressor()\n\n# Fit the model on the numerical data (this errored before since our data wasn't fully numeric)\nmodel.fit(X_train, y_train)\n\n# Score the model (returns r^2 metric by default, also called coefficient of determination, higher is better)\nmodel.score(X_test, y_test)\n</pre> np.random.seed(42)  # Create train and test splits with transformed_X X_train, X_test, y_train, y_test = train_test_split(transformed_X,                                                     y,                                                     test_size=0.2)  # Create the model instance model = RandomForestRegressor()  # Fit the model on the numerical data (this errored before since our data wasn't fully numeric) model.fit(X_train, y_train)  # Score the model (returns r^2 metric by default, also called coefficient of determination, higher is better) model.score(X_test, y_test) Out[44]: <pre>0.3235867221569877</pre> In\u00a0[45]: Copied! <pre># Import car sales dataframe with missing values\ncar_sales_missing = pd.read_csv(\"../data/car-sales-extended-missing-data.csv\")\ncar_sales_missing\n</pre> # Import car sales dataframe with missing values car_sales_missing = pd.read_csv(\"../data/car-sales-extended-missing-data.csv\") car_sales_missing Out[45]: Make Colour Odometer (KM) Doors Price 0 Honda White 35431.0 4.0 15323.0 1 BMW Blue 192714.0 5.0 19943.0 2 Honda White 84714.0 4.0 28343.0 3 Toyota White 154365.0 4.0 13434.0 4 Nissan Blue 181577.0 3.0 14043.0 ... ... ... ... ... ... 995 Toyota Black 35820.0 4.0 32042.0 996 NaN White 155144.0 3.0 5716.0 997 Nissan Blue 66604.0 4.0 31570.0 998 Honda White 215883.0 4.0 4001.0 999 Toyota Blue 248360.0 4.0 12732.0 <p>1000 rows \u00d7 5 columns</p> <p>If you're dataset is large, it's likely you aren't going to go through it sample by sample to find the missing values.</p> <p>Luckily, pandas has a method called <code>pd.DataFrame.isna()</code> which is able to detect missing values.</p> <p>Let's try it on our DataFrame.</p> In\u00a0[46]: Copied! <pre># Get the sum of all missing values\ncar_sales_missing.isna().sum()\n</pre> # Get the sum of all missing values car_sales_missing.isna().sum() Out[46]: <pre>Make             49\nColour           50\nOdometer (KM)    50\nDoors            50\nPrice            50\ndtype: int64</pre> <p>Hmm... seems there's about 50 or so missing values per column.</p> <p>How about we try and split the data into features and labels, then convert the categorical data to numbers, then split the data into training and test and then try and fit a model on it (just like we did before)?</p> In\u00a0[47]: Copied! <pre># Create features\nX_missing = car_sales_missing.drop(\"Price\", axis=1)\nprint(f\"Number of missing X values:\\n{X_missing.isna().sum()}\")\n</pre> # Create features X_missing = car_sales_missing.drop(\"Price\", axis=1) print(f\"Number of missing X values:\\n{X_missing.isna().sum()}\") <pre>Number of missing X values:\nMake             49\nColour           50\nOdometer (KM)    50\nDoors            50\ndtype: int64\n</pre> In\u00a0[48]: Copied! <pre># Create labels\ny_missing = car_sales_missing[\"Price\"]\nprint(f\"Number of missing y values: {y_missing.isna().sum()}\")\n</pre> # Create labels y_missing = car_sales_missing[\"Price\"] print(f\"Number of missing y values: {y_missing.isna().sum()}\") <pre>Number of missing y values: 50\n</pre> <p>Now we can convert the categorical columns into one-hot encodings (just as before).</p> In\u00a0[49]: Copied! <pre># Let's convert the categorical columns to one hot encoded (code copied from above)\n# Turn the categories (Make and Colour) into numbers\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\ncategorical_features = [\"Make\", \"Colour\", \"Doors\"]\n\none_hot = OneHotEncoder()\n\ntransformer = ColumnTransformer([(\"one_hot\", \n                                  one_hot, \n                                  categorical_features)],\n                                remainder=\"passthrough\",\n                                sparse_threshold=0) # return a sparse matrix or not\n\ntransformed_X_missing = transformer.fit_transform(X_missing)\ntransformed_X_missing\n</pre> # Let's convert the categorical columns to one hot encoded (code copied from above) # Turn the categories (Make and Colour) into numbers from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer  categorical_features = [\"Make\", \"Colour\", \"Doors\"]  one_hot = OneHotEncoder()  transformer = ColumnTransformer([(\"one_hot\",                                    one_hot,                                    categorical_features)],                                 remainder=\"passthrough\",                                 sparse_threshold=0) # return a sparse matrix or not  transformed_X_missing = transformer.fit_transform(X_missing) transformed_X_missing Out[49]: <pre>array([[0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 0.00000e+00,\n        0.00000e+00, 3.54310e+04],\n       [1.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n        0.00000e+00, 1.92714e+05],\n       [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 0.00000e+00,\n        0.00000e+00, 8.47140e+04],\n       ...,\n       [0.00000e+00, 0.00000e+00, 1.00000e+00, ..., 0.00000e+00,\n        0.00000e+00, 6.66040e+04],\n       [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 0.00000e+00,\n        0.00000e+00, 2.15883e+05],\n       [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 0.00000e+00,\n        0.00000e+00, 2.48360e+05]])</pre> <p>Finally, let's split the missing data samples into train and test sets and then try to fit and score a model on them.</p> In\u00a0[50]: Copied! <pre># Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(transformed_X_missing,\n                                                    y_missing,\n                                                    test_size=0.2)\n\n# Fit and score a model\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\n</pre> # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(transformed_X_missing,                                                     y_missing,                                                     test_size=0.2)  # Fit and score a model model = RandomForestRegressor() model.fit(X_train, y_train) model.score(X_test, y_test) <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/Users/daniel/code/zero-to-mastery-ml/section-2-data-science-and-ml-tools/introduction-to-scikit-learn.ipynb Cell 96 line 8\n      &lt;a href='vscode-notebook-cell:/Users/daniel/code/zero-to-mastery-ml/section-2-data-science-and-ml-tools/introduction-to-scikit-learn.ipynb#Y164sZmlsZQ%3D%3D?line=5'&gt;6&lt;/a&gt; # Fit and score a model\n      &lt;a href='vscode-notebook-cell:/Users/daniel/code/zero-to-mastery-ml/section-2-data-science-and-ml-tools/introduction-to-scikit-learn.ipynb#Y164sZmlsZQ%3D%3D?line=6'&gt;7&lt;/a&gt; model = RandomForestRegressor()\n----&gt; &lt;a href='vscode-notebook-cell:/Users/daniel/code/zero-to-mastery-ml/section-2-data-science-and-ml-tools/introduction-to-scikit-learn.ipynb#Y164sZmlsZQ%3D%3D?line=7'&gt;8&lt;/a&gt; model.fit(X_train, y_train)\n      &lt;a href='vscode-notebook-cell:/Users/daniel/code/zero-to-mastery-ml/section-2-data-science-and-ml-tools/introduction-to-scikit-learn.ipynb#Y164sZmlsZQ%3D%3D?line=8'&gt;9&lt;/a&gt; model.score(X_test, y_test)\n\nFile ~/code/zero-to-mastery-ml/env/lib/python3.10/site-packages/sklearn/base.py:1152, in _fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(estimator, *args, **kwargs)\n   1145     estimator._validate_params()\n   1147 with config_context(\n   1148     skip_parameter_validation=(\n   1149         prefer_skip_nested_validation or global_skip_validation\n   1150     )\n   1151 ):\n-&gt; 1152     return fit_method(estimator, *args, **kwargs)\n\nFile ~/code/zero-to-mastery-ml/env/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:348, in BaseForest.fit(self, X, y, sample_weight)\n    346 if issparse(y):\n    347     raise ValueError(\"sparse multilabel-indicator for y is not supported.\")\n--&gt; 348 X, y = self._validate_data(\n    349     X, y, multi_output=True, accept_sparse=\"csc\", dtype=DTYPE\n    350 )\n    351 if sample_weight is not None:\n    352     sample_weight = _check_sample_weight(sample_weight, X)\n\nFile ~/code/zero-to-mastery-ml/env/lib/python3.10/site-packages/sklearn/base.py:622, in BaseEstimator._validate_data(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\n    620         y = check_array(y, input_name=\"y\", **check_y_params)\n    621     else:\n--&gt; 622         X, y = check_X_y(X, y, **check_params)\n    623     out = X, y\n    625 if not no_val_X and check_params.get(\"ensure_2d\", True):\n\nFile ~/code/zero-to-mastery-ml/env/lib/python3.10/site-packages/sklearn/utils/validation.py:1146, in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\n   1141         estimator_name = _check_estimator_name(estimator)\n   1142     raise ValueError(\n   1143         f\"{estimator_name} requires y to be passed, but the target y is None\"\n   1144     )\n-&gt; 1146 X = check_array(\n   1147     X,\n   1148     accept_sparse=accept_sparse,\n   1149     accept_large_sparse=accept_large_sparse,\n   1150     dtype=dtype,\n   1151     order=order,\n   1152     copy=copy,\n   1153     force_all_finite=force_all_finite,\n   1154     ensure_2d=ensure_2d,\n   1155     allow_nd=allow_nd,\n   1156     ensure_min_samples=ensure_min_samples,\n   1157     ensure_min_features=ensure_min_features,\n   1158     estimator=estimator,\n   1159     input_name=\"X\",\n   1160 )\n   1162 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n   1164 check_consistent_length(X, y)\n\nFile ~/code/zero-to-mastery-ml/env/lib/python3.10/site-packages/sklearn/utils/validation.py:957, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\n    951         raise ValueError(\n    952             \"Found array with dim %d. %s expected &lt;= 2.\"\n    953             % (array.ndim, estimator_name)\n    954         )\n    956     if force_all_finite:\n--&gt; 957         _assert_all_finite(\n    958             array,\n    959             input_name=input_name,\n    960             estimator_name=estimator_name,\n    961             allow_nan=force_all_finite == \"allow-nan\",\n    962         )\n    964 if ensure_min_samples &gt; 0:\n    965     n_samples = _num_samples(array)\n\nFile ~/code/zero-to-mastery-ml/env/lib/python3.10/site-packages/sklearn/utils/validation.py:122, in _assert_all_finite(X, allow_nan, msg_dtype, estimator_name, input_name)\n    119 if first_pass_isfinite:\n    120     return\n--&gt; 122 _assert_all_finite_element_wise(\n    123     X,\n    124     xp=xp,\n    125     allow_nan=allow_nan,\n    126     msg_dtype=msg_dtype,\n    127     estimator_name=estimator_name,\n    128     input_name=input_name,\n    129 )\n\nFile ~/code/zero-to-mastery-ml/env/lib/python3.10/site-packages/sklearn/utils/validation.py:171, in _assert_all_finite_element_wise(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\n    154 if estimator_name and input_name == \"X\" and has_nan_error:\n    155     # Improve the error message on how to handle missing values in\n    156     # scikit-learn.\n    157     msg_err += (\n    158         f\"\\n{estimator_name} does not accept missing values\"\n    159         \" encoded as NaN natively. For supervised learning, you might want\"\n   (...)\n    169         \"#estimators-that-handle-nan-values\"\n    170     )\n--&gt; 171 raise ValueError(msg_err)\n\nValueError: Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values</pre> <p>Ahh... dam! Looks like the model we're trying to use doesn't work with missing values.</p> <p>When we try to fit it on a dataset with missing samples, Scikit-Learn produces the error:</p> <p><code>ValueError: Input X contains NaN. RandomForestRegressor does not accept missing values encoded as NaN natively...</code></p> <p>Looks like if we want to use <code>RandomForestRegressor</code>, we'll have to either fill or remove the missing values.</p> Note: Scikit-Learn does have a          list of models which can handle NaNs or missing values directly.      <p>Such as,          <code>sklearn.ensemble.HistGradientBoostingClassifier</code>          or          <code>sklearn.ensemble.HistGradientBoostingRegressor</code>.     </p> <p>As an experiment, you may want to try the following:</p> <pre><code>\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\n<p># Try a model that can handle NaNs natively\nnan_model = HistGradientBoostingRegressor()\nnan_model.fit(X_train, y_train)\nnan_model.score(X_test, y_test)\n</p></code></pre>  Let's see what values are missing again.    In\u00a0[51]: Copied! <pre>car_sales_missing.isna().sum()\n</pre> car_sales_missing.isna().sum() Out[51]: <pre>Make             49\nColour           50\nOdometer (KM)    50\nDoors            50\nPrice            50\ndtype: int64</pre> <p>How can fill (impute) or remove these?</p> In\u00a0[52]: Copied! <pre># Fill the missing values in the Make column\ncar_sales_missing[\"Make\"].fillna(value=\"missing\", inplace=True)\n</pre> # Fill the missing values in the Make column car_sales_missing[\"Make\"].fillna(value=\"missing\", inplace=True) <p>And we can do the same with the <code>Colour</code> column.</p> In\u00a0[53]: Copied! <pre># Fill the Colour column\ncar_sales_missing[\"Colour\"].fillna(value=\"missing\", inplace=True)\n</pre> # Fill the Colour column car_sales_missing[\"Colour\"].fillna(value=\"missing\", inplace=True) <p>How many missing values do we have now?</p> In\u00a0[54]: Copied! <pre>car_sales_missing.isna().sum()\n</pre> car_sales_missing.isna().sum() Out[54]: <pre>Make              0\nColour            0\nOdometer (KM)    50\nDoors            50\nPrice            50\ndtype: int64</pre> <p>Wonderful! We're making some progress.</p> <p>Now let's fill the <code>Doors</code> column with <code>4</code> (the most common value), this is the same as filling it with the median or mode of the <code>Doors</code> column.</p> In\u00a0[55]: Copied! <pre># Find the most common value of the Doors column\ncar_sales_missing[\"Doors\"].value_counts()\n</pre> # Find the most common value of the Doors column car_sales_missing[\"Doors\"].value_counts() Out[55]: <pre>Doors\n4.0    811\n5.0     75\n3.0     64\nName: count, dtype: int64</pre> In\u00a0[56]: Copied! <pre># Fill the Doors column with the most common value\ncar_sales_missing[\"Doors\"].fillna(value=4, inplace=True)\n</pre> # Fill the Doors column with the most common value car_sales_missing[\"Doors\"].fillna(value=4, inplace=True) <p>Next, we'll fill the <code>Odometer (KM)</code> column with the mean value of itself.</p> In\u00a0[57]: Copied! <pre># Fill the Odometer (KM) column\ncar_sales_missing[\"Odometer (KM)\"].fillna(value=car_sales_missing[\"Odometer (KM)\"].mean(), inplace=True)\n</pre> # Fill the Odometer (KM) column car_sales_missing[\"Odometer (KM)\"].fillna(value=car_sales_missing[\"Odometer (KM)\"].mean(), inplace=True) <p>How many missing values do we have now?</p> In\u00a0[58]: Copied! <pre># Check the number of missing values\ncar_sales_missing.isna().sum()\n</pre> # Check the number of missing values car_sales_missing.isna().sum() Out[58]: <pre>Make              0\nColour            0\nOdometer (KM)     0\nDoors             0\nPrice            50\ndtype: int64</pre> <p>Woohoo! That's looking a lot better.</p> <p>Finally, we can remove the rows which are missing the target value <code>Price</code>.</p> <p>Note: Another option would be to impute the <code>Price</code> value with the mean or median or some other calculated value (such as by using similar cars to estimate the price), however, to keep things simple and prevent introducing too many fake labels to the data, we'll remove the samples missing a <code>Price</code> value.</p> In\u00a0[59]: Copied! <pre># Remove rows with missing Price labels\ncar_sales_missing.dropna(inplace=True)\n</pre> # Remove rows with missing Price labels car_sales_missing.dropna(inplace=True) <p>That should be no more missing values!</p> In\u00a0[60]: Copied! <pre># Check the number of missing values\ncar_sales_missing.isna().sum()\n</pre> # Check the number of missing values car_sales_missing.isna().sum() Out[60]: <pre>Make             0\nColour           0\nOdometer (KM)    0\nDoors            0\nPrice            0\ndtype: int64</pre> <p>Since we removed samples missing a <code>Price</code> value, there's now less overall samples but none of them have missing values.</p> In\u00a0[61]: Copied! <pre># Check the number of total samples (previously was 1000)\nlen(car_sales_missing)\n</pre> # Check the number of total samples (previously was 1000) len(car_sales_missing) Out[61]: <pre>950</pre> <p>Can we fit a model now?</p> <p>Let's try!</p> <p>First we'll create the features and labels.</p> <p>Then we'll convert categorical variables into numbers via one-hot encoding.</p> <p>Then we'll split the data into training and test sets just like before.</p> <p>Finally, we'll try to fit a <code>RandomForestRegressor()</code> model to the newly filled data.</p> In\u00a0[62]: Copied! <pre># Create features\nX_missing = car_sales_missing.drop(\"Price\", axis=1)\nprint(f\"Number of missing X values:\\n{X_missing.isna().sum()}\")\n\n# Create labels\ny_missing = car_sales_missing[\"Price\"]\nprint(f\"Number of missing y values: {y_missing.isna().sum()}\")\n</pre> # Create features X_missing = car_sales_missing.drop(\"Price\", axis=1) print(f\"Number of missing X values:\\n{X_missing.isna().sum()}\")  # Create labels y_missing = car_sales_missing[\"Price\"] print(f\"Number of missing y values: {y_missing.isna().sum()}\") <pre>Number of missing X values:\nMake             0\nColour           0\nOdometer (KM)    0\nDoors            0\ndtype: int64\nNumber of missing y values: 0\n</pre> In\u00a0[63]: Copied! <pre>from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\ncategorical_features = [\"Make\", \"Colour\", \"Doors\"]\n\none_hot = OneHotEncoder()\n\ntransformer = ColumnTransformer([(\"one_hot\", \n                                  one_hot, \n                                  categorical_features)],\n                                remainder=\"passthrough\",\n                                sparse_threshold=0) # return a sparse matrix or not\n\ntransformed_X_missing = transformer.fit_transform(X_missing)\ntransformed_X_missing\n</pre> from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer  categorical_features = [\"Make\", \"Colour\", \"Doors\"]  one_hot = OneHotEncoder()  transformer = ColumnTransformer([(\"one_hot\",                                    one_hot,                                    categorical_features)],                                 remainder=\"passthrough\",                                 sparse_threshold=0) # return a sparse matrix or not  transformed_X_missing = transformer.fit_transform(X_missing) transformed_X_missing Out[63]: <pre>array([[0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n        0.00000e+00, 3.54310e+04],\n       [1.00000e+00, 0.00000e+00, 0.00000e+00, ..., 0.00000e+00,\n        1.00000e+00, 1.92714e+05],\n       [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n        0.00000e+00, 8.47140e+04],\n       ...,\n       [0.00000e+00, 0.00000e+00, 1.00000e+00, ..., 1.00000e+00,\n        0.00000e+00, 6.66040e+04],\n       [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n        0.00000e+00, 2.15883e+05],\n       [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n        0.00000e+00, 2.48360e+05]])</pre> In\u00a0[64]: Copied! <pre># Split data into training and test sets\nnp.random.seed(42)\nX_train, X_test, y_train, y_test = train_test_split(transformed_X_missing,\n                                                    y_missing,\n                                                    test_size=0.2)\n\n# Fit and score a model\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\n</pre> # Split data into training and test sets np.random.seed(42) X_train, X_test, y_train, y_test = train_test_split(transformed_X_missing,                                                     y_missing,                                                     test_size=0.2)  # Fit and score a model model = RandomForestRegressor() model.fit(X_train, y_train) model.score(X_test, y_test) Out[64]: <pre>0.22011714008302485</pre> <p>Fantastic!!!</p> <p>Looks like filling the missing values with pandas worked!</p> <p>Our model can be fit to the data without issues.</p> In\u00a0[65]: Copied! <pre>car_sales_missing.isna().sum()\n</pre> car_sales_missing.isna().sum() Out[65]: <pre>Make             0\nColour           0\nOdometer (KM)    0\nDoors            0\nPrice            0\ndtype: int64</pre> <p>Let's reimport it so it has missing values and we can fill them with Scikit-Learn.</p> In\u00a0[66]: Copied! <pre># Reimport the DataFrame\ncar_sales_missing = pd.read_csv(\"../data/car-sales-extended-missing-data.csv\")\ncar_sales_missing.isna().sum()\n</pre> # Reimport the DataFrame car_sales_missing = pd.read_csv(\"../data/car-sales-extended-missing-data.csv\") car_sales_missing.isna().sum() Out[66]: <pre>Make             49\nColour           50\nOdometer (KM)    50\nDoors            50\nPrice            50\ndtype: int64</pre> <p>To begin, we'll remove the rows which are missing a <code>Price</code> value.</p> In\u00a0[67]: Copied! <pre># Drop the rows with missing in the Price column\ncar_sales_missing.dropna(subset=[\"Price\"], inplace=True)\n</pre> # Drop the rows with missing in the Price column car_sales_missing.dropna(subset=[\"Price\"], inplace=True) <p>Now there are no rows missing a <code>Price</code> value.</p> In\u00a0[68]: Copied! <pre>car_sales_missing.isna().sum()\n</pre> car_sales_missing.isna().sum() Out[68]: <pre>Make             47\nColour           46\nOdometer (KM)    48\nDoors            47\nPrice             0\ndtype: int64</pre> <p>Since we don't have to fill any <code>Price</code> values, let's split our data into features (<code>X</code>) and labels (<code>y</code>).</p> <p>We'll also split the data into training and test sets.</p> In\u00a0[69]: Copied! <pre># Split into X and y\nX = car_sales_missing.drop(\"Price\", axis=1)\ny = car_sales_missing[\"Price\"]\n\n# Split data into train and test\nnp.random.seed(42)\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.2)\n</pre> # Split into X and y X = car_sales_missing.drop(\"Price\", axis=1) y = car_sales_missing[\"Price\"]  # Split data into train and test np.random.seed(42) X_train, X_test, y_train, y_test = train_test_split(X,                                                     y,                                                     test_size=0.2) <p>Note: We've split the data into train &amp; test sets here first to perform filling missing values on them separately. This is best practice as the test set is supposed to emulate data the model has never seen before. For categorical variables, it's generally okay to fill values across the whole dataset. However, for numerical vairables, you should only fill values on the test set that have been computed from the training set.</p> <p>Training and test sets created!</p> <p>Let's now setup a few instances of <code>SimpleImputer()</code> to fill various missing values.</p> <p>We'll use the following strategies and fill values:</p> <ul> <li>For categorical columns (<code>Make</code>, <code>Colour</code>), <code>strategy=\"constant\"</code>, <code>fill_value=\"missing\"</code> (fill the missing samples with a consistent value of <code>\"missing\"</code>.</li> <li>For the <code>Door</code> column, <code>strategy=\"constant\"</code>, <code>fill_value=4</code> (fill the missing samples with a consistent value of <code>4</code>).</li> <li>For the numerical column (<code>Odometer (KM)</code>), <code>strategy=\"mean\"</code> (fill the missing samples with the mean of the target column).<ul> <li>Note: There are more <code>strategy</code> and fill options in the <code>SimpleImputer()</code> documentation.</li> </ul> </li> </ul> In\u00a0[70]: Copied! <pre>from sklearn.impute import SimpleImputer\n\n# Create categorical variable imputer\ncat_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"missing\")\n\n# Create Door column imputer\ndoor_imputer = SimpleImputer(strategy=\"constant\", fill_value=4)\n\n# Create Odometer (KM) column imputer\nnum_imputer = SimpleImputer(strategy=\"mean\")\n</pre> from sklearn.impute import SimpleImputer  # Create categorical variable imputer cat_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"missing\")  # Create Door column imputer door_imputer = SimpleImputer(strategy=\"constant\", fill_value=4)  # Create Odometer (KM) column imputer num_imputer = SimpleImputer(strategy=\"mean\") <p>Imputers created!</p> <p>Now let's define which columns we'd like to impute on.</p> <p>Why?</p> <p>Because we'll need these shortly (I'll explain in the next text cell).</p> In\u00a0[71]: Copied! <pre># Define different column features\ncategorical_features = [\"Make\", \"Colour\"]\ndoor_feature = [\"Doors\"]\nnumerical_feature = [\"Odometer (KM)\"]\n</pre> # Define different column features categorical_features = [\"Make\", \"Colour\"] door_feature = [\"Doors\"] numerical_feature = [\"Odometer (KM)\"] <p>Columns defined!</p> <p>Now how might we transform our columns?</p> <p>Hint: we can use the <code>sklearn.compose.ColumnTransformer</code> class from Scikit-Learn, in a similar way to what we did before to get our data to all numeric values.</p> <p>That's the reason we defined the columns we'd like to transform.</p> <p>So we can use the <code>ColumnTransformer()</code> class.</p> <p><code>ColumnTransformer()</code> takes as input a list of tuples in the form <code>(name_of_transform, transformer_to_use, columns_to_transform)</code> specifying which columns to transform and how to transform them.</p> <p>For example:</p> <pre>imputer = ColumnTransformer([\n    (\"cat_imputer\", cat_imputer, categorical_features)\n])\n</pre> <p>In this case, the variables in the tuple are:</p> <ul> <li><code>name_of_transform</code> = <code>\"cat_imputer\"</code></li> <li><code>transformer_to_use</code> = <code>cat_imputer</code> (the instance of <code>SimpleImputer()</code> we defined above)</li> <li><code>columns_to_transform</code> = <code>categorical_features</code> (the list of categorical features we defined above).</li> </ul> <p>Let's exapnd upon this by extending the example.</p> In\u00a0[72]: Copied! <pre>from sklearn.compose import ColumnTransformer\n\n# Create series of column transforms to perform\nimputer = ColumnTransformer([\n    (\"cat_imputer\", cat_imputer, categorical_features),\n    (\"door_imputer\", door_imputer, door_feature),\n    (\"num_imputer\", num_imputer, numerical_feature)])\n</pre> from sklearn.compose import ColumnTransformer  # Create series of column transforms to perform imputer = ColumnTransformer([     (\"cat_imputer\", cat_imputer, categorical_features),     (\"door_imputer\", door_imputer, door_feature),     (\"num_imputer\", num_imputer, numerical_feature)]) <p>Nice!</p> <p>The next step is to fit our <code>ColumnTransformer()</code> instance (<code>imputer</code>) to the training data and transform the testing data.</p> <p>In other words we want to:</p> <ol> <li>Learn the imputation values from the training set.</li> <li>Fill the missing values in the training set with the values learned in 1.</li> <li>Fill the missing values in the testing set with the values learned in 1.</li> </ol> <p>Why this way?</p> <p>In our case, we're not calculating many variables (except the mean of the <code>Odometer (KM)</code> column), however, remember that the test set should always remain as unseen data.</p> <p>So when filling values in the test set, they should only be with values calculated or imputed from the training sets.</p> <p>We can achieve steps 1 &amp; 2 simultaneously with the <code>ColumnTransformer.fit_transform()</code> method (<code>fit</code> = find the values to fill, <code>transform</code> = fill them).</p> <p>And then we can perform step 3 with the <code>ColumnTransformer.transform()</code> method (we only want to transform the test set, not learn different values to fill).</p> In\u00a0[73]: Copied! <pre># Find values to fill and transform training data\nfilled_X_train = imputer.fit_transform(X_train)\n\n# Fill values in to the test set with values learned from the training set\nfilled_X_test = imputer.transform(X_test)\n\n# Check filled X_train\nfilled_X_train\n</pre> # Find values to fill and transform training data filled_X_train = imputer.fit_transform(X_train)  # Fill values in to the test set with values learned from the training set filled_X_test = imputer.transform(X_test)  # Check filled X_train filled_X_train Out[73]: <pre>array([['Honda', 'White', 4.0, 71934.0],\n       ['Toyota', 'Red', 4.0, 162665.0],\n       ['Honda', 'White', 4.0, 42844.0],\n       ...,\n       ['Toyota', 'White', 4.0, 196225.0],\n       ['Honda', 'Blue', 4.0, 133117.0],\n       ['Honda', 'missing', 4.0, 150582.0]], dtype=object)</pre> <p>Wonderful!</p> <p>Let's now turn our <code>filled_X_train</code> and <code>filled_X_test</code> arrays into DataFrames to inspect their missing values.</p> In\u00a0[74]: Copied! <pre># Get our transformed data array's back into DataFrame's\nfilled_X_train_df = pd.DataFrame(filled_X_train, \n                                 columns=[\"Make\", \"Colour\", \"Doors\", \"Odometer (KM)\"])\n\nfilled_X_test_df = pd.DataFrame(filled_X_test, \n                                columns=[\"Make\", \"Colour\", \"Doors\", \"Odometer (KM)\"])\n\n# Check missing data in training set\nfilled_X_train_df.isna().sum()\n</pre> # Get our transformed data array's back into DataFrame's filled_X_train_df = pd.DataFrame(filled_X_train,                                   columns=[\"Make\", \"Colour\", \"Doors\", \"Odometer (KM)\"])  filled_X_test_df = pd.DataFrame(filled_X_test,                                  columns=[\"Make\", \"Colour\", \"Doors\", \"Odometer (KM)\"])  # Check missing data in training set filled_X_train_df.isna().sum() Out[74]: <pre>Make             0\nColour           0\nDoors            0\nOdometer (KM)    0\ndtype: int64</pre> <p>And is there any missing data in the test set?</p> In\u00a0[75]: Copied! <pre># Check missing data in the testing set\nfilled_X_test_df.isna().sum()\n</pre> # Check missing data in the testing set filled_X_test_df.isna().sum() Out[75]: <pre>Make             0\nColour           0\nDoors            0\nOdometer (KM)    0\ndtype: int64</pre> <p>What about the original?</p> In\u00a0[76]: Copied! <pre># Check to see the original... still missing values\ncar_sales_missing.isna().sum()\n</pre> # Check to see the original... still missing values car_sales_missing.isna().sum() Out[76]: <pre>Make             47\nColour           46\nOdometer (KM)    48\nDoors            47\nPrice             0\ndtype: int64</pre> <p>Perfect!</p> <p>No more missing values!</p> <p>But wait...</p> <p>Is our data all numerical?</p> In\u00a0[77]: Copied! <pre>filled_X_train_df.head()\n</pre> filled_X_train_df.head() Out[77]: Make Colour Doors Odometer (KM) 0 Honda White 4.0 71934.0 1 Toyota Red 4.0 162665.0 2 Honda White 4.0 42844.0 3 Honda White 4.0 195829.0 4 Honda Blue 4.0 219217.0 <p>Ahh... looks like our <code>Make</code> and <code>Colour</code> columns are still strings.</p> <p>Let's one-hot encode them along with the <code>Doors</code> column to make sure they're numerical, just as we did previously.</p> In\u00a0[78]: Copied! <pre># Now let's one hot encode the features with the same code as before \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\ncategorical_features = [\"Make\", \"Colour\", \"Doors\"]\n\none_hot = OneHotEncoder()\n\ntransformer = ColumnTransformer([(\"one_hot\", \n                                  one_hot, \n                                  categorical_features)],\n                                remainder=\"passthrough\",\n                                sparse_threshold=0) # return a sparse matrix or not\n\n# Fill train and test values separately\ntransformed_X_train = transformer.fit_transform(filled_X_train_df)\ntransformed_X_test = transformer.transform(filled_X_test_df)\n\n# Check transformed and filled X_train\ntransformed_X_train\n</pre> # Now let's one hot encode the features with the same code as before  from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer  categorical_features = [\"Make\", \"Colour\", \"Doors\"]  one_hot = OneHotEncoder()  transformer = ColumnTransformer([(\"one_hot\",                                    one_hot,                                    categorical_features)],                                 remainder=\"passthrough\",                                 sparse_threshold=0) # return a sparse matrix or not  # Fill train and test values separately transformed_X_train = transformer.fit_transform(filled_X_train_df) transformed_X_test = transformer.transform(filled_X_test_df)  # Check transformed and filled X_train transformed_X_train Out[78]: <pre>array([[0.0, 1.0, 0.0, ..., 1.0, 0.0, 71934.0],\n       [0.0, 0.0, 0.0, ..., 1.0, 0.0, 162665.0],\n       [0.0, 1.0, 0.0, ..., 1.0, 0.0, 42844.0],\n       ...,\n       [0.0, 0.0, 0.0, ..., 1.0, 0.0, 196225.0],\n       [0.0, 1.0, 0.0, ..., 1.0, 0.0, 133117.0],\n       [0.0, 1.0, 0.0, ..., 1.0, 0.0, 150582.0]], dtype=object)</pre> <p>Nice!</p> <p>Now our data is:</p> <ol> <li>All numerical</li> <li>No missing values</li> </ol> <p>Let's try and fit a model!</p> In\u00a0[79]: Copied! <pre># Now we've transformed X, let's see if we can fit a model\nnp.random.seed(42)\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor()\n\n# Make sure to use the transformed data (filled and one-hot encoded X data)\nmodel.fit(transformed_X_train, y_train)\nmodel.score(transformed_X_test, y_test)\n</pre> # Now we've transformed X, let's see if we can fit a model np.random.seed(42) from sklearn.ensemble import RandomForestRegressor  model = RandomForestRegressor()  # Make sure to use the transformed data (filled and one-hot encoded X data) model.fit(transformed_X_train, y_train) model.score(transformed_X_test, y_test) Out[79]: <pre>0.21229043336119102</pre> <p>You might have noticed this result is slightly different to before.</p> <p>Why do you think this is?</p> <p>It's because we've created our training and testing sets differently.</p> <p>We split the data into training and test sets before filling the missing values.</p> <p>Previously, we did the reverse, filled missing values before splitting the data into training and test sets.</p> <p>Doing this can lead to information from the training set leaking into the testing set.</p> <p>Remember, one of the most important concepts in machine learning is making sure your model doesn't see any testing data before evaluation.</p> <p>We'll keep practicing but for now, some of the main takeaways are:</p> <ul> <li>Keep your training and test sets separate.</li> <li>Most datasets you come across won't be in a form ready to immediately start using them with machine learning models. And some may take more preparation than others to get ready to use.</li> <li>For most machine learning models, your data has to be numerical. This will involve converting whatever you're working with into numbers. This process is often referred to as feature engineering or feature encoding.</li> <li>Some machine learning models aren't compatible with missing data. The process of filling missing data is referred to as data imputation.</li> </ul> In\u00a0[80]: Copied! <pre># Get California Housing dataset\nfrom sklearn.datasets import fetch_california_housing\nhousing = fetch_california_housing()\nhousing; # gets downloaded as dictionary\n</pre> # Get California Housing dataset from sklearn.datasets import fetch_california_housing housing = fetch_california_housing() housing; # gets downloaded as dictionary <p>Since it's in a dictionary, let's turn it into a DataFrame so we can inspect it better.</p> In\u00a0[81]: Copied! <pre>housing_df = pd.DataFrame(housing[\"data\"], columns=housing[\"feature_names\"])\nhousing_df[\"target\"] = pd.Series(housing[\"target\"])\nhousing_df.head()\n</pre> housing_df = pd.DataFrame(housing[\"data\"], columns=housing[\"feature_names\"]) housing_df[\"target\"] = pd.Series(housing[\"target\"]) housing_df.head() Out[81]: MedInc HouseAge AveRooms AveBedrms Population AveOccup Latitude Longitude target 0 8.3252 41.0 6.984127 1.023810 322.0 2.555556 37.88 -122.23 4.526 1 8.3014 21.0 6.238137 0.971880 2401.0 2.109842 37.86 -122.22 3.585 2 7.2574 52.0 8.288136 1.073446 496.0 2.802260 37.85 -122.24 3.521 3 5.6431 52.0 5.817352 1.073059 558.0 2.547945 37.85 -122.25 3.413 4 3.8462 52.0 6.281853 1.081081 565.0 2.181467 37.85 -122.25 3.422 In\u00a0[82]: Copied! <pre># How many samples?\nlen(housing_df)\n</pre> # How many samples? len(housing_df) Out[82]: <pre>20640</pre> <p>Beautiful, our goal here is to use the feature columns, such as:</p> <ul> <li><code>MedInc</code> - median income in block group</li> <li><code>HouseAge</code> - median house age in block group</li> <li><code>AveRooms</code> - average number of rooms per household</li> <li><code>AveBedrms</code> - average number of bedrooms per household</li> </ul> <p>To predict the <code>target</code> column which expresses the median house value for specfici California districts in hundreds of thousands of dollars (e.g. 4.526 = $452,600).</p> <p>In essence, each row is a different district in California (the data) and we're trying to build a model to predict the median house value in that distract (the target/label) given a series of attributes about the houses in that district.</p> <p>Since we have data and labels, this is a supervised learning problem.</p> <p>And since we're trying to predict a number, it's a regression problem.</p> <p>Knowing these two things, how do they line up on the Scikit-Learn machine learning algorithm cheat-sheet?</p> <p>Following the map through, knowing what we know, it suggests we try <code>RidgeRegression</code>. Let's chek it out.</p> In\u00a0[83]: Copied! <pre># Import the Ridge model class from the linear_model module\nfrom sklearn.linear_model import Ridge\n\n# Setup random seed\nnp.random.seed(42)\n\n# Split the data into features (X) and labels (y)\nX = housing_df.drop(\"target\", axis=1)\ny = housing_df[\"target\"]\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Institate and fit the model (on the training set)\nmodel = Ridge()\nmodel.fit(X_train, y_train)\n\n# Check the score of the model (on the test set)\n# The default score() metirc of regression aglorithms is R^2\nmodel.score(X_test, y_test)\n</pre> # Import the Ridge model class from the linear_model module from sklearn.linear_model import Ridge  # Setup random seed np.random.seed(42)  # Split the data into features (X) and labels (y) X = housing_df.drop(\"target\", axis=1) y = housing_df[\"target\"]  # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  # Institate and fit the model (on the training set) model = Ridge() model.fit(X_train, y_train)  # Check the score of the model (on the test set) # The default score() metirc of regression aglorithms is R^2 model.score(X_test, y_test) Out[83]: <pre>0.5758549611440122</pre> <p>What if <code>RidgeRegression</code> didn't work? Or what if we wanted to improve our results?</p> <p>Following the diagram, the next step would be to try <code>EnsembleRegressors</code>.</p> <p>Ensemble is another word for multiple models put together to make a decision.</p> <p>One of the most common and useful ensemble methods is the Random Forest. Known for its fast training and prediction times and adaptibility to different problems.</p> <p>The basic premise of the Random Forest is to combine a number of different decision trees, each one random from the other and make a prediction on a sample by averaging the result of each decision tree.</p> <p>An in-depth discussion of the Random Forest algorithm is beyond the scope of this notebook but if you're interested in learning more, An Implementation and Explanation of the Random Forest in Python by Will Koehrsen is a great read.</p> <p>Since we're working with regression, we'll use Scikit-Learn's <code>RandomForestRegressor</code>.</p> <p>We can use the exact same workflow as above. Except for changing the model.</p> In\u00a0[84]: Copied! <pre># Import the RandomForestRegressor model class from the ensemble module\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Setup random seed\nnp.random.seed(42)\n\n# Split the data into features (X) and labels (y)\nX = housing_df.drop(\"target\", axis=1)\ny = housing_df[\"target\"]\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Institate and fit the model (on the training set)\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\n\n# Check the score of the model (on the test set)\n# The default score metirc of regression aglorithms is R^2\nmodel.score(X_test, y_test)\n</pre> # Import the RandomForestRegressor model class from the ensemble module from sklearn.ensemble import RandomForestRegressor  # Setup random seed np.random.seed(42)  # Split the data into features (X) and labels (y) X = housing_df.drop(\"target\", axis=1) y = housing_df[\"target\"]  # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  # Institate and fit the model (on the training set) model = RandomForestRegressor() model.fit(X_train, y_train)  # Check the score of the model (on the test set) # The default score metirc of regression aglorithms is R^2 model.score(X_test, y_test) Out[84]: <pre>0.8059809073051385</pre> <p>Woah!</p> <p>We get a good boost in score on the test set by changing the model.</p> <p>This is another incredibly important concept in machine learning, if at first something doesn't achieve what you'd like, experiment, experiment, experiment!</p> <p>At first, the Scikit-Learn algorithm diagram can seem confusing.</p> <p>But once you get a little practice applying different models to different problems, you'll start to pick up which sorts of algorithms do better with different types of data.</p> In\u00a0[85]: Copied! <pre>heart_disease = pd.read_csv(\"../data/heart-disease.csv\")\nheart_disease.head()\n</pre> heart_disease = pd.read_csv(\"../data/heart-disease.csv\") heart_disease.head() Out[85]: age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target 0 63 1 3 145 233 1 0 150 0 2.3 0 0 1 1 1 37 1 2 130 250 0 1 187 0 3.5 0 0 2 1 2 41 0 1 130 204 0 0 172 0 1.4 2 0 2 1 3 56 1 1 120 236 0 1 178 0 0.8 2 0 2 1 4 57 0 0 120 354 0 1 163 1 0.6 2 0 2 1 In\u00a0[86]: Copied! <pre># How many samples are there?\nlen(heart_disease)\n</pre> # How many samples are there? len(heart_disease) Out[86]: <pre>303</pre> <p>Similar to the California Housing dataset, here we want to use all of the available data to predict the target column (1 for if a patient has heart disease and 0 for if they don't).</p> <p>So what do we know?</p> <p>We've got 303 samples (1 row = 1 sample) and we're trying to predict whether or not a patient has heart disease.</p> <p>Because we're trying to predict whether each sample is one thing or another, we've got a classification problem.</p> <p>Let's see how it lines up with our Scikit-Learn algorithm cheat-sheet.</p> <p>Following the cheat-sheet we end up at <code>LinearSVC</code> which stands for Linear Support Vector Classifier. Let's try it on our data.</p> In\u00a0[87]: Copied! <pre># Import LinearSVC from the svm module\nfrom sklearn.svm import LinearSVC\n\n# Setup random seed\nnp.random.seed(42)\n\n# Split the data into X (features/data) and y (target/labels)\nX = heart_disease.drop(\"target\", axis=1)\ny = heart_disease[\"target\"]\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Instantiate and fit the model (on the training set)\nclf = LinearSVC(max_iter=1000, # iterations on the data, 1000 is the default\n                dual=\"auto\") # dual=\"auto\" chooses best parameters for the model automatically\nclf.fit(X_train, y_train)\n\n# Check the score of the model (on the test set)\nclf.score(X_test, y_test)\n</pre> # Import LinearSVC from the svm module from sklearn.svm import LinearSVC  # Setup random seed np.random.seed(42)  # Split the data into X (features/data) and y (target/labels) X = heart_disease.drop(\"target\", axis=1) y = heart_disease[\"target\"]  # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  # Instantiate and fit the model (on the training set) clf = LinearSVC(max_iter=1000, # iterations on the data, 1000 is the default                 dual=\"auto\") # dual=\"auto\" chooses best parameters for the model automatically clf.fit(X_train, y_train)  # Check the score of the model (on the test set) clf.score(X_test, y_test) Out[87]: <pre>0.8688524590163934</pre> <p>Straight out of the box (with no tuning or improvements) our model achieves over 85% accruacy!</p> <p>Although this is a sensational result to begin with, let's check out the diagram and see what other models we might use.</p> <p>Following the path (and skipping a few, don't worry, we'll get to this) we come up to <code>EnsembleMethods</code> again.</p> <p>Except this time, we'll be looking at ensemble classifiers instead of regressors.</p> <p>Remember our <code>RandomForestRegressor</code> from above?</p> <p>We'll it has a dance partner, <code>RandomForestClassifier</code> which is an ensemble based machine model learning model for classification.</p> <p>You might be able to guess what we can use it for (hint: classification problems).</p> <p>Let's try!</p> In\u00a0[88]: Copied! <pre># Import the RandomForestClassifier model class from the ensemble module\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Setup random seed\nnp.random.seed(42)\n\n# Split the data into X (features/data) and y (target/labels)\nX = heart_disease.drop(\"target\", axis=1)\ny = heart_disease[\"target\"]\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Instantiate and fit the model (on the training set)\nclf = RandomForestClassifier(n_estimators=100) # 100 is the default, but you could try 1000 and see what happens\nclf.fit(X_train, y_train)\n\n# Check the score of the model (on the test set)\nclf.score(X_test, y_test)\n</pre> # Import the RandomForestClassifier model class from the ensemble module from sklearn.ensemble import RandomForestClassifier  # Setup random seed np.random.seed(42)  # Split the data into X (features/data) and y (target/labels) X = heart_disease.drop(\"target\", axis=1) y = heart_disease[\"target\"]  # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  # Instantiate and fit the model (on the training set) clf = RandomForestClassifier(n_estimators=100) # 100 is the default, but you could try 1000 and see what happens clf.fit(X_train, y_train)  # Check the score of the model (on the test set) clf.score(X_test, y_test) Out[88]: <pre>0.8524590163934426</pre> <p>Hmmm, it looks like the default hyperparameters of <code>RandomForestClassifier</code> don't perform as well as <code>LinearSVC</code>.</p> <p>Other than trying another classification model, we could start to run experiments to try and improve these models via hyperparameter tuning.</p> <p>Hyperparameter tuning is fancy term for adjusting some settings on a model to try and make it better.</p> <p>It usually happens once you've found a decent baseline model that you'd like to improve upon.</p> <p>In this case, we could take either the <code>RandomForestClassifier</code> or the <code>LinearSVC</code> and try and improve it with hyperparameter tuning (which we'll see later on).</p> <p>For example, you could try and take the <code>n_estimators</code> parameter (the number of trees in the forest) of <code>RandomForestClassifier</code> and change it from <code>100</code> (default) to <code>1000</code> and see what happens.</p> In\u00a0[89]: Copied! <pre># Import the RandomForestClassifier model class from the ensemble module\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Setup random seed\nnp.random.seed(42)\n\n# Split the data into X (features/data) and y (target/labels)\nX = heart_disease.drop(\"target\", axis=1)\ny = heart_disease[\"target\"]\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Instantiate the model (on the training set)\nclf = RandomForestClassifier(n_estimators=100)\n\n# Call the fit method on the model and pass it training data\nclf.fit(X_train, y_train)\n\n# Check the score of the model (on the test set)\nclf.score(X_test, y_test)\n</pre> # Import the RandomForestClassifier model class from the ensemble module from sklearn.ensemble import RandomForestClassifier  # Setup random seed np.random.seed(42)  # Split the data into X (features/data) and y (target/labels) X = heart_disease.drop(\"target\", axis=1) y = heart_disease[\"target\"]  # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  # Instantiate the model (on the training set) clf = RandomForestClassifier(n_estimators=100)  # Call the fit method on the model and pass it training data clf.fit(X_train, y_train)  # Check the score of the model (on the test set) clf.score(X_test, y_test) Out[89]: <pre>0.8524590163934426</pre> <p>What's happening here?</p> <p>Calling the <code>fit()</code> method will cause the machine learning algorithm to attempt to find patterns between <code>X</code> and <code>y</code>. Or if there's no <code>y</code>, it'll only find the patterns within <code>X</code>.</p> <p>Let's see <code>X</code>.</p> In\u00a0[90]: Copied! <pre>X.head()\n</pre> X.head() Out[90]: age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal 0 63 1 3 145 233 1 0 150 0 2.3 0 0 1 1 37 1 2 130 250 0 1 187 0 3.5 0 0 2 2 41 0 1 130 204 0 0 172 0 1.4 2 0 2 3 56 1 1 120 236 0 1 178 0 0.8 2 0 2 4 57 0 0 120 354 0 1 163 1 0.6 2 0 2 <p>And <code>y</code>.</p> In\u00a0[91]: Copied! <pre>y.head()\n</pre> y.head() Out[91]: <pre>0    1\n1    1\n2    1\n3    1\n4    1\nName: target, dtype: int64</pre> <p>Passing <code>X</code> and <code>y</code> to <code>fit()</code> will cause the model to go through all of the examples in <code>X</code> (data) and see what their corresponding <code>y</code> (label) is.</p> <p>How the model does this is different depending on the model you use.</p> <p>Explaining the details of each would take an entire textbook.</p> <p>For now, you could imagine it similar to how you would figure out patterns if you had enough time.</p> <p>You'd look at the feature variables, <code>X</code>, the <code>age</code>, <code>sex</code>, <code>chol</code> (cholesterol) and see what different values led to the labels, <code>y</code>, <code>1</code> for heart disease, <code>0</code> for not heart disease.</p> <p>This concept, regardless of the problem, is similar throughout all of machine learning.</p> <p>During training (finding patterns in data):</p> <p>A machine learning algorithm looks at a dataset, finds patterns, tries to use those patterns to predict something and corrects itself as best it can with the available data and labels. It stores these patterns for later use.</p> <p>During testing or in production (using learned patterns):</p> <p>A machine learning algorithm uses the patterns its previously learned in a dataset to make a prediction on some unseen data.</p> In\u00a0[92]: Copied! <pre># Use a trained model to make predictions\nclf.predict(X_test)\n</pre> # Use a trained model to make predictions clf.predict(X_test) Out[92]: <pre>array([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0])</pre> <p>Given data in the form of <code>X</code>, the <code>predict()</code> function returns labels in the form of <code>y</code>.</p> <p>Note: For the <code>predict()</code> function to work, it must be passed <code>X</code> (data) in the same format the model was trained on. For example, if a model was trained on 10 features formatted in a certain way, predictions should be made on data with 10 features fortmatted in a certain way. Anything different and it will return an error.</p> <p>It's standard practice to save these predictions to a variable named something like <code>y_preds</code> for later comparison to <code>y_test</code> or <code>y_true</code> (usually same as <code>y_test</code> just another name).</p> In\u00a0[93]: Copied! <pre># Compare predictions to truth\ny_preds = clf.predict(X_test)\nnp.mean(y_preds == y_test)\n</pre> # Compare predictions to truth y_preds = clf.predict(X_test) np.mean(y_preds == y_test) Out[93]: <pre>0.8524590163934426</pre> <p>Another way evaluating predictions (comparing them to the truth labels) is with Scikit-Learn's <code>sklearn.metrics</code> module.</p> <p>Inside, you'll find method such as <code>accuracy_score()</code>, which is the default evaluation metric for classification problems.</p> In\u00a0[94]: Copied! <pre>from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_preds)\n</pre> from sklearn.metrics import accuracy_score accuracy_score(y_test, y_preds) Out[94]: <pre>0.8524590163934426</pre> <p><code>predict_proba()</code> returns the probabilities (proba is short for probability) of a classification label.</p> In\u00a0[95]: Copied! <pre># Return probabilities rather than labels\nclf.predict_proba(X_test[:5])\n</pre> # Return probabilities rather than labels clf.predict_proba(X_test[:5]) Out[95]: <pre>array([[0.89, 0.11],\n       [0.49, 0.51],\n       [0.43, 0.57],\n       [0.84, 0.16],\n       [0.18, 0.82]])</pre> <p>Let's see the difference.</p> In\u00a0[96]: Copied! <pre># Return labels\nclf.predict(X_test[:5])\n</pre> # Return labels clf.predict(X_test[:5]) Out[96]: <pre>array([0, 1, 1, 0, 1])</pre> <p><code>predict_proba()</code> returns an array of five arrays each containing two values.</p> <p>Each number is the probability of a label given a sample.</p> In\u00a0[97]: Copied! <pre># Find prediction probabilities for 1 sample\nclf.predict_proba(X_test[:1])\n</pre> # Find prediction probabilities for 1 sample clf.predict_proba(X_test[:1]) Out[97]: <pre>array([[0.89, 0.11]])</pre> <p>This output means for the sample <code>X_test[:1]</code>, the model is predicting label 0 (index 0) with a probability score of 0.9.</p> <p>Because the highest probability score is at index <code>0</code> (and it's over 0.5), when using <code>predict()</code>, a label of <code>0</code> is assigned.</p> In\u00a0[98]: Copied! <pre># Return the label for 1 sample\nclf.predict(X_test[:1])\n</pre> # Return the label for 1 sample clf.predict(X_test[:1]) Out[98]: <pre>array([0])</pre> <p>Where does 0.5 come from?</p> <p>Because our problem is a binary classification task (heart disease or not heart disease), predicting a label with 0.5 probability every time would be the same as a coin toss (guessing 50/50 every time).</p> <p>Therefore, once the prediction probability of a sample passes 0.5 for a certain label, it's assigned that label.</p> <p><code>predict()</code> can also be used for regression models.</p> In\u00a0[99]: Copied! <pre># Import the RandomForestRegressor model class from the ensemble module\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Setup random seed\nnp.random.seed(42)\n\n# Split the data into features (X) and labels (y)\nX = housing_df.drop(\"target\", axis=1)\ny = housing_df[\"target\"]\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Institate and fit the model (on the training set)\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_preds = model.predict(X_test)\n</pre> # Import the RandomForestRegressor model class from the ensemble module from sklearn.ensemble import RandomForestRegressor  # Setup random seed np.random.seed(42)  # Split the data into features (X) and labels (y) X = housing_df.drop(\"target\", axis=1) y = housing_df[\"target\"]  # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  # Institate and fit the model (on the training set) model = RandomForestRegressor() model.fit(X_train, y_train)  # Make predictions y_preds = model.predict(X_test) <p>Now we can evaluate our regression model by using <code>sklearn.metrics.mean_absolute_error</code> which returns the average error across all samples.</p> In\u00a0[100]: Copied! <pre># Compare the predictions to the truth\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(y_test, y_preds)\n</pre> # Compare the predictions to the truth from sklearn.metrics import mean_absolute_error mean_absolute_error(y_test, y_preds) Out[100]: <pre>0.3270458119670544</pre> <p>Now we've seen how to get a model how to find patterns in data using the <code>fit()</code> function and make predictions using what its learned using the <code>predict()</code> and <code>predict_proba()</code> functions, it's time to evaluate those predictions.</p> In\u00a0[101]: Copied! <pre># Import the RandomForestClassifier model class from the ensemble module\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Setup random seed\nnp.random.seed(42)\n\n# Split the data into X (features/data) and y (target/labels)\nX = heart_disease.drop(\"target\", axis=1)\ny = heart_disease[\"target\"]\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Instantiate the model (on the training set)\nclf = RandomForestClassifier(n_estimators=100)\n\n# Call the fit method on the model and pass it training data\nclf.fit(X_train, y_train);\n</pre> # Import the RandomForestClassifier model class from the ensemble module from sklearn.ensemble import RandomForestClassifier  # Setup random seed np.random.seed(42)  # Split the data into X (features/data) and y (target/labels) X = heart_disease.drop(\"target\", axis=1) y = heart_disease[\"target\"]  # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  # Instantiate the model (on the training set) clf = RandomForestClassifier(n_estimators=100)  # Call the fit method on the model and pass it training data clf.fit(X_train, y_train); <p>Once the model has been fit on the training data (<code>X_train</code>, <code>y_train</code>), we can call the <code>score()</code> method on it and evaluate our model on the test data, data the model has never seen before (<code>X_test</code>, <code>y_test</code>).</p> In\u00a0[102]: Copied! <pre># Check the score of the model (on the test set)\nclf.score(X_test, y_test)\n</pre> # Check the score of the model (on the test set) clf.score(X_test, y_test) Out[102]: <pre>0.8524590163934426</pre> <p>Each model in Scikit-Learn implements a default metric for <code>score()</code> which is suitable for the problem.</p> <p>For example:</p> <ul> <li>Classifier models generally use <code>metrics.accuracy_score()</code> as the default <code>score()</code> metric.</li> <li>Regression models generally use <code>metrics.r2_score</code> as the default <code>score()</code> metric.</li> <li>There many more classification and regression specific metrics implemented in <code>sklearn.metrics</code>.</li> </ul> <p>Because <code>clf</code> is an instance of <code>RandomForestClassifier</code>, the <code>score()</code> method uses mean accuracy as its score method.</p> <p>You can find this by pressing SHIFT + TAB (inside a Jupyter Notebook, may be different elsewhere) within the brackets of <code>score()</code> when called on a model instance.</p> <p>Behind the scenes, <code>score()</code> makes predictions on <code>X_test</code> using the trained model and then compares those predictions to the actual labels <code>y_test</code>.</p> <p>A classification model which predicts everything 100% correct would receive an accuracy score of 1.0 (or 100%).</p> <p>Our model doesn't get everything correct, but at ~85% accuracy (0.85 * 100), it's still far better than guessing.</p> <p>Let's do the same but with the regression code from above.</p> In\u00a0[103]: Copied! <pre># Import the RandomForestRegressor model class from the ensemble module\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Setup random seed\nnp.random.seed(42)\n\n# Split the data into features (X) and labels (y)\nX = housing_df.drop(\"target\", axis=1)\ny = housing_df[\"target\"]\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Institate and fit the model (on the training set)\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train);\n</pre> # Import the RandomForestRegressor model class from the ensemble module from sklearn.ensemble import RandomForestRegressor  # Setup random seed np.random.seed(42)  # Split the data into features (X) and labels (y) X = housing_df.drop(\"target\", axis=1) y = housing_df[\"target\"]  # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  # Institate and fit the model (on the training set) model = RandomForestRegressor() model.fit(X_train, y_train); <p>Due to the consistent design of the Scikit-Learn library, we can call the same <code>score()</code> method on <code>model</code>.</p> In\u00a0[104]: Copied! <pre># Check the score of the model (on the test set)\nmodel.score(X_test, y_test)\n</pre> # Check the score of the model (on the test set) model.score(X_test, y_test) Out[104]: <pre>0.8059809073051385</pre> <p>Here, <code>model</code> is an instance of <code>RandomForestRegressor</code>.</p> <p>And since it's a regression model, the default metric built into <code>score()</code> is the coefficient of determination or R^2 (pronounced R-sqaured).</p> <p>Remember, you can find this by pressing SHIFT + TAB within the brackets of <code>score()</code> when called on a model instance.</p> <p>The best possible value here is 1.0, this means the model predicts the target regression values exactly.</p> <p>Calling the <code>score()</code> method on any model instance and passing it test data is a good quick way to see how your model is going.</p> <p>However, when you get further into a problem, it's likely you'll want to start using more powerful metrics to evaluate your models performance.</p> In\u00a0[105]: Copied! <pre># Import cross_val_score from the model_selection module\nfrom sklearn.model_selection import cross_val_score\n\n# Import the RandomForestClassifier model class from the ensemble module\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Setup random seed\nnp.random.seed(42)\n\n# Split the data into X (features/data) and y (target/labels)\nX = heart_disease.drop(\"target\", axis=1)\ny = heart_disease[\"target\"]\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Instantiate the model (on the training set)\nclf = RandomForestClassifier(n_estimators=100)\n\n# Call the fit method on the model and pass it training data\nclf.fit(X_train, y_train);\n</pre> # Import cross_val_score from the model_selection module from sklearn.model_selection import cross_val_score  # Import the RandomForestClassifier model class from the ensemble module from sklearn.ensemble import RandomForestClassifier  # Setup random seed np.random.seed(42)  # Split the data into X (features/data) and y (target/labels) X = heart_disease.drop(\"target\", axis=1) y = heart_disease[\"target\"]  # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  # Instantiate the model (on the training set) clf = RandomForestClassifier(n_estimators=100)  # Call the fit method on the model and pass it training data clf.fit(X_train, y_train); <p>Using <code>cross_val_score()</code> is slightly different to <code>score()</code>.</p> <p>Let's see a code example first and then we'll go through the details.</p> In\u00a0[106]: Copied! <pre># Using score()\nclf.score(X_test, y_test)\n</pre> # Using score() clf.score(X_test, y_test) Out[106]: <pre>0.8524590163934426</pre> In\u00a0[107]: Copied! <pre># Using cross_val_score()\ncross_val_score(clf, X, y, cv=5) # cv = number of splits to test (5 by default)\n</pre> # Using cross_val_score() cross_val_score(clf, X, y, cv=5) # cv = number of splits to test (5 by default) Out[107]: <pre>array([0.81967213, 0.86885246, 0.81967213, 0.78333333, 0.76666667])</pre> <p>What's happening here?</p> <p>The first difference you might notice is <code>cross_val_score()</code> returns an array where as <code>score()</code> only returns a single number.</p> <p><code>cross_val_score()</code> returns an array because of a parameter called <code>cv</code>, which stands for cross-validation.</p> <p>When <code>cv</code> isn't set, <code>cross_val_score()</code> will return an array of 5 numbers by default (<code>cv=None</code> is the same as setting <code>cv=5</code>).</p> <p>Remember, you can see the parameters of a function using SHIFT + TAB (inside a Jupyter Notebook) from within the brackets.</p> <p>But wait, you might be thinking, what even is cross-validation?</p> <p>A visual might be able to help.</p> <p>We've dealt with Figure 1.0 before using <code>score(X_test, y_test)</code>.</p> <p>But looking deeper into this, if a model is trained using the training data or 80% of samples, this means 20% of samples aren't used for the model to learn anything.</p> <p>This also means depending on what 80% is used to train on and what 20% is used to evaluate the model, it may achieve a score which doesn't reflect the entire dataset.</p> <p>For example, if a lot of easy examples are in the 80% training data, when it comes to test on the 20%, your model may perform poorly.</p> <p>The same goes for the reverse.</p> <p>Figure 2.0 shows 5-fold cross-validation, a method which tries to provide a solution to:</p> <ol> <li>Not training on all the data (always keeping training and test sets separate).</li> <li>Avoiding getting lucky scores on single splits of the data.</li> </ol> <p>Instead of training only on 1 training split and evaluating on 1 testing split, 5-fold cross-validation does it 5 times.</p> <p>On a different split each time, returning a score for each.</p> <p>Why 5-fold?</p> <p>The actual name of this setup K-fold cross-validation. Where K is an abitrary number. We've used 5 because it looks nice visually, and it is the default value in <code>sklearn.model_selection.cross_val_score</code>.</p> <p>Figure 2.0 is what happens when we run the following.</p> In\u00a0[108]: Copied! <pre># 5-fold cross-validation\ncross_val_score(clf, X, y, cv=5) # cv is equivalent to K\n</pre> # 5-fold cross-validation cross_val_score(clf, X, y, cv=5) # cv is equivalent to K Out[108]: <pre>array([0.83606557, 0.8852459 , 0.7704918 , 0.8       , 0.8       ])</pre> <p>Since we set <code>cv=5</code> (5-fold cross-validation), we get back 5 different scores instead of 1.</p> <p>Taking the mean of this array gives us a more in-depth idea of how our model is performing by converting the 5 scores into one.</p> In\u00a0[109]: Copied! <pre>np.random.seed(42)\n\n# Single training and test split score\nclf_single_score = clf.score(X_test, y_test)\n\n# Take mean of 5-fold cross-validation\nclf_cross_val_score = np.mean(cross_val_score(clf, X, y, cv=5))\n\nclf_single_score, clf_cross_val_score\n</pre> np.random.seed(42)  # Single training and test split score clf_single_score = clf.score(X_test, y_test)  # Take mean of 5-fold cross-validation clf_cross_val_score = np.mean(cross_val_score(clf, X, y, cv=5))  clf_single_score, clf_cross_val_score Out[109]: <pre>(0.8524590163934426, 0.8248087431693989)</pre> <p>Notice, the average <code>cross_val_score()</code> is slightly lower than single value returned by <code>score()</code>.</p> <p>In this case, if you were asked to report the accuracy of your model, even though it's lower, you'd prefer the cross-validated metric over the non-cross-validated metric.</p> <p>Wait?</p> <p>We haven't used the <code>scoring</code> parameter at all.</p> <p>By default, it's set to <code>None</code>.</p> In\u00a0[110]: Copied! <pre>cross_val_score(clf, X, y, cv=5, scoring=None) # default scoring value, this can be set to other scoring metrics\n</pre> cross_val_score(clf, X, y, cv=5, scoring=None) # default scoring value, this can be set to other scoring metrics Out[110]: <pre>array([0.78688525, 0.86885246, 0.80327869, 0.78333333, 0.76666667])</pre> <p>Note: If you notice different scores each time you call <code>cross_val_score</code>, this is because each data split is random every time. So the model may achieve higher/lower scores on different splits of the data. To get reproducible scores, you can set the random seed.</p> <p>When <code>scoring</code> is set to <code>None</code> (by default), it uses the same metric as <code>score()</code> for whatever model is passed to <code>cross_val_score()</code>.</p> <p>In this case, our model is <code>clf</code> which is an instance of <code>RandomForestClassifier</code> which uses mean accuracy as the default <code>score()</code> metric.</p> <p>You can change the evaluation score <code>cross_val_score()</code> uses by changing the <code>scoring</code> parameter.</p> <p>And as you might have guessed, different problems call for different evaluation scores.</p> <p>The Scikit-Learn documentation outlines a vast range of evaluation metrics for different problems but let's have a look at a few.</p> In\u00a0[111]: Copied! <pre># Import cross_val_score from the model_selection module\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\n\nnp.random.seed(42)\n\nX = heart_disease.drop(\"target\", axis=1)\ny = heart_disease[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)\n</pre> # Import cross_val_score from the model_selection module from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestClassifier  np.random.seed(42)  X = heart_disease.drop(\"target\", axis=1) y = heart_disease[\"target\"]  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  clf = RandomForestClassifier() clf.fit(X_train, y_train) clf.score(X_test, y_test) Out[111]: <pre>0.8524590163934426</pre> In\u00a0[112]: Copied! <pre># Accuracy as percentage\nprint(f\"Heart Disease Classifier Accuracy: {clf.score(X_test, y_test) * 100:.2f}%\")\n</pre> # Accuracy as percentage print(f\"Heart Disease Classifier Accuracy: {clf.score(X_test, y_test) * 100:.2f}%\") <pre>Heart Disease Classifier Accuracy: 85.25%\n</pre> In\u00a0[113]: Copied! <pre>from sklearn.metrics import roc_curve\n\n# Make predictions with probabilities\ny_probs = clf.predict_proba(X_test)\n\n# Keep the probabilites of the positive class only\ny_probs = y_probs[:, 1]\n\n# Calculate fpr, tpr and thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_probs)\n\n# Check the false positive rate\nfpr\n</pre> from sklearn.metrics import roc_curve  # Make predictions with probabilities y_probs = clf.predict_proba(X_test)  # Keep the probabilites of the positive class only y_probs = y_probs[:, 1]  # Calculate fpr, tpr and thresholds fpr, tpr, thresholds = roc_curve(y_test, y_probs)  # Check the false positive rate fpr Out[113]: <pre>array([0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.03448276, 0.03448276, 0.03448276, 0.03448276, 0.06896552,\n       0.06896552, 0.10344828, 0.13793103, 0.13793103, 0.17241379,\n       0.17241379, 0.27586207, 0.4137931 , 0.48275862, 0.55172414,\n       0.65517241, 0.72413793, 0.72413793, 0.82758621, 1.        ])</pre> <p>Looking at these on their own doesn't make much sense. It's much easier to see their value visually.</p> <p>Let's create a helper function to make a ROC curve given the false positive rates (<code>fpr</code>) and true positive rates (<code>tpr</code>).</p> <p>Note: As of Scikit-Learn 1.2+, there is functionality of plotting a ROC curve. You can find this under <code>sklearn.metrics.RocCurveDisplay</code>.</p> In\u00a0[114]: Copied! <pre>import matplotlib.pyplot as plt\n\ndef plot_roc_curve(fpr, tpr):\n    \"\"\"\n    Plots a ROC curve given the false positve rate (fpr) and \n    true postive rate (tpr) of a classifier.\n    \"\"\"\n    # Plot ROC curve\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    # Plot line with no predictive power (baseline)\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--', label='Guessing')\n    # Customize the plot\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()\n    \nplot_roc_curve(fpr, tpr)\n</pre> import matplotlib.pyplot as plt  def plot_roc_curve(fpr, tpr):     \"\"\"     Plots a ROC curve given the false positve rate (fpr) and      true postive rate (tpr) of a classifier.     \"\"\"     # Plot ROC curve     plt.plot(fpr, tpr, color='orange', label='ROC')     # Plot line with no predictive power (baseline)     plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--', label='Guessing')     # Customize the plot     plt.xlabel('False Positive Rate')     plt.ylabel('True Positive Rate')     plt.title('Receiver Operating Characteristic (ROC) Curve')     plt.legend()     plt.show()      plot_roc_curve(fpr, tpr) <p>Looking at the plot for the first time, it might seem a bit confusing.</p> <p>The main thing to take away here is our model is doing far better than guessing.</p> <p>A metric you can use to quantify the ROC curve in a single number is AUC (Area Under Curve).</p> <p>Scikit-Learn implements a function to caculate this called <code>sklearn.metrics.roc_auc_score</code>.</p> <p>The maximum ROC AUC score you can achieve is 1.0 and generally, the closer to 1.0, the better the model.</p> In\u00a0[115]: Copied! <pre>from sklearn.metrics import roc_auc_score\n\nroc_auc_score_value = roc_auc_score(y_test, y_probs)\nroc_auc_score_value\n</pre> from sklearn.metrics import roc_auc_score  roc_auc_score_value = roc_auc_score(y_test, y_probs) roc_auc_score_value Out[115]: <pre>0.9304956896551724</pre> <p>I'll let you in a secret...</p> <p>Although it was good practice, we didn't actually need to create our own <code>plot_roc_curve</code> function.</p> <p>Scikit-Learn allows us to plot a ROC curve directly from our estimator/model by using the class method <code>sklearn.metrics.RocCurveDisplay.from_estimator</code> and passing it our <code>estimator</code>, <code>X_test</code> and <code>y_test</code>.</p> In\u00a0[116]: Copied! <pre>from sklearn.metrics import RocCurveDisplay\nroc_curve_display = RocCurveDisplay.from_estimator(estimator=clf, \n                                                   X=X_test, \n                                                   y=y_test)\n</pre> from sklearn.metrics import RocCurveDisplay roc_curve_display = RocCurveDisplay.from_estimator(estimator=clf,                                                     X=X_test,                                                     y=y_test) <p>The most ideal position for a ROC curve to run along the top left corner of the plot.</p> <p>This would mean the model predicts only true positives and no false positives. And would result in a ROC AUC score of 1.0.</p> <p>You can see this by creating a ROC curve using only the <code>y_test</code> labels.</p> In\u00a0[117]: Copied! <pre># Plot perfect ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_test)\nplot_roc_curve(fpr, tpr)\n</pre> # Plot perfect ROC curve fpr, tpr, thresholds = roc_curve(y_test, y_test) plot_roc_curve(fpr, tpr) In\u00a0[118]: Copied! <pre># Perfect ROC AUC score\nroc_auc_score(y_test, y_test)\n</pre> # Perfect ROC AUC score roc_auc_score(y_test, y_test) Out[118]: <pre>1.0</pre> <p>In reality, a perfect ROC curve is unlikely.</p> In\u00a0[119]: Copied! <pre>from sklearn.metrics import confusion_matrix\n\ny_preds = clf.predict(X_test)\n\nconfusion_matrix(y_test, y_preds)\n</pre> from sklearn.metrics import confusion_matrix  y_preds = clf.predict(X_test)  confusion_matrix(y_test, y_preds) Out[119]: <pre>array([[24,  5],\n       [ 4, 28]])</pre> <p>Again, this is probably easier visualized.</p> <p>One way to do it is with <code>pd.crosstab()</code>.</p> In\u00a0[120]: Copied! <pre>pd.crosstab(y_test, \n            y_preds, \n            rownames=[\"Actual Label\"], \n            colnames=[\"Predicted Label\"])\n</pre> pd.crosstab(y_test,              y_preds,              rownames=[\"Actual Label\"],              colnames=[\"Predicted Label\"]) Out[120]: Predicted Label 0 1 Actual Label 0 24 5 1 4 28 In\u00a0[121]: Copied! <pre>from sklearn.metrics import ConfusionMatrixDisplay\n\nConfusionMatrixDisplay.from_estimator(estimator=clf, X=X, y=y);\n</pre> from sklearn.metrics import ConfusionMatrixDisplay  ConfusionMatrixDisplay.from_estimator(estimator=clf, X=X, y=y); In\u00a0[122]: Copied! <pre># Plot confusion matrix from predictions\nConfusionMatrixDisplay.from_predictions(y_true=y_test, \n                                        y_pred=y_preds);\n</pre> # Plot confusion matrix from predictions ConfusionMatrixDisplay.from_predictions(y_true=y_test,                                          y_pred=y_preds); In\u00a0[123]: Copied! <pre>from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_preds))\n</pre> from sklearn.metrics import classification_report  print(classification_report(y_test, y_preds)) <pre>              precision    recall  f1-score   support\n\n           0       0.86      0.83      0.84        29\n           1       0.85      0.88      0.86        32\n\n    accuracy                           0.85        61\n   macro avg       0.85      0.85      0.85        61\nweighted avg       0.85      0.85      0.85        61\n\n</pre> <p>It returns four columns: precision, recall, f1-score and support.</p> <p>The number of rows will depend on how many different classes there are. But there will always be three rows labell accuracy, macro avg and weighted avg.</p> <p>Each term measures something slightly different:</p> <ul> <li>Precision - Indicates the proportion of positive identifications (model predicted class <code>1</code>) which were actually correct. A model which produces no false positives has a precision of 1.0.</li> <li>Recall - Indicates the proportion of actual positives which were correctly classified. A model which produces no false negatives has a recall of 1.0.</li> <li>F1 score - A combination of precision and recall. A perfect model achieves an F1 score of 1.0.</li> <li>Support - The number of samples each metric was calculated on.</li> <li>Accuracy - The accuracy of the model in decimal form. Perfect accuracy is equal to 1.0, in other words, getting the prediction right 100% of the time.</li> <li>Macro avg - Short for macro average, the average precision, recall and F1 score between classes. Macro avg doesn't take class imbalance into effect. So if you do have class imbalances (more examples of one class than another), you should pay attention to this.</li> <li>Weighted avg - Short for weighted average, the weighted average precision, recall and F1 score between classes. Weighted means each metric is calculated with respect to how many samples there are in each class. This metric will favour the majority class (e.g. it will give a high value when one class out performs another due to having more samples).</li> </ul> <p>When should you use each?</p> <p>It can be tempting to base your classification models perfomance only on accuracy. And accuracy is a good metric to report, except when you have very imbalanced classes.</p> <p>For example, let's say there were 10,000 people. And 1 of them had a disease. You're asked to build a model to predict who has it.</p> <p>You build the model and find your model to be 99.99% accurate. Which sounds great! ...until you realise, all its doing is predicting no one has the disease, in other words all 10,000 predictions are false.</p> <p>In this case, you'd want to turn to metrics such as precision, recall and F1 score.</p> In\u00a0[124]: Copied! <pre># Where precision and recall become valuable\ndisease_true = np.zeros(10000)\ndisease_true[0] = 1 # only one case\n\ndisease_preds = np.zeros(10000) # every prediction is 0\n\npd.DataFrame(classification_report(disease_true, \n                                   disease_preds, \n                                   output_dict=True,\n                                   zero_division=0))\n</pre> # Where precision and recall become valuable disease_true = np.zeros(10000) disease_true[0] = 1 # only one case  disease_preds = np.zeros(10000) # every prediction is 0  pd.DataFrame(classification_report(disease_true,                                     disease_preds,                                     output_dict=True,                                    zero_division=0)) Out[124]: 0.0 1.0 accuracy macro avg weighted avg precision 0.99990 0.0 0.9999 0.499950 0.99980 recall 1.00000 0.0 0.9999 0.500000 0.99990 f1-score 0.99995 0.0 0.9999 0.499975 0.99985 support 9999.00000 1.0 0.9999 10000.000000 10000.00000 <p>You can see here, we've got an accuracy of 0.9999 (99.99%), great precision and recall on class 0.0 but nothing for class 1.0.</p> <p>Ask yourself, although the model achieves 99.99% accuracy, is it useful?</p> <p>To summarize:</p> <ul> <li>Accuracy is a good measure to start with if all classes are balanced (e.g. same amount of samples which are labelled with 0 or 1)</li> <li>Precision and recall become more important when classes are imbalanced.</li> <li>If false positive predictions are worse than false negatives, aim for higher precision.</li> <li>If false negative predictions are worse than false positives, aim for higher recall.</li> </ul> <p>Resource: For more on precision and recall and the tradeoffs between them, I'd suggest going through the Scikit-Learn Precision-Recall guide.</p> In\u00a0[125]: Copied! <pre># Import the RandomForestRegressor model class from the ensemble module\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Setup random seed\nnp.random.seed(42)\n\n# Split data into features (X) and labels (y)\nX = housing_df.drop(\"target\", axis=1)\ny = housing_df[\"target\"]\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Institate and fit the model (on the training set)\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train);\n</pre> # Import the RandomForestRegressor model class from the ensemble module from sklearn.ensemble import RandomForestRegressor  # Setup random seed np.random.seed(42)  # Split data into features (X) and labels (y) X = housing_df.drop(\"target\", axis=1) y = housing_df[\"target\"]  # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  # Institate and fit the model (on the training set) model = RandomForestRegressor() model.fit(X_train, y_train); <p>R^2 Score (coefficient of determination)</p> <p>Once you've got a trained regression model, the default evaluation metric in the <code>score()</code> function is R^2.</p> In\u00a0[126]: Copied! <pre># Calculate the models R^2 score\nmodel.score(X_test, y_test)\n</pre> # Calculate the models R^2 score model.score(X_test, y_test) Out[126]: <pre>0.8059809073051385</pre> <p>Outside of the <code>score()</code> function, R^2 can be calculated using Scikit-Learn's <code>r2_score()</code> function.</p> <p>A model which only predicted the mean would get a score of 0.</p> In\u00a0[127]: Copied! <pre>from sklearn.metrics import r2_score\n\n# Fill an array with y_test mean\ny_test_mean = np.full(len(y_test), y_test.mean())\n\nr2_score(y_test, y_test_mean)\n</pre> from sklearn.metrics import r2_score  # Fill an array with y_test mean y_test_mean = np.full(len(y_test), y_test.mean())  r2_score(y_test, y_test_mean) Out[127]: <pre>0.0</pre> <p>And a perfect model would get a score of 1.</p> In\u00a0[128]: Copied! <pre>r2_score(y_test, y_test)\n</pre> r2_score(y_test, y_test) Out[128]: <pre>1.0</pre> <p>For your regression models, you'll want to maximise R^2, whilst minimising MAE and MSE.</p> <p>Mean Absolute Error (MAE)</p> <p>A model's mean absolute error can be calculated with Scikit-Learn's <code>sklearn.metrics.mean_absolute_error</code> method.</p> In\u00a0[129]: Copied! <pre># Mean absolute error\nfrom sklearn.metrics import mean_absolute_error\n\ny_preds = model.predict(X_test)\nmae = mean_absolute_error(y_test, y_preds)\nmae\n</pre> # Mean absolute error from sklearn.metrics import mean_absolute_error  y_preds = model.predict(X_test) mae = mean_absolute_error(y_test, y_preds) mae Out[129]: <pre>0.3270458119670544</pre> <p>Our model achieves an MAE of 0.327.</p> <p>This means, on average our models predictions are 0.327 units away from the actual value.</p> <p>Let's make it a little more visual.</p> In\u00a0[130]: Copied! <pre>df = pd.DataFrame(data={\"actual values\": y_test, \n                   \"predictions\": y_preds})\n\ndf\n</pre> df = pd.DataFrame(data={\"actual values\": y_test,                     \"predictions\": y_preds})  df Out[130]: actual values predictions 20046 0.47700 0.490580 3024 0.45800 0.759890 15663 5.00001 4.935016 20484 2.18600 2.558640 9814 2.78000 2.334610 ... ... ... 15362 2.63300 2.225000 16623 2.66800 1.972540 18086 5.00001 4.853989 2144 0.72300 0.714910 3665 1.51500 1.665680 <p>4128 rows \u00d7 2 columns</p> <p>You can see the predictions are slightly different to the actual values.</p> <p>Depending what problem you're working on, having a difference like we do now, might be okay. On the flip side, it may also not be okay, meaning the predictions would have to be closer.</p> In\u00a0[131]: Copied! <pre>fig, ax = plt.subplots()\nx = np.arange(0, len(df), 1)\nax.scatter(x, df[\"actual values\"], c='b', label=\"Acutual Values\")\nax.scatter(x, df[\"predictions\"], c='r', label=\"Predictions\")\nax.legend(loc=(1, 0.5));\n</pre> fig, ax = plt.subplots() x = np.arange(0, len(df), 1) ax.scatter(x, df[\"actual values\"], c='b', label=\"Acutual Values\") ax.scatter(x, df[\"predictions\"], c='r', label=\"Predictions\") ax.legend(loc=(1, 0.5)); <p>Mean Squared Error (MSE)</p> <p>How about MSE?</p> <p>We can calculate it with Scikit-Learn's <code>sklearn.metrics.mean_squared_error</code>.</p> In\u00a0[132]: Copied! <pre># Mean squared error\nfrom sklearn.metrics import mean_squared_error\n\nmse = mean_squared_error(y_test, y_preds)\nmse\n</pre> # Mean squared error from sklearn.metrics import mean_squared_error  mse = mean_squared_error(y_test, y_preds) mse Out[132]: <pre>0.2542443610174998</pre> <p>MSE will often be higher than MAE because is squares the errors rather than only taking the absolute difference into account.</p> <p>Now you might be thinking, which regression evaluation metric should you use?</p> <ul> <li>R^2 is similar to accuracy. It gives you a quick indication of how well your model might be doing. Generally, the closer your R^2 value is to 1.0, the better the model. But it doesn't really tell exactly how wrong your model is in terms of how far off each prediction is.</li> <li>MAE gives a better indication of how far off each of your model's predictions are on average.</li> <li>As for MAE or MSE, because of the way MSE is calculated, squaring the differences between predicted values and actual values, it amplifies larger differences. Let's say we're predicting the value of houses (which we are).<ul> <li>Pay more attention to MAE: When being $10,000 off is twice as bad as being $5,000 off.</li> <li>Pay more attention to MSE: When being $10,000 off is more than twice as bad as being $5,000 off.</li> </ul> </li> </ul> <p>Note: What we've covered here is only a handful of potential metrics you can use to evaluate your models. If you're after a complete list, check out the Scikit-Learn metrics and scoring documentation.</p> In\u00a0[133]: Copied! <pre>from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\n\nnp.random.seed(42)\n\nX = heart_disease.drop(\"target\", axis=1)\ny = heart_disease[\"target\"]\n\nclf = RandomForestClassifier(n_estimators=100)\n</pre> from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestClassifier  np.random.seed(42)  X = heart_disease.drop(\"target\", axis=1) y = heart_disease[\"target\"]  clf = RandomForestClassifier(n_estimators=100) <p>First, we'll use the default, which is mean accuracy.</p> In\u00a0[134]: Copied! <pre>np.random.seed(42)\ncv_acc = cross_val_score(clf, X, y, cv=5)\ncv_acc\n</pre> np.random.seed(42) cv_acc = cross_val_score(clf, X, y, cv=5) cv_acc Out[134]: <pre>array([0.81967213, 0.90163934, 0.83606557, 0.78333333, 0.78333333])</pre> <p>We've seen this before, now we got 5 different accuracy scores on different test splits of the data.</p> <p>Averaging this gives the cross-validated accuracy.</p> In\u00a0[135]: Copied! <pre># Cross-validated accuracy\nprint(f\"The cross-validated accuracy is: {np.mean(cv_acc)*100:.2f}%\")\n</pre> # Cross-validated accuracy print(f\"The cross-validated accuracy is: {np.mean(cv_acc)*100:.2f}%\") <pre>The cross-validated accuracy is: 82.48%\n</pre> <p>We can find the same using the <code>scoring</code> parameter and passing it <code>\"accuracy\"</code>.</p> In\u00a0[136]: Copied! <pre>np.random.seed(42)\ncv_acc = cross_val_score(clf, X, y, cv=5, scoring=\"accuracy\")\nprint(f\"The cross-validated accuracy is: {np.mean(cv_acc)*100:.2f}%\")\n</pre> np.random.seed(42) cv_acc = cross_val_score(clf, X, y, cv=5, scoring=\"accuracy\") print(f\"The cross-validated accuracy is: {np.mean(cv_acc)*100:.2f}%\") <pre>The cross-validated accuracy is: 82.48%\n</pre> <p>The same goes for the other metrics we've been using for classification.</p> <p>Let's try <code>\"precision\"</code>.</p> In\u00a0[137]: Copied! <pre>np.random.seed(42)\ncv_precision = cross_val_score(clf, X, y, cv=5, scoring=\"precision\")\nprint(f\"The cross-validated precision is: {np.mean(cv_precision):.2f}\")\n</pre> np.random.seed(42) cv_precision = cross_val_score(clf, X, y, cv=5, scoring=\"precision\") print(f\"The cross-validated precision is: {np.mean(cv_precision):.2f}\") <pre>The cross-validated precision is: 0.83\n</pre> <p>How about <code>\"recall\"</code>?</p> In\u00a0[138]: Copied! <pre>np.random.seed(42)\ncv_recall = cross_val_score(clf, X, y, cv=5, scoring=\"recall\")\nprint(f\"The cross-validated recall is: {np.mean(cv_recall):.2f}\")\n</pre> np.random.seed(42) cv_recall = cross_val_score(clf, X, y, cv=5, scoring=\"recall\") print(f\"The cross-validated recall is: {np.mean(cv_recall):.2f}\") <pre>The cross-validated recall is: 0.85\n</pre> <p>And <code>\"f1\"</code> (for F1 score)?</p> In\u00a0[139]: Copied! <pre>np.random.seed(42)\ncv_f1 = cross_val_score(clf, X, y, cv=5, scoring=\"f1\")\nprint(f\"The cross-validated F1 score is: {np.mean(cv_f1):.2f}\")\n</pre> np.random.seed(42) cv_f1 = cross_val_score(clf, X, y, cv=5, scoring=\"f1\") print(f\"The cross-validated F1 score is: {np.mean(cv_f1):.2f}\") <pre>The cross-validated F1 score is: 0.84\n</pre> <p>We can repeat this process with our regression metrics.</p> <p>Let's revisit our regression model.</p> In\u00a0[140]: Copied! <pre>from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\n\nnp.random.seed(42)\n\nX = housing_df.drop(\"target\", axis=1)\ny = housing_df[\"target\"]\n\nmodel = RandomForestRegressor(n_estimators=100)\n</pre> from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestRegressor  np.random.seed(42)  X = housing_df.drop(\"target\", axis=1) y = housing_df[\"target\"]  model = RandomForestRegressor(n_estimators=100) <p>The default is <code>\"r2\"</code>.</p> In\u00a0[141]: Copied! <pre>np.random.seed(42)\ncv_r2 = cross_val_score(model, X, y, cv=5, scoring=\"r2\")\nprint(f\"The cross-validated R^2 score is: {np.mean(cv_r2):.2f}\")\n</pre> np.random.seed(42) cv_r2 = cross_val_score(model, X, y, cv=5, scoring=\"r2\") print(f\"The cross-validated R^2 score is: {np.mean(cv_r2):.2f}\") <pre>The cross-validated R^2 score is: 0.65\n</pre> <p>But we can use <code>\"neg_mean_absolute_error\"</code> for MAE (mean absolute error).</p> In\u00a0[142]: Copied! <pre>np.random.seed(42)\ncv_mae = cross_val_score(model, X, y, cv=5, scoring=\"neg_mean_absolute_error\")\nprint(f\"The cross-validated MAE score is: {np.mean(cv_mae):.2f}\")\n</pre> np.random.seed(42) cv_mae = cross_val_score(model, X, y, cv=5, scoring=\"neg_mean_absolute_error\") print(f\"The cross-validated MAE score is: {np.mean(cv_mae):.2f}\") <pre>The cross-validated MAE score is: -0.47\n</pre> <p>Why the <code>\"neg_\"</code>?</p> <p>Because Scikit-Learn documentation states:</p> <p>\"All scorer objects follow the convention that higher return values are better than lower return values.\"</p> <p>Which in this case, means a lower negative value (closer to 0) is better.</p> <p>What about <code>\"neg_mean_squared_error\"</code> for MSE (mean squared error)?</p> In\u00a0[143]: Copied! <pre>np.random.seed(42)\ncv_mse = cross_val_score(model, \n                         X, \n                         y, \n                         cv=5,\n                         scoring=\"neg_mean_squared_error\")\nprint(f\"The cross-validated MSE score is: {np.mean(cv_mse):.2f}\")\n</pre> np.random.seed(42) cv_mse = cross_val_score(model,                           X,                           y,                           cv=5,                          scoring=\"neg_mean_squared_error\") print(f\"The cross-validated MSE score is: {np.mean(cv_mse):.2f}\") <pre>The cross-validated MSE score is: -0.43\n</pre> In\u00a0[144]: Copied! <pre>from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(42)\n\nX = heart_disease.drop(\"target\", axis=1)\ny = heart_disease[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\n\n# Make predictions\ny_preds = clf.predict(X_test)\n\n# Evaluate the classifier\nprint(\"Classifier metrics on the test set:\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_preds) * 100:.2f}%\")\nprint(f\"Precision: {precision_score(y_test, y_preds):.2f}\")\nprint(f\"Recall: {recall_score(y_test, y_preds):.2f}\")\nprint(f\"F1: {f1_score(y_test, y_preds):.2f}\")\n</pre> from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split  np.random.seed(42)  X = heart_disease.drop(\"target\", axis=1) y = heart_disease[\"target\"]  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  clf = RandomForestClassifier(n_estimators=100) clf.fit(X_train, y_train)  # Make predictions y_preds = clf.predict(X_test)  # Evaluate the classifier print(\"Classifier metrics on the test set:\") print(f\"Accuracy: {accuracy_score(y_test, y_preds) * 100:.2f}%\") print(f\"Precision: {precision_score(y_test, y_preds):.2f}\") print(f\"Recall: {recall_score(y_test, y_preds):.2f}\") print(f\"F1: {f1_score(y_test, y_preds):.2f}\") <pre>Classifier metrics on the test set:\nAccuracy: 85.25%\nPrecision: 0.85\nRecall: 0.88\nF1: 0.86\n</pre> In\u00a0[145]: Copied! <pre>from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(42)\n\nX = housing_df.drop(\"target\", axis=1)\ny = housing_df[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.2)\n\nmodel = RandomForestRegressor(n_estimators=100, \n                              n_jobs=-1)\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_preds = model.predict(X_test)\n\n# Evaluate the model\nprint(\"Regression model metrics on the test set:\")\nprint(f\"R^2: {r2_score(y_test, y_preds):.2f}\")\nprint(f\"MAE: {mean_absolute_error(y_test, y_preds):.2f}\")\nprint(f\"MSE: {mean_squared_error(y_test, y_preds):.2f}\")\n</pre> from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split  np.random.seed(42)  X = housing_df.drop(\"target\", axis=1) y = housing_df[\"target\"]  X_train, X_test, y_train, y_test = train_test_split(X,                                                      y,                                                      test_size=0.2)  model = RandomForestRegressor(n_estimators=100,                                n_jobs=-1) model.fit(X_train, y_train)  # Make predictions y_preds = model.predict(X_test)  # Evaluate the model print(\"Regression model metrics on the test set:\") print(f\"R^2: {r2_score(y_test, y_preds):.2f}\") print(f\"MAE: {mean_absolute_error(y_test, y_preds):.2f}\") print(f\"MSE: {mean_squared_error(y_test, y_preds):.2f}\") <pre>Regression model metrics on the test set:\nR^2: 0.81\nMAE: 0.33\nMSE: 0.25\n</pre> <p>Wow!</p> <p>We've covered a lot!</p> <p>But it's worth it.</p> <p>Because evaluating a model's predictions is as important as training a model in any machine learning project.</p> <p>There's nothing worse than training a machine learning model and optimizing for the wrong evaluation metric.</p> <p>Keep the metrics and evaluation methods we've gone through when training your future models.</p> <p>If you're after extra reading, I'd go through the Scikit-Learn guide for model evaluation.</p> <p>Now we've seen some different metrics we can use to evaluate a model, let's see some ways we can improve those metrics.</p> In\u00a0[146]: Copied! <pre>from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()\n</pre> from sklearn.ensemble import RandomForestClassifier  clf = RandomForestClassifier() <p>When we instantiate a model like above, we're using the default hyperparameters.</p> <p>These get printed out when you call the model instance and <code>get_params()</code>.</p> In\u00a0[147]: Copied! <pre>clf.get_params()\n</pre> clf.get_params() Out[147]: <pre>{'bootstrap': True,\n 'ccp_alpha': 0.0,\n 'class_weight': None,\n 'criterion': 'gini',\n 'max_depth': None,\n 'max_features': 'sqrt',\n 'max_leaf_nodes': None,\n 'max_samples': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'n_estimators': 100,\n 'n_jobs': None,\n 'oob_score': False,\n 'random_state': None,\n 'verbose': 0,\n 'warm_start': False}</pre> <p>You'll see things like <code>max_depth</code>, <code>min_samples_split</code>, <code>n_estimators</code>.</p> <p>Each of these is a hyperparameter of the <code>RandomForestClassifier</code> you can adjust.</p> <p>You can think of hyperparameters as being similar to dials on an oven.</p> <p>On the default setting your oven might do an okay job cooking your favourite meal. But with a little experimentation, you find it does better when you adjust the settings.</p> <p>The same goes for imporving a machine learning model by hyperparameter tuning.</p> <p>The default hyperparameters on a machine learning model may find patterns in data well. But there's a chance a adjusting the hyperparameters may improve a models performance.</p> <p>Every machine learning model will have different hyperparameters you can tune.</p> <p>You might be thinking, \"how the hell do I remember all of these?\"</p> <p>Another good question.</p> <p>It's why we're focused on the Random Forest.</p> <p>Instead of memorizing all of the hyperparameters for every model, we'll see how it's done with one.</p> <p>And then knowing these principles, you can apply them to a different model if needed.</p> <p>Reading the Scikit-Learn documentation for the Random Forest, you'll find they suggest trying to change <code>n_estimators</code> (the number of trees in the forest) and <code>min_samples_split</code> (the minimum number of samples required to split an internal node).</p> <p>We'll try tuning these as well as:</p> <ul> <li><code>max_features</code> (the number of features to consider when looking for the best split)</li> <li><code>max_depth</code> (the maximum depth of the tree)</li> <li><code>min_samples_leaf</code> (the minimum number of samples required to be at a leaf node)</li> </ul> <p>If this still sounds like a lot, the good news is, the process we're taking with the Random Forest and tuning its hyperparameters, can be used for other machine learning models in Scikit-Learn. The only difference is, with a different model, the hyperparameters you tune will be different.</p> <p>Adjusting hyperparameters is usually an experimental process to figure out which are best. As there's no real way of knowing which hyperparameters will be best when starting out.</p> <p>To get familar with hyparameter tuning, we'll take our RandomForestClassifier and adjust its hyperparameters in 3 ways.</p> <ol> <li>By hand</li> <li>Randomly with <code>sklearn.model_selection.RandomizedSearchCV</code></li> <li>Exhaustively with <code>sklearn.model_selection.GridSearchCV</code></li> </ol> In\u00a0[148]: Copied! <pre>clf.get_params()\n</pre> clf.get_params() Out[148]: <pre>{'bootstrap': True,\n 'ccp_alpha': 0.0,\n 'class_weight': None,\n 'criterion': 'gini',\n 'max_depth': None,\n 'max_features': 'sqrt',\n 'max_leaf_nodes': None,\n 'max_samples': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'n_estimators': 100,\n 'n_jobs': None,\n 'oob_score': False,\n 'random_state': None,\n 'verbose': 0,\n 'warm_start': False}</pre> <p>And we're going to adjust:</p> <ul> <li><code>max_depth</code></li> <li><code>max_features</code></li> <li><code>min_samples_leaf</code></li> <li><code>min_samples_split</code></li> <li><code>n_estimators</code></li> </ul> <p>We'll use the same code as before, except this time we'll create a training, validation and test split.</p> <p>With the training set containing 70% of the data and the validation and test sets each containing 15%.</p> <p>Let's get some baseline results, then we'll tune the model.</p> <p>And since we're going to be evaluating a few models, let's make an evaluation function.</p> In\u00a0[149]: Copied! <pre>def evaluate_preds(y_true: np.array, \n                   y_preds: np.array) -&gt; dict:\n    \"\"\"\n    Performs evaluation comparison on y_true labels vs. y_pred labels.\n\n    Returns several metrics in the form of a dictionary.\n    \"\"\"\n    accuracy = accuracy_score(y_true, y_preds)\n    precision = precision_score(y_true, y_preds)\n    recall = recall_score(y_true, y_preds)\n    f1 = f1_score(y_true, y_preds)\n    metric_dict = {\"accuracy\": round(accuracy, 2),\n                   \"precision\": round(precision, 2), \n                   \"recall\": round(recall, 2),\n                   \"f1\": round(f1, 2)}\n    print(f\"Acc: {accuracy * 100:.2f}%\")\n    print(f\"Precision: {precision:.2f}\")\n    print(f\"Recall: {recall:.2f}\")\n    print(f\"F1 score: {f1:.2f}\")\n\n    return metric_dict\n</pre> def evaluate_preds(y_true: np.array,                     y_preds: np.array) -&gt; dict:     \"\"\"     Performs evaluation comparison on y_true labels vs. y_pred labels.      Returns several metrics in the form of a dictionary.     \"\"\"     accuracy = accuracy_score(y_true, y_preds)     precision = precision_score(y_true, y_preds)     recall = recall_score(y_true, y_preds)     f1 = f1_score(y_true, y_preds)     metric_dict = {\"accuracy\": round(accuracy, 2),                    \"precision\": round(precision, 2),                     \"recall\": round(recall, 2),                    \"f1\": round(f1, 2)}     print(f\"Acc: {accuracy * 100:.2f}%\")     print(f\"Precision: {precision:.2f}\")     print(f\"Recall: {recall:.2f}\")     print(f\"F1 score: {f1:.2f}\")      return metric_dict <p>Wonderful!</p> <p>Now let's recreate a previous workflow, except we'll add in the creation of a validation set.</p> In\u00a0[150]: Copied! <pre>from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Set the seed\nnp.random.seed(42)\n\n# Read in the data\nheart_disease = pd.read_csv(\"../data/heart-disease.csv\")\n\n# Split into X (features) &amp; y (labels)\nX = heart_disease.drop(\"target\", axis=1)\ny = heart_disease[\"target\"]\n\n# Training and test split (70% train, 30% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n# Create validation and test split by spliting testing data in half (30% test -&gt; 15% validation, 15% test)\nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5)\n\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\n\n# Make predictions\ny_preds = clf.predict(X_valid)\n\n# Evaluate the classifier\nbaseline_metrics = evaluate_preds(y_valid, y_preds)\nbaseline_metrics\n</pre> from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier  # Set the seed np.random.seed(42)  # Read in the data heart_disease = pd.read_csv(\"../data/heart-disease.csv\")  # Split into X (features) &amp; y (labels) X = heart_disease.drop(\"target\", axis=1) y = heart_disease[\"target\"]  # Training and test split (70% train, 30% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)  # Create validation and test split by spliting testing data in half (30% test -&gt; 15% validation, 15% test) X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5)  clf = RandomForestClassifier() clf.fit(X_train, y_train)  # Make predictions y_preds = clf.predict(X_valid)  # Evaluate the classifier baseline_metrics = evaluate_preds(y_valid, y_preds) baseline_metrics <pre>Acc: 80.00%\nPrecision: 0.78\nRecall: 0.88\nF1 score: 0.82\n</pre> Out[150]: <pre>{'accuracy': 0.8, 'precision': 0.78, 'recall': 0.88, 'f1': 0.82}</pre> In\u00a0[151]: Copied! <pre># Check the sizes of the splits\nprint(f\"Training data: {len(X_train)} samples, {len(y_train)} labels\")\nprint(f\"Validation data: {len(X_valid)} samples, {len(y_valid)} labels\")\nprint(f\"Testing data: {len(X_test)} samples, {len(y_test)} labels\")\n</pre> # Check the sizes of the splits print(f\"Training data: {len(X_train)} samples, {len(y_train)} labels\") print(f\"Validation data: {len(X_valid)} samples, {len(y_valid)} labels\") print(f\"Testing data: {len(X_test)} samples, {len(y_test)} labels\") <pre>Training data: 212 samples, 212 labels\nValidation data: 45 samples, 45 labels\nTesting data: 46 samples, 46 labels\n</pre> <p>Beautiful, now let's try and improve the results.</p> <p>We'll change 1 of the hyperparameters, <code>n_estimators=100</code> (default) to <code>n_estimators=200</code> and see if it improves on the validation set.</p> In\u00a0[152]: Copied! <pre>np.random.seed(42)\n\n# Create a second classifier\nclf_2 = RandomForestClassifier(n_estimators=200)\nclf_2.fit(X_train, y_train)\n\n# Make predictions\ny_preds_2 = clf_2.predict(X_valid)\n\n# Evaluate the 2nd classifier\nclf_2_metrics = evaluate_preds(y_valid, y_preds_2)\n</pre> np.random.seed(42)  # Create a second classifier clf_2 = RandomForestClassifier(n_estimators=200) clf_2.fit(X_train, y_train)  # Make predictions y_preds_2 = clf_2.predict(X_valid)  # Evaluate the 2nd classifier clf_2_metrics = evaluate_preds(y_valid, y_preds_2) <pre>Acc: 77.78%\nPrecision: 0.77\nRecall: 0.83\nF1 score: 0.80\n</pre> <p>Hmm, it looks like doubling the <code>n_estimators</code> value performs worse than the default, perhaps there's a better value for <code>n_estimators</code>?</p> <p>And what other hyperparameters could we change?</p> <p>Wait...</p> <p>This could take a while if all we're doing is building new models with new hyperparameters each time.</p> <p>Surely there's a better way?</p> <p>There is.</p> In\u00a0[153]: Copied! <pre># Hyperparameter grid RandomizedSearchCV will search over\nparam_distributions = {\"n_estimators\": [10, 100, 200, 500, 1000, 1200],\n                       \"max_depth\": [None, 5, 10, 20, 30],\n                       \"max_features\": [\"sqrt\", \"log2\", None],\n                       \"min_samples_split\": [2, 4, 6, 8],\n                       \"min_samples_leaf\": [1, 2, 4, 8]}\n</pre> # Hyperparameter grid RandomizedSearchCV will search over param_distributions = {\"n_estimators\": [10, 100, 200, 500, 1000, 1200],                        \"max_depth\": [None, 5, 10, 20, 30],                        \"max_features\": [\"sqrt\", \"log2\", None],                        \"min_samples_split\": [2, 4, 6, 8],                        \"min_samples_leaf\": [1, 2, 4, 8]} <p>Where did these values come from?</p> <p>They're made up.</p> <p>Made up?</p> <p>Yes.</p> <p>Not completely pulled out of the air but after reading the Scikit-Learn documentation on Random Forest's you'll see some of these values have certain values which usually perform well and certain hyperparameters take strings rather than integers.</p> <p>Now we've got the parameter distribution dictionary setup, Scikit-Learn's <code>RandomizedSearchCV</code> will look at it, pick a random value from each, instantiate a model with those values and test each model.</p> <p>How many models will it test?</p> <p>As many as there are for each combination of hyperparameters to be tested. Let's add them up.</p> In\u00a0[154]: Copied! <pre># Count the total number of hyperparameter combinations to test\ntotal_randomized_hyperparameter_combintions_to_test = np.prod([len(value) for value in param_distributions.values()])\nprint(f\"There are {total_randomized_hyperparameter_combintions_to_test} potential combinations of hyperparameters to test.\")\n</pre> # Count the total number of hyperparameter combinations to test total_randomized_hyperparameter_combintions_to_test = np.prod([len(value) for value in param_distributions.values()]) print(f\"There are {total_randomized_hyperparameter_combintions_to_test} potential combinations of hyperparameters to test.\") <pre>There are 1440 potential combinations of hyperparameters to test.\n</pre> <p>Woah!</p> <p>That's a lot of combinations!</p> <p>Or...</p> <p>We can set the <code>n_iter</code> parameter to limit the number of models <code>RandomizedSearchCV</code> tests (e.g. <code>n_iter=20</code> means to try <code>20</code> different random combintations of hyperparameters and will cross-validate each set, so if <code>cv=5</code>, 5x20 = 100 total fits).</p> <p>The best thing?</p> <p>The results we get will be cross-validated (hence the CV in <code>RandomizedSearchCV</code>) so we can use <code>train_test_split()</code>.</p> <p>And since we're going over so many different models, we'll set <code>n_jobs=-1</code> in our <code>RandomForestClassifier</code> so Scikit-Learn takes advantage of all the cores (processors) on our computers.</p> <p>Let's see it in action.</p> <p>Note: Depending on <code>n_iter</code> (how many models you test), the different values in the hyperparameter grid, and the power of your computer, running the cell below may take a while (for reference, it took about ~1-minute on my M1 Pro MacBook Pro).</p> In\u00a0[155]: Copied! <pre># Start the timer\nimport time\nstart_time = time.time()\n\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\n\nnp.random.seed(42)\n\n# Split into X &amp; y\nX = heart_disease.drop(\"target\", axis=1)\ny = heart_disease[\"target\"]\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Set n_jobs to -1 to use all available cores on your machine (if this causes errors, try n_jobs=1)\nclf = RandomForestClassifier(n_jobs=-1)\n\n# Setup RandomizedSearchCV \nn_iter = 30 # try 30 models total\nrs_clf = RandomizedSearchCV(estimator=clf,\n                            param_distributions=param_distributions,\n                            n_iter=n_iter, \n                            cv=5, # 5-fold cross-validation\n                            verbose=2) # print out results\n\n# Fit the RandomizedSearchCV version of clf (does cross-validation for us, so no need to use a validation set)\nrs_clf.fit(X_train, y_train);\n\n# Finish the timer\nend_time = time.time()\nprint(f\"[INFO] Total time taken for {n_iter} random combinations of hyperparameters: {end_time - start_time:.2f} seconds.\")\n</pre> # Start the timer import time start_time = time.time()  from sklearn.model_selection import RandomizedSearchCV, train_test_split  np.random.seed(42)  # Split into X &amp; y X = heart_disease.drop(\"target\", axis=1) y = heart_disease[\"target\"]  # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  # Set n_jobs to -1 to use all available cores on your machine (if this causes errors, try n_jobs=1) clf = RandomForestClassifier(n_jobs=-1)  # Setup RandomizedSearchCV  n_iter = 30 # try 30 models total rs_clf = RandomizedSearchCV(estimator=clf,                             param_distributions=param_distributions,                             n_iter=n_iter,                              cv=5, # 5-fold cross-validation                             verbose=2) # print out results  # Fit the RandomizedSearchCV version of clf (does cross-validation for us, so no need to use a validation set) rs_clf.fit(X_train, y_train);  # Finish the timer end_time = time.time() print(f\"[INFO] Total time taken for {n_iter} random combinations of hyperparameters: {end_time - start_time:.2f} seconds.\") <pre>Fitting 5 folds for each of 30 candidates, totalling 150 fits\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=10, max_features=None, min_samples_leaf=8, min_samples_split=2, n_estimators=500; total time=   0.4s\n[CV] END max_depth=10, max_features=None, min_samples_leaf=8, min_samples_split=2, n_estimators=500; total time=   0.4s\n[CV] END max_depth=10, max_features=None, min_samples_leaf=8, min_samples_split=2, n_estimators=500; total time=   0.4s\n[CV] END max_depth=10, max_features=None, min_samples_leaf=8, min_samples_split=2, n_estimators=500; total time=   0.4s\n[CV] END max_depth=10, max_features=None, min_samples_leaf=8, min_samples_split=2, n_estimators=500; total time=   0.5s\n[CV] END max_depth=5, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=10; total time=   0.0s\n[CV] END max_depth=5, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=10; total time=   0.0s\n[CV] END max_depth=5, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=10; total time=   0.0s\n[CV] END max_depth=5, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=10; total time=   0.0s\n[CV] END max_depth=5, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=10; total time=   0.0s\n[CV] END max_depth=5, max_features=log2, min_samples_leaf=2, min_samples_split=8, n_estimators=100; total time=   0.1s\n[CV] END max_depth=5, max_features=log2, min_samples_leaf=2, min_samples_split=8, n_estimators=100; total time=   0.1s\n[CV] END max_depth=5, max_features=log2, min_samples_leaf=2, min_samples_split=8, n_estimators=100; total time=   0.1s\n[CV] END max_depth=5, max_features=log2, min_samples_leaf=2, min_samples_split=8, n_estimators=100; total time=   0.1s\n[CV] END max_depth=5, max_features=log2, min_samples_leaf=2, min_samples_split=8, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=10, max_features=log2, min_samples_leaf=8, min_samples_split=6, n_estimators=10; total time=   0.0s\n[CV] END max_depth=10, max_features=log2, min_samples_leaf=8, min_samples_split=6, n_estimators=10; total time=   0.0s\n[CV] END max_depth=10, max_features=log2, min_samples_leaf=8, min_samples_split=6, n_estimators=10; total time=   0.0s\n[CV] END max_depth=10, max_features=log2, min_samples_leaf=8, min_samples_split=6, n_estimators=10; total time=   0.0s\n[CV] END max_depth=10, max_features=log2, min_samples_leaf=8, min_samples_split=6, n_estimators=10; total time=   0.0s\n[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=8, min_samples_split=4, n_estimators=1200; total time=   1.0s\n[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=8, min_samples_split=4, n_estimators=1200; total time=   1.0s\n[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=8, min_samples_split=4, n_estimators=1200; total time=   1.0s\n[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=8, min_samples_split=4, n_estimators=1200; total time=   1.0s\n[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=8, min_samples_split=4, n_estimators=1200; total time=   1.0s\n[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=10; total time=   0.0s\n[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=10; total time=   0.0s\n[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=10; total time=   0.0s\n[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=10; total time=   0.0s\n[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=10; total time=   0.0s\n[CV] END max_depth=20, max_features=log2, min_samples_leaf=8, min_samples_split=6, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, max_features=log2, min_samples_leaf=8, min_samples_split=6, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, max_features=log2, min_samples_leaf=8, min_samples_split=6, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, max_features=log2, min_samples_leaf=8, min_samples_split=6, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, max_features=log2, min_samples_leaf=8, min_samples_split=6, n_estimators=100; total time=   0.1s\n[CV] END max_depth=5, max_features=log2, min_samples_leaf=8, min_samples_split=4, n_estimators=10; total time=   0.0s\n[CV] END max_depth=5, max_features=log2, min_samples_leaf=8, min_samples_split=4, n_estimators=10; total time=   0.0s\n[CV] END max_depth=5, max_features=log2, min_samples_leaf=8, min_samples_split=4, n_estimators=10; total time=   0.0s\n[CV] END max_depth=5, max_features=log2, min_samples_leaf=8, min_samples_split=4, n_estimators=10; total time=   0.0s\n[CV] END max_depth=5, max_features=log2, min_samples_leaf=8, min_samples_split=4, n_estimators=10; total time=   0.0s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   0.9s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   0.9s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   0.9s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   0.9s\n[CV] END max_depth=None, max_features=None, min_samples_leaf=4, min_samples_split=4, n_estimators=1200; total time=   1.0s\n[CV] END max_depth=None, max_features=None, min_samples_leaf=4, min_samples_split=4, n_estimators=1200; total time=   1.0s\n[CV] END max_depth=None, max_features=None, min_samples_leaf=4, min_samples_split=4, n_estimators=1200; total time=   1.0s\n[CV] END max_depth=None, max_features=None, min_samples_leaf=4, min_samples_split=4, n_estimators=1200; total time=   1.0s\n[CV] END max_depth=None, max_features=None, min_samples_leaf=4, min_samples_split=4, n_estimators=1200; total time=   1.0s\n[CV] END max_depth=5, max_features=None, min_samples_leaf=2, min_samples_split=8, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=5, max_features=None, min_samples_leaf=2, min_samples_split=8, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=5, max_features=None, min_samples_leaf=2, min_samples_split=8, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=5, max_features=None, min_samples_leaf=2, min_samples_split=8, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=5, max_features=None, min_samples_leaf=2, min_samples_split=8, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=10; total time=   0.0s\n[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=10; total time=   0.0s\n[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=10; total time=   0.0s\n[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=10; total time=   0.0s\n[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=10; total time=   0.0s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=2, min_samples_split=8, n_estimators=1200; total time=   1.1s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=2, min_samples_split=8, n_estimators=1200; total time=   1.0s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=2, min_samples_split=8, n_estimators=1200; total time=   1.0s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=2, min_samples_split=8, n_estimators=1200; total time=   1.0s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=2, min_samples_split=8, n_estimators=1200; total time=   1.1s\n[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=8, min_samples_split=2, n_estimators=1000; total time=   0.9s\n[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=8, min_samples_split=2, n_estimators=1000; total time=   0.9s\n[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=8, min_samples_split=2, n_estimators=1000; total time=   0.9s\n[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=8, min_samples_split=2, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=8, min_samples_split=2, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=10; total time=   0.0s\n[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=10; total time=   0.0s\n[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=10; total time=   0.0s\n[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=10; total time=   0.0s\n[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=10; total time=   0.0s\n[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=6, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=6, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=6, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=6, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=6, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=4, min_samples_split=8, n_estimators=500; total time=   0.4s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=4, min_samples_split=8, n_estimators=500; total time=   0.4s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=4, min_samples_split=8, n_estimators=500; total time=   0.4s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=4, min_samples_split=8, n_estimators=500; total time=   0.4s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=4, min_samples_split=8, n_estimators=500; total time=   0.4s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=5, max_features=None, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   0.5s\n[CV] END max_depth=5, max_features=None, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   0.4s\n[CV] END max_depth=5, max_features=None, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   0.4s\n[CV] END max_depth=5, max_features=None, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   0.5s\n[CV] END max_depth=5, max_features=None, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   0.4s\n[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=8, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=8, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=8, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=8, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=8, n_estimators=200; total time=   0.2s\n[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=10; total time=   0.0s\n[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=10; total time=   0.0s\n[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=10; total time=   0.0s\n[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=10; total time=   0.0s\n[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=10; total time=   0.0s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=8, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=8, n_estimators=1000; total time=   0.9s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=8, n_estimators=1000; total time=   0.9s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=8, n_estimators=1000; total time=   0.9s\n[CV] END max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=8, n_estimators=1000; total time=   0.9s\n[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=None, max_features=None, min_samples_leaf=1, min_samples_split=8, n_estimators=10; total time=   0.0s\n[CV] END max_depth=None, max_features=None, min_samples_leaf=1, min_samples_split=8, n_estimators=10; total time=   0.0s\n[CV] END max_depth=None, max_features=None, min_samples_leaf=1, min_samples_split=8, n_estimators=10; total time=   0.0s\n[CV] END max_depth=None, max_features=None, min_samples_leaf=1, min_samples_split=8, n_estimators=10; total time=   0.0s\n[CV] END max_depth=None, max_features=None, min_samples_leaf=1, min_samples_split=8, n_estimators=10; total time=   0.0s\n[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.1s\n[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.1s\n[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   0.4s\n[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   0.4s\n[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   0.4s\n[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   0.4s\n[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   0.4s\n[CV] END max_depth=None, max_features=None, min_samples_leaf=8, min_samples_split=4, n_estimators=10; total time=   0.0s\n[CV] END max_depth=None, max_features=None, min_samples_leaf=8, min_samples_split=4, n_estimators=10; total time=   0.0s\n[CV] END max_depth=None, max_features=None, min_samples_leaf=8, min_samples_split=4, n_estimators=10; total time=   0.0s\n[CV] END max_depth=None, max_features=None, min_samples_leaf=8, min_samples_split=4, n_estimators=10; total time=   0.0s\n[CV] END max_depth=None, max_features=None, min_samples_leaf=8, min_samples_split=4, n_estimators=10; total time=   0.0s\n[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=10; total time=   0.0s\n[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=10; total time=   0.0s\n[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=10; total time=   0.0s\n[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=10; total time=   0.0s\n[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=10; total time=   0.0s\n[INFO] Total time taken for 30 random combinations of hyperparameters: 49.50 seconds.\n</pre> <p>When <code>RandomizedSearchCV</code> goes through <code>n_iter</code> combinations of of hyperparameter search space, it stores the best ones in the attribute <code>best_params_</code>.</p> In\u00a0[156]: Copied! <pre># Find the best hyperparameters found by RandomizedSearchCV\nrs_clf.best_params_\n</pre> # Find the best hyperparameters found by RandomizedSearchCV rs_clf.best_params_ Out[156]: <pre>{'n_estimators': 200,\n 'min_samples_split': 6,\n 'min_samples_leaf': 4,\n 'max_features': 'log2',\n 'max_depth': 30}</pre> <p>Now when we call <code>predict()</code> on <code>rs_clf</code> (our <code>RandomizedSearchCV</code> version of our classifier), it'll use the best hyperparameters it found.</p> In\u00a0[157]: Copied! <pre># Make predictions with the best hyperparameters\nrs_y_preds = rs_clf.predict(X_test)\n\n# Evaluate the predictions\nrs_metrics = evaluate_preds(y_test, rs_y_preds)\n</pre> # Make predictions with the best hyperparameters rs_y_preds = rs_clf.predict(X_test)  # Evaluate the predictions rs_metrics = evaluate_preds(y_test, rs_y_preds) <pre>Acc: 85.25%\nPrecision: 0.85\nRecall: 0.88\nF1 score: 0.86\n</pre> <p>Excellent!</p> <p>Thanks to <code>RandomizedSearchCV</code> testing out a bunch of different hyperparameters, we get a nice boost to all of the evaluation metrics for our classification model.</p> In\u00a0[158]: Copied! <pre>param_distributions\n</pre> param_distributions Out[158]: <pre>{'n_estimators': [10, 100, 200, 500, 1000, 1200],\n 'max_depth': [None, 5, 10, 20, 30],\n 'max_features': ['sqrt', 'log2', None],\n 'min_samples_split': [2, 4, 6, 8],\n 'min_samples_leaf': [1, 2, 4, 8]}</pre> <p><code>RandomizedSearchCV</code> tries <code>n_iter</code> combinations of different values.</p> <p>Where as, <code>GridSearchCV</code> will try every single possible combination.</p> <p>And if you remember from before when we did the calculation: <code>max_depth</code> has 4 values, <code>max_features</code> has 2, <code>min_samples_leaf</code> has 3, <code>min_samples_split</code> has 3, <code>n_estimators</code> has 5.</p> <p>That's 4x2x3x3x5 = 360 models!</p> <p>This could take a long time depending on the power of the computer you're using, the amount of data you have and the complexity of the hyperparamters (usually higher values means a more complex model).</p> <p>In our case, the data we're using is relatively small (only ~300 samples).</p> <p>Since we've already tried to find some ideal hyperparameters using <code>RandomizedSearchCV</code>, we'll create another hyperparameter grid based on the <code>best_params_</code> of <code>rs_clf</code> with less options and then try to use <code>GridSearchCV</code> to find a more ideal set.</p> <p>In essence, the workflow could be:</p> <ol> <li>Tune hyperparameters by hand to get a feel of the data/model.</li> <li>Create a large set of hyperparameter distributions and search across them randomly with <code>RandomizedSearchCV</code>.</li> <li>Find the best hyperparameters from 2 and reduce the search space before searching across a smaller subset exhaustively with <code>GridSearchCV</code>.</li> </ol> <p>Note: Based on the <code>best_params_</code> of <code>rs_clf</code> implies the next set of hyperparameters we'll try are roughly in the same range of the best set found by <code>RandomizedSearchCV</code>.</p> In\u00a0[159]: Copied! <pre># Create hyperparameter grid similar to rs_clf.best_params_\nparam_grid = {\"n_estimators\": [200, 1000],\n              \"max_depth\": [30, 40, 50],\n              \"max_features\": [\"log2\"],\n              \"min_samples_split\": [2, 4, 6, 8],\n              \"min_samples_leaf\": [4]}\n</pre> # Create hyperparameter grid similar to rs_clf.best_params_ param_grid = {\"n_estimators\": [200, 1000],               \"max_depth\": [30, 40, 50],               \"max_features\": [\"log2\"],               \"min_samples_split\": [2, 4, 6, 8],               \"min_samples_leaf\": [4]} <p>We've created another grid of hyperparameters to search over, this time with less total.</p> In\u00a0[160]: Copied! <pre># Count the total number of hyperparameter combinations to test\ntotal_grid_search_hyperparameter_combinations_to_test = np.prod([len(value) for value in param_grid.values()])\nprint(f\"There are {total_grid_search_hyperparameter_combinations_to_test} combinations of hyperparameters to test.\")\nprint(f\"This is {total_randomized_hyperparameter_combintions_to_test/total_grid_search_hyperparameter_combinations_to_test} times less\\\n than before (previous: {total_randomized_hyperparameter_combintions_to_test}).\")\n</pre> # Count the total number of hyperparameter combinations to test total_grid_search_hyperparameter_combinations_to_test = np.prod([len(value) for value in param_grid.values()]) print(f\"There are {total_grid_search_hyperparameter_combinations_to_test} combinations of hyperparameters to test.\") print(f\"This is {total_randomized_hyperparameter_combintions_to_test/total_grid_search_hyperparameter_combinations_to_test} times less\\  than before (previous: {total_randomized_hyperparameter_combintions_to_test}).\") <pre>There are 24 combinations of hyperparameters to test.\nThis is 60.0 times less than before (previous: 1440).\n</pre> <p>Now when we run <code>GridSearchCV</code>, passing it our classifier (<code>clf</code>), parameter grid (<code>param_grid</code>) and the number of cross-validation folds we'd like to use (<code>cv=5</code>), it'll create a model with every single combination of hyperparameters, and then cross-validate each 5 times (for example, 36 hyperparameter combinations * 5 = 135 fits in total) and check the results.</p> <p>Note: Depending on the compute power of the machine you're using, the following cell may take a few minutes to run (for reference, it took ~60 seconds on my M1 Pro MacBook Pro).</p> In\u00a0[161]: Copied! <pre># Start the timer\nimport time\nstart_time = time.time()\n\nfrom sklearn.model_selection import GridSearchCV, train_test_split\n\nnp.random.seed(42)\n\n# Split into X &amp; y\nX = heart_disease.drop(\"target\", axis=1)\ny = heart_disease[\"target\"]\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Set n_jobs to -1 to use all available machine cores (if this produces errors, try n_jobs=1)\nclf = RandomForestClassifier(n_jobs=-1)\n\n# Setup GridSearchCV\ngs_clf = GridSearchCV(estimator=clf,\n                      param_grid=param_grid,\n                      cv=5, # 5-fold cross-validation\n                      verbose=2) # print out progress\n\n# Fit the RandomizedSearchCV version of clf\ngs_clf.fit(X_train, y_train);\n\n# Find the running time\nend_time = time.time()\n</pre> # Start the timer import time start_time = time.time()  from sklearn.model_selection import GridSearchCV, train_test_split  np.random.seed(42)  # Split into X &amp; y X = heart_disease.drop(\"target\", axis=1) y = heart_disease[\"target\"]  # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  # Set n_jobs to -1 to use all available machine cores (if this produces errors, try n_jobs=1) clf = RandomForestClassifier(n_jobs=-1)  # Setup GridSearchCV gs_clf = GridSearchCV(estimator=clf,                       param_grid=param_grid,                       cv=5, # 5-fold cross-validation                       verbose=2) # print out progress  # Fit the RandomizedSearchCV version of clf gs_clf.fit(X_train, y_train);  # Find the running time end_time = time.time() <pre>Fitting 5 folds for each of 24 candidates, totalling 120 fits\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.2s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   0.9s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   0.9s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.2s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.2s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.2s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.2s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.2s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.2s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.2s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   0.9s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.2s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.2s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.2s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.2s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=200; total time=   0.2s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   0.8s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   0.9s\n[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=1000; total time=   0.8s\n</pre> In\u00a0[162]: Copied! <pre># How long did it take? \ntotal_time = end_time - start_time\nprint(f\"[INFO] The total running time for running GridSearchCV was {total_time:.2f} seconds.\")\n</pre> # How long did it take?  total_time = end_time - start_time print(f\"[INFO] The total running time for running GridSearchCV was {total_time:.2f} seconds.\") <pre>[INFO] The total running time for running GridSearchCV was 61.45 seconds.\n</pre> <p>Once it completes, we can check the best hyperparameter combinations it found using the <code>best_params_</code> attribute.</p> In\u00a0[163]: Copied! <pre># Check the best hyperparameters found with GridSearchCV\ngs_clf.best_params_\n</pre> # Check the best hyperparameters found with GridSearchCV gs_clf.best_params_ Out[163]: <pre>{'max_depth': 30,\n 'max_features': 'log2',\n 'min_samples_leaf': 4,\n 'min_samples_split': 2,\n 'n_estimators': 200}</pre> <p>And by default when we call the <code>predict()</code> function on <code>gs_clf</code>, it'll use the best hyperparameters.</p> In\u00a0[164]: Copied! <pre># Max predictions with the GridSearchCV classifier\ngs_y_preds = gs_clf.predict(X_test)\n\n# Evaluate the predictions\ngs_metrics = evaluate_preds(y_test, gs_y_preds)\ngs_metrics\n</pre> # Max predictions with the GridSearchCV classifier gs_y_preds = gs_clf.predict(X_test)  # Evaluate the predictions gs_metrics = evaluate_preds(y_test, gs_y_preds) gs_metrics <pre>Acc: 88.52%\nPrecision: 0.88\nRecall: 0.91\nF1 score: 0.89\n</pre> Out[164]: <pre>{'accuracy': 0.89, 'precision': 0.88, 'recall': 0.91, 'f1': 0.89}</pre> <p>Let's create a DataFrame to compare the different metrics.</p> In\u00a0[165]: Copied! <pre>compare_metrics = pd.DataFrame({\"baseline\": baseline_metrics,\n                                \"clf_2\": clf_2_metrics,\n                                \"random search\": rs_metrics,\n                                \"grid search\": gs_metrics})\ncompare_metrics.plot.bar(figsize=(10, 8));\n</pre> compare_metrics = pd.DataFrame({\"baseline\": baseline_metrics,                                 \"clf_2\": clf_2_metrics,                                 \"random search\": rs_metrics,                                 \"grid search\": gs_metrics}) compare_metrics.plot.bar(figsize=(10, 8)); <p>Nice!</p> <p>After trying many different combinations of hyperparamters, we get a slight improvement in results.</p> <p>However, sometimes you'll notice that your results don't change much.</p> <p>These things might happen.</p> <p>But it's important to remember, it's not over. There more things you can try.</p> <p>In a hyperparameter tuning sense, there may be a better set we could find through more extensive searching with <code>RandomizedSearchCV</code> and <code>GridSearchCV</code>, this would require more experimentation.</p> <p>Other techniques you could:</p> <ul> <li>Collecting more data - Based on the results our models are getting now, it seems like they're very capable of finding patterns. Collecting more data may improve a models ability to find patterns. However, your ability to do this will largely depend on the project you're working on.</li> <li>Try a more advanced model - Although our tuned Random Forest model is doing pretty well, a more advanced ensemble method such as XGBoost or CatBoost might perform better. I'll leave these for extra-curriculum.</li> </ul> <p>Since machine learning is part engineering, part science, these kind of experiments are common place in any machine learning project.</p> <p>Now we've got a tuned Random Forest model, let's find out how we might save it and export it so we can share it with others or potentially use it in an external application.</p> In\u00a0[166]: Copied! <pre>import pickle\n\n# Save an existing model to file\nbest_model_file_name_pickle = \"gs_random_forest_model_1.pkl\" # .pkl extension stands for \"pickle\"\npickle.dump(gs_clf, open(best_model_file_name_pickle, \"wb\"))\n</pre> import pickle  # Save an existing model to file best_model_file_name_pickle = \"gs_random_forest_model_1.pkl\" # .pkl extension stands for \"pickle\" pickle.dump(gs_clf, open(best_model_file_name_pickle, \"wb\")) <p>Once it's saved, we can import it using <code>pickle</code>'s <code>load()</code> function, passing it <code>open()</code> containing the filename as a string and <code>\"rb\"</code> standing for \"read binary\".</p> In\u00a0[167]: Copied! <pre># Load a saved model\nloaded_pickle_model = pickle.load(open(best_model_file_name_pickle, \"rb\"))\n</pre> # Load a saved model loaded_pickle_model = pickle.load(open(best_model_file_name_pickle, \"rb\")) <p>Once you've reimported your trained model using <code>pickle</code>, you can use it to make predictions as usual.</p> In\u00a0[168]: Copied! <pre># Make predictions and evaluate the loaded model\npickle_y_preds = loaded_pickle_model.predict(X_test)\nloaded_pickle_model_metrics = evaluate_preds(y_test, pickle_y_preds)\nloaded_pickle_model_metrics\n</pre> # Make predictions and evaluate the loaded model pickle_y_preds = loaded_pickle_model.predict(X_test) loaded_pickle_model_metrics = evaluate_preds(y_test, pickle_y_preds) loaded_pickle_model_metrics <pre>Acc: 88.52%\nPrecision: 0.88\nRecall: 0.91\nF1 score: 0.89\n</pre> Out[168]: <pre>{'accuracy': 0.89, 'precision': 0.88, 'recall': 0.91, 'f1': 0.89}</pre> <p>You'll notice the reimported model evaluation metrics are the same as the model before we exported it.</p> In\u00a0[169]: Copied! <pre>loaded_pickle_model_metrics == gs_metrics\n</pre> loaded_pickle_model_metrics == gs_metrics Out[169]: <pre>True</pre> In\u00a0[170]: Copied! <pre>from joblib import dump, load\n\n# Save a model to file\nbest_model_file_name_joblib = \"gs_random_forest_model_1.joblib\"\ndump(gs_clf, filename=best_model_file_name_joblib)\n</pre> from joblib import dump, load  # Save a model to file best_model_file_name_joblib = \"gs_random_forest_model_1.joblib\" dump(gs_clf, filename=best_model_file_name_joblib)  Out[170]: <pre>['gs_random_forest_model_1.joblib']</pre> <p>Once you've saved a model using <code>dump()</code>, you can import it using <code>load()</code> and passing it the filename of the model.</p> In\u00a0[171]: Copied! <pre># Import a saved joblib model\nloaded_joblib_model = load(filename=best_model_file_name_joblib)\n</pre> # Import a saved joblib model loaded_joblib_model = load(filename=best_model_file_name_joblib) <p>Again, once imported, we can make predictions with our model.</p> In\u00a0[172]: Copied! <pre># Make and evaluate joblib predictions \njoblib_y_preds = loaded_joblib_model.predict(X_test)\nloaded_joblib_model_metrics = evaluate_preds(y_test, joblib_y_preds)\nloaded_joblib_model_metrics\n</pre> # Make and evaluate joblib predictions  joblib_y_preds = loaded_joblib_model.predict(X_test) loaded_joblib_model_metrics = evaluate_preds(y_test, joblib_y_preds) loaded_joblib_model_metrics <pre>Acc: 88.52%\nPrecision: 0.88\nRecall: 0.91\nF1 score: 0.89\n</pre> Out[172]: <pre>{'accuracy': 0.89, 'precision': 0.88, 'recall': 0.91, 'f1': 0.89}</pre> <p>And once again, you'll notice the evaluation metrics are the same as before.</p> In\u00a0[173]: Copied! <pre>loaded_joblib_model_metrics == gs_metrics\n</pre> loaded_joblib_model_metrics == gs_metrics Out[173]: <pre>True</pre> <p>So which one should you use, <code>pickle</code> or <code>joblib</code>?</p> <p>According to Scikit-Learn's model persistence documentation, they suggest it may be more efficient to use <code>joblib</code> as it's more efficient with large numpy arrays (which is what may be contained in trained/fitted Scikit-Learn models).</p> In\u00a0[174]: Copied! <pre>data = pd.read_csv(\"../data/car-sales-extended-missing-data.csv\")\ndata.head()\n</pre> data = pd.read_csv(\"../data/car-sales-extended-missing-data.csv\") data.head() Out[174]: Make Colour Odometer (KM) Doors Price 0 Honda White 35431.0 4.0 15323.0 1 BMW Blue 192714.0 5.0 19943.0 2 Honda White 84714.0 4.0 28343.0 3 Toyota White 154365.0 4.0 13434.0 4 Nissan Blue 181577.0 3.0 14043.0 In\u00a0[175]: Copied! <pre>data.dtypes\n</pre> data.dtypes Out[175]: <pre>Make              object\nColour            object\nOdometer (KM)    float64\nDoors            float64\nPrice            float64\ndtype: object</pre> In\u00a0[176]: Copied! <pre>data.isna().sum()\n</pre> data.isna().sum() Out[176]: <pre>Make             49\nColour           50\nOdometer (KM)    50\nDoors            50\nPrice            50\ndtype: int64</pre> <p>There's 1000 rows, three features are categorical (<code>Make</code>, <code>Colour</code>, <code>Doors</code>), the other two are numerical (<code>Odometer (KM)</code>, <code>Price</code>) and there's 249 missing values.</p> <p>We're going to have to turn the categorical features into numbers and fill the missing values before we can fit a model.</p> <p>We'll build a <code>Pipeline</code> to do so.</p> <p><code>Pipeline</code>'s main input parameter is <code>steps</code> which is a list of tuples (<code>[(step_name, action_to_take)]</code>) of the step name, plus the action you'd like it to perform.</p> <p>In our case, you could think of the steps as:</p> <ol> <li>Fill missing data</li> <li>Convert data to numbers</li> <li>Build a model on the data</li> </ol> <p>Let's do it!</p> In\u00a0[177]: Copied! <pre># Getting data ready\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Modelling\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n# Setup random seed\nimport numpy as np\nnp.random.seed(42)\n\n# Import data and drop the rows with missing labels\ndata = pd.read_csv(\"../data/car-sales-extended-missing-data.csv\")\ndata.dropna(subset=[\"Price\"], inplace=True)\n\n# Define different features and transformer pipelines\ncategorical_features = [\"Make\", \"Colour\"]\ncategorical_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))])\n\ndoor_feature = [\"Doors\"]\ndoor_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=4))])\n\nnumeric_features = [\"Odometer (KM)\"]\nnumeric_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"mean\"))\n])\n\n# Setup preprocessing steps (fill missing values, then convert to numbers)\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"cat\", categorical_transformer, categorical_features),\n        (\"door\", door_transformer, door_feature),\n        (\"num\", numeric_transformer, numeric_features)])\n\n# Create a preprocessing and modelling pipeline\nmodel = Pipeline(steps=[(\"preprocessor\", preprocessor),\n                        (\"model\", RandomForestRegressor(n_jobs=-1))])\n\n# Split data\nX = data.drop(\"Price\", axis=1)\ny = data[\"Price\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Fit and score the model\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\n</pre> # Getting data ready import pandas as pd from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import OneHotEncoder  # Modelling from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split, GridSearchCV  # Setup random seed import numpy as np np.random.seed(42)  # Import data and drop the rows with missing labels data = pd.read_csv(\"../data/car-sales-extended-missing-data.csv\") data.dropna(subset=[\"Price\"], inplace=True)  # Define different features and transformer pipelines categorical_features = [\"Make\", \"Colour\"] categorical_transformer = Pipeline(steps=[     (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),     (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))])  door_feature = [\"Doors\"] door_transformer = Pipeline(steps=[     (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=4))])  numeric_features = [\"Odometer (KM)\"] numeric_transformer = Pipeline(steps=[     (\"imputer\", SimpleImputer(strategy=\"mean\")) ])  # Setup preprocessing steps (fill missing values, then convert to numbers) preprocessor = ColumnTransformer(     transformers=[         (\"cat\", categorical_transformer, categorical_features),         (\"door\", door_transformer, door_feature),         (\"num\", numeric_transformer, numeric_features)])  # Create a preprocessing and modelling pipeline model = Pipeline(steps=[(\"preprocessor\", preprocessor),                         (\"model\", RandomForestRegressor(n_jobs=-1))])  # Split data X = data.drop(\"Price\", axis=1) y = data[\"Price\"] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  # Fit and score the model model.fit(X_train, y_train) model.score(X_test, y_test) Out[177]: <pre>0.22188417408787875</pre> <p>What we've done is combine a series of data preprocessing steps (filling missing values, encoding numerical values) as well as a model into a <code>Pipeline</code>.</p> <p>Doing so not only cleans up the code, it ensures the same steps are taken every time the code is run rather than having multiple different processing steps happening in different stages.</p> <p>It's also possible to <code>GridSearchCV</code> or <code>RandomizedSearchCV</code> with a <code>Pipeline</code>.</p> <p>The main difference is when creating a hyperparameter grid, you have to add a prefix to each hyperparameter (see the documentation for <code>RandomForestRegressor</code> for a full list of possible hyperparameters to tune).</p> <p>The prefix is the name of the <code>Pipeline</code> step you'd like to alter, followed by two underscores.</p> <p>For example, to adjust <code>n_estimators</code> of <code>\"model\"</code> in the <code>Pipeline</code>, you'd use: <code>\"model__n_estimators\"</code> (note the double underscore after <code>model__</code> at the start).</p> <p>Let's see it!</p> <p>Note: Depending on your computer's processing power, the cell below may take a few minutes to run. For reference, it took about ~60 seconds on my M1 Pro MacBook Pro.</p> In\u00a0[178]: Copied! <pre># Using grid search with pipeline\npipe_grid = {\n    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"], # note the double underscore after each prefix \"preprocessor__\"\n    \"model__n_estimators\": [100, 1000],\n    \"model__max_depth\": [None, 5],\n    \"model__max_features\": [\"sqrt\"],\n    \"model__min_samples_split\": [2, 4]\n}\n\ngs_model = GridSearchCV(model, pipe_grid, cv=5, verbose=2)\ngs_model.fit(X_train, y_train)\n</pre> # Using grid search with pipeline pipe_grid = {     \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"], # note the double underscore after each prefix \"preprocessor__\"     \"model__n_estimators\": [100, 1000],     \"model__max_depth\": [None, 5],     \"model__max_features\": [\"sqrt\"],     \"model__min_samples_split\": [2, 4] }  gs_model = GridSearchCV(model, pipe_grid, cv=5, verbose=2) gs_model.fit(X_train, y_train) <pre>Fitting 5 folds for each of 16 candidates, totalling 80 fits\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean; total time=   0.1s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean; total time=   0.1s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean; total time=   0.1s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean; total time=   0.1s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean; total time=   0.1s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median; total time=   0.1s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median; total time=   0.1s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median; total time=   0.1s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median; total time=   0.1s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median; total time=   0.1s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean; total time=   0.9s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean; total time=   0.9s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean; total time=   0.8s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean; total time=   0.9s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean; total time=   0.9s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median; total time=   0.9s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median; total time=   0.9s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median; total time=   0.9s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median; total time=   0.9s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median; total time=   0.9s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean; total time=   0.1s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean; total time=   0.1s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean; total time=   0.1s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean; total time=   0.1s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean; total time=   0.1s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median; total time=   0.1s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median; total time=   0.1s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median; total time=   0.1s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median; total time=   0.1s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median; total time=   0.1s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean; total time=   0.8s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean; total time=   0.8s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean; total time=   0.8s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean; total time=   0.8s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean; total time=   0.9s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median; total time=   0.8s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median; total time=   0.8s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median; total time=   0.9s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median; total time=   0.8s\n[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median; total time=   0.8s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean; total time=   0.1s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean; total time=   0.1s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean; total time=   0.1s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean; total time=   0.1s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean; total time=   0.1s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median; total time=   0.1s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median; total time=   0.1s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median; total time=   0.1s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median; total time=   0.1s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median; total time=   0.1s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean; total time=   0.8s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean; total time=   0.7s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean; total time=   0.7s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean; total time=   0.7s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean; total time=   0.7s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median; total time=   0.7s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median; total time=   0.8s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median; total time=   0.8s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median; total time=   0.8s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median; total time=   0.8s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean; total time=   0.1s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean; total time=   0.1s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean; total time=   0.1s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean; total time=   0.1s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean; total time=   0.1s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median; total time=   0.1s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median; total time=   0.1s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median; total time=   0.1s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median; total time=   0.1s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median; total time=   0.1s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean; total time=   0.8s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean; total time=   0.8s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean; total time=   0.8s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean; total time=   0.7s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean; total time=   0.7s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median; total time=   0.7s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median; total time=   0.7s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median; total time=   0.7s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median; total time=   0.7s\n[CV] END model__max_depth=5, model__max_features=sqrt, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median; total time=   0.7s\n</pre> Out[178]: <pre>GridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('cat',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='constant')),\n                                                                                         ('onehot',\n                                                                                          OneHotEncoder(handle_unknown='ignore'))]),\n                                                                         ['Make',\n                                                                          'Colour']),\n                                                                        ('door',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(fill_value=4,\n                                                                                                        strategy='constant'))]),\n                                                                         ['Doors']),\n                                                                        ('num',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer())]),\n                                                                         ['Odometer '\n                                                                          '(KM)'])])),\n                                       ('model',\n                                        RandomForestRegressor(n_jobs=-1))]),\n             param_grid={'model__max_depth': [None, 5],\n                         'model__max_features': ['sqrt'],\n                         'model__min_samples_split': [2, 4],\n                         'model__n_estimators': [100, 1000],\n                         'preprocessor__num__imputer__strategy': ['mean',\n                                                                  'median']},\n             verbose=2)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV<pre>GridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('cat',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='constant')),\n                                                                                         ('onehot',\n                                                                                          OneHotEncoder(handle_unknown='ignore'))]),\n                                                                         ['Make',\n                                                                          'Colour']),\n                                                                        ('door',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(fill_value=4,\n                                                                                                        strategy='constant'))]),\n                                                                         ['Doors']),\n                                                                        ('num',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer())]),\n                                                                         ['Odometer '\n                                                                          '(KM)'])])),\n                                       ('model',\n                                        RandomForestRegressor(n_jobs=-1))]),\n             param_grid={'model__max_depth': [None, 5],\n                         'model__max_features': ['sqrt'],\n                         'model__min_samples_split': [2, 4],\n                         'model__n_estimators': [100, 1000],\n                         'preprocessor__num__imputer__strategy': ['mean',\n                                                                  'median']},\n             verbose=2)</pre>estimator: Pipeline<pre>Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Make', 'Colour']),\n                                                 ('door',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(fill_value=4,\n                                                                                 strategy='constant'))]),\n                                                  ['Doors']),\n                                                 ('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer())]),\n                                                  ['Odometer (KM)'])])),\n                ('model', RandomForestRegressor(n_jobs=-1))])</pre>preprocessor: ColumnTransformer<pre>ColumnTransformer(transformers=[('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['Make', 'Colour']),\n                                ('door',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(fill_value=4,\n                                                                strategy='constant'))]),\n                                 ['Doors']),\n                                ('num',\n                                 Pipeline(steps=[('imputer', SimpleImputer())]),\n                                 ['Odometer (KM)'])])</pre>cat<pre>['Make', 'Colour']</pre>SimpleImputer<pre>SimpleImputer(fill_value='missing', strategy='constant')</pre>OneHotEncoder<pre>OneHotEncoder(handle_unknown='ignore')</pre>door<pre>['Doors']</pre>SimpleImputer<pre>SimpleImputer(fill_value=4, strategy='constant')</pre>num<pre>['Odometer (KM)']</pre>SimpleImputer<pre>SimpleImputer()</pre>RandomForestRegressor<pre>RandomForestRegressor(n_jobs=-1)</pre> <p>Now let's find the score of our model (by default <code>GridSearchCV</code> saves the best model to the <code>gs_model</code> object).</p> In\u00a0[179]: Copied! <pre># Score the best model\ngs_model.score(X_test, y_test)\n</pre> # Score the best model gs_model.score(X_test, y_test) Out[179]: <pre>0.2848784564026805</pre> <p>Beautiful!</p> <p>Using <code>GridSearchCV</code> we see a nice boost in our models score.</p> <p>And the best thing is, because it's all in a <code>Pipeline</code>, we could easily replicate these results.</p>"},{"location":"introduction-to-scikit-learn/#a-quick-machine-learning-modelling-tutorial-with-python-and-scikit-learn","title":"A Quick Machine Learning Modelling Tutorial with Python and Scikit-Learn\u00b6","text":"<p>This notebook goes through a range of common and useful featues of the Scikit-Learn library.</p> <p>There's a bunch here but I'm calling it quick because of how vast the Scikit-Learn library is.</p> <p>Covering everything requires a full-blown documentation, of which, if you ever get stuck, I'd highly recommend checking out.</p>"},{"location":"introduction-to-scikit-learn/#what-is-scikit-learn-sklearn","title":"What is Scikit-Learn (sklearn)?\u00b6","text":"<p>Scikit-Learn, also referred to as <code>sklearn</code>, is an open-source Python machine learning library.</p> <p>It's built on top on NumPy (Python library for numerical computing) and Matplotlib (Python library for data visualization).</p> <p></p>"},{"location":"introduction-to-scikit-learn/#why-scikit-learn","title":"Why Scikit-Learn?\u00b6","text":"<p>Although the fields of data science and machine learning are vast, the main goal is finding patterns within data and then using those patterns to make predictions.</p> <p>And there are certain categories which a majority of problems fall into.</p> <p>If you're trying to create a machine learning model to predict whether an email is spam and or not spam, you're working on a classification problem (whether something is one thing or another).</p> <p>If you're trying to create a machine learning model to predict the price of houses given their characteristics, you're working on a regression problem (predicting a number).</p> <p>If you're trying to get a machine learning algorithm to group together similar samples (that you don't necessarily know which should go together), you're working on a clustering problem.</p> <p>Once you know what kind of problem you're working on, there are also similar steps you'll take for each. Steps like splitting the data into different sets, one for your machine learning algorithms to learn on (the training set) and another to test them on (the testing set).</p> <p>Choosing a machine learning model and then evaluating whether or not your model has learned anything.</p> <p>Scikit-Learn offers Python implementations for doing all of these kinds of tasks (from preparing data to modelling data). Saving you from having to build them from scratch.</p>"},{"location":"introduction-to-scikit-learn/#what-does-this-notebook-cover","title":"What does this notebook cover?\u00b6","text":"<p>The Scikit-Learn library is very capable. However, learning everything off by heart isn't necessary. Instead, this notebook focuses some of the main use cases of the library.</p> <p>More specifically, we'll cover:</p> <p></p> <ol> <li>An end-to-end Scikit-Learn worfklow</li> <li>Getting the data ready</li> <li>Choosing the right maching learning estimator/aglorithm/model for your problem</li> <li>Fitting your chosen machine learning model to data and using it to make a prediction</li> <li>Evaluting a machine learning model</li> <li>Improving predictions through experimentation (hyperparameter tuning)</li> <li>Saving and loading a pretrained model</li> <li>Putting it all together in a pipeline</li> </ol> <p>Note: All of the steps in this notebook are focused on supervised learning (having data and labels). The other side of supervised learning is unsupervised learning (having data but no labels).</p> <p>After going through it, you'll have the base knolwedge of Scikit-Learn you need to keep moving forward.</p>"},{"location":"introduction-to-scikit-learn/#where-can-i-get-help","title":"Where can I get help?\u00b6","text":"<p>If you get stuck or think of something you'd like to do which this notebook doesn't cover, don't fear!</p> <p>The recommended steps you take are:</p> <ol> <li>Try it - Since Scikit-Learn has been designed with usability in mind, your first step should be to use what you know and try figure out the answer to your own question (getting it wrong is part of the process). If in doubt, run your code.</li> <li>Press SHIFT+TAB - See you can the docstring of a function (information on what the function does) by pressing SHIFT + TAB inside it. Doing this is a good habit to develop. It'll improve your research skills and give you a better understanding of the library.</li> <li>Search for it - If trying it on your own doesn't work, since someone else has probably tried to do something similar, try searching for your problem. You'll likely end up in 1 of 2 places:<ul> <li>Scikit-Learn documentation/user guide - the most extensive resource you'll find for Scikit-Learn information.</li> <li>Stack Overflow - this is the developers Q&amp;A hub, it's full of questions and answers of different problems across a wide range of software development topics and chances are, there's one related to your problem.</li> <li>ChatGPT - ChatGPT is very good at explaining code, however, it can make mistakes. Best to verify the code it writes first before using it. Try asking \"Can you explain the following code for me? {your code here}\" and then continue with follow up questions from there.</li> </ul> </li> </ol> <p>An example of searching for a Scikit-Learn solution might be:</p> <p>\"how to tune the hyperparameters of a sklearn model\"</p> <p>Searching this on Google leads to the Scikit-Learn documentation for the <code>GridSearchCV</code> function: http://scikit-learn.org/stable/modules/grid_search.html</p> <p>The next steps here are to read through the documentation, check the examples and see if they line up to the problem you're trying to solve. If they do, rewrite the code to suit your needs, run it, and see what the outcomes are.</p> <ol> <li>Ask for help - If you've been through the above 3 steps and you're still stuck, you might want to ask your question on Stack Overflow or in the ZTM Machine Learning and AI Discord channel. Be as specific as possible and provide details on what you've tried.</li> </ol> <p>Remember, you don't have to learn all of the functions off by heart to begin with.</p> <p>What's most important is continually asking yourself, \"what am I trying to do with the data?\".</p> <p>Start by answering that question and then practicing finding the code which does it.</p> <p>Let's get started.</p> <p>First we'll import the libraries we've been using previously.</p> <p>We'll also check the version of <code>sklearn</code> we've got.</p>"},{"location":"introduction-to-scikit-learn/#0-an-end-to-end-scikit-learn-workflow","title":"0. An end-to-end Scikit-Learn workflow\u00b6","text":"<p>Before we get in-depth, let's quickly check out what an end-to-end Scikit-Learn workflow might look like.</p> <p>Once we've seen an end-to-end workflow, we'll dive into each step a little deeper.</p> <p>Specifically, we'll get hands-on with the following steps:</p> <ol> <li>Getting data ready (split into features and labels, prepare train and test steps)</li> <li>Choosing a model for our problem</li> <li>Fit the model to the data and use it to make a prediction</li> <li>Evaluate the model</li> <li>Experiment to improve</li> <li>Save a model for someone else to use</li> </ol> <p>Note: The following section is a bit information heavy but it is an end-to-end workflow. We'll go through it quite swiftly but we'll break it down more throughout the rest of the notebook. And since Scikit-Learn is such a vast library, capable of tackling many problems, the workflow we're using is only one example of how you can use it.</p>"},{"location":"introduction-to-scikit-learn/#random-forest-classifier-workflow-for-classifying-heart-disease","title":"Random Forest Classifier Workflow for Classifying Heart Disease\u00b6","text":""},{"location":"introduction-to-scikit-learn/#1-get-the-data-ready","title":"1. Get the data ready\u00b6","text":"<p>As an example dataset, we'll import <code>heart-disease.csv</code>.</p> <p>This file contains anonymised patient medical records and whether or not they have heart disease or not (this is a classification problem since we're trying to predict whether something is one thing or another).</p>"},{"location":"introduction-to-scikit-learn/#2-choose-the-model-and-hyperparameters","title":"2. Choose the model and hyperparameters\u00b6","text":"<p>Choosing a model often depends on the type of problem you're working on.</p> <p>For example, there are different models that Scikit-Learn recommends whether you're working on a classification or regression problem.</p> <p>You can see a map breaking down the different kinds of model options and recommendations in the Scikit-Learn documentation.</p> <p>Scikit-Learn refers to models as \"estimators\", however, they are often also referred to as <code>model</code> or <code>clf</code> (short for classifier).</p> <p>A model's hyperparameters are settings you can change to adjust it for your problem, much like knobs on an oven you can tune to cook your favourite dish.</p>"},{"location":"introduction-to-scikit-learn/#3-fit-the-model-to-the-data-and-use-it-to-make-a-prediction","title":"3. Fit the model to the data and use it to make a prediction\u00b6","text":"<p>Fitting a model a dataset involves passing it the data and asking it to figure out the patterns.</p> <p>If there are labels (supervised learning), the model tries to work out the relationship between the data and the labels.</p> <p>If there are no labels (unsupervised learning), the model tries to find patterns and group similar samples together.</p> <p>Most Scikit-Learn models have the <code>fit(X, y)</code> method built-in, where the <code>X</code> parameter is the features and the <code>y</code> parameter is the labels.</p> <p>In our case, we start by fitting a model on the training split (<code>X_train</code>, <code>y_train</code>).</p>"},{"location":"introduction-to-scikit-learn/#use-the-model-to-make-a-prediction","title":"Use the model to make a prediction\u00b6","text":"<p>The whole point of training a machine learning model is to use it to make some kind of prediction in the future.</p> <p>Once your model instance is trained, you can use the <code>predict()</code> method to predict a target value given a set of features.</p> <p>In other words, use the model, along with some new, unseen and unlabelled data to predict the label.</p> <p>Note: Data you predict on should be in the same shape and format as data you trained on.</p>"},{"location":"introduction-to-scikit-learn/#4-evaluate-the-model","title":"4. Evaluate the model\u00b6","text":"<p>Now we've made some predictions, we can start to use some more Scikit-Learn methods to figure out how good our model is.</p> <p>Each model or estimator has a built-in <code>score()</code> method.</p> <p>This method compares how well the model was able to learn the patterns between the features and labels.</p> <p>The <code>score()</code> method for each model uses a standard evaluation metric to measure your model's results.</p> <p>In the case of a classifier (our model), one of the most common evaluation metrics is accuracy (the fraction of correct predictions out of total predictions).</p> <p>Let's check out our model's accuracy on the training set.</p>"},{"location":"introduction-to-scikit-learn/#5-experiment-to-improve","title":"5. Experiment to improve\u00b6","text":"<p>The first model you build is often referred to as a baseline (a baseline is often even simpler than the model we've used, a baseline could be \"let's just by default predict the most common value and then try to improve\").</p> <p>Once you've got a baseline model, like we have here, it's important to remember, this is often not the final model you'll use.</p> <p>The next step in the workflow is to try and improve upon your baseline model.</p> <p>How?</p> <p>With one of the most important mottos in machine learning...</p> <p>Experiment, experiment, experiment!</p> <p>Experiments can come in many different forms.</p> <p>But let's break it into two.</p> <ol> <li>From a model perspective.</li> <li>From a data perspective.</li> </ol> <p>From a model perspective may involve things such as using a more complex model or tuning your models hyperparameters.</p> <p>From a data perspective may involve collecting more data or better quality data so your existing model has more of a chance to learn the patterns within.</p> <p>If you're already working on an existing dataset, it's often easier try a series of model perspective experiments first and then turn to data perspective experiments if you aren't getting the results you're looking for.</p> <p>One thing you should be aware of is if you're tuning a models hyperparameters in a series of experiments, your reuslts should always be cross-validated (we'll see this later on!).</p> <p>Cross-validation is a way of making sure the results you're getting are consistent across your training and test datasets (because it uses multiple versions of training and test sets) rather than just luck because of the order the original training and test sets were created.</p> <ul> <li>Try different hyperparameters.</li> <li>All different parameters should be cross-validated.<ul> <li>Note: Beware of cross-validation for time series problems (as for time series, you don't want to mix samples from the future with samples from the past).</li> </ul> </li> </ul> <p>Different models you use will have different hyperparameters you can tune.</p> <p>For the case of our model, the <code>RandomForestClassifier()</code>, we'll start trying different values for <code>n_estimators</code> (a measure for the number of trees in the random forest).</p> <p>By default, <code>n_estimators=100</code>, so how about we try values from <code>100</code> to <code>200</code> and see what happens (generally more is better)?</p>"},{"location":"introduction-to-scikit-learn/#6-save-a-model-for-someone-else-to-use","title":"6. Save a model for someone else to use\u00b6","text":"<p>When you've done a few experiments and you're happy with how your model is doing, you'll likely want someone else to be able to use it.</p> <p>This may come in the form of a teammate or colleague trying to replicate and validate your results or through a customer using your model as part of a service or application you offer.</p> <p>Saving a model also allows you to reuse it later without having to go through retraining it. Which is helpful, especially when your training times start to increase.</p> <p>You can save a Scikit-Learn model using Python's in-built <code>pickle</code> module.</p>"},{"location":"introduction-to-scikit-learn/#1-getting-the-data-ready","title":"1. Getting the data ready\u00b6","text":"<p>Data doesn't always come ready to use with a Scikit-Learn machine learning model.</p> <p>Three of the main steps you'll often have to take are:</p> <ul> <li>Splitting the data into features (usually <code>X</code>) and labels (usually <code>y</code>).</li> <li>Splitting the data into training and testing sets (and possibly a validation set).</li> <li>Filling (also called imputing) or disregarding missing values.</li> <li>Converting non-numerical values to numerical values (also call feature encoding).</li> </ul> <p>Let's see an example.</p>"},{"location":"introduction-to-scikit-learn/#11-make-sure-its-all-numerical","title":"1.1 Make sure it's all numerical\u00b6","text":"<p>Computers love numbers.</p> <p>So one thing you'll often have to make sure of is that your datasets are in numerical form.</p> <p>This even goes for datasets which contain non-numerical features that you may want to include in a model.</p> <p>For example, if we were working with a car sales dataset, how might we turn features such as <code>Make</code> and <code>Colour</code> into numbers?</p> <p>Let's figure it out.</p> <p>First, we'll import the <code>car-sales-extended.csv</code> dataset.</p>"},{"location":"introduction-to-scikit-learn/#111-nuemrically-encoding-data-with-pandas","title":"1.1.1 Nuemrically encoding data with pandas\u00b6","text":"<p>Another way we can numerically encode data is directly with pandas.</p> <p>We can use the <code>pandas.get_dummies()</code> (or <code>pd.get_dummies()</code> for short) method and then pass it our target columns.</p> <p>In return, we'll get a one-hot encoded version of our target columns.</p> <p>Let's remind ourselves of what our DataFrame looks like.</p>"},{"location":"introduction-to-scikit-learn/#12-what-if-there-were-missing-values-in-the-data","title":"1.2 What if there were missing values in the data?\u00b6","text":"<p>Holes in the data means holes in the patterns your machine learning model can learn.</p> <p>Many machine learning models don't work well or produce errors when they're used on datasets with missing values.</p> <p>A missing value can appear as a blank, as a <code>NaN</code> or something similar.</p> <p>There are two main options when dealing with missing values:</p> <ol> <li>Fill them with some given or calculated value (imputation) - For example, you might fill missing values of a numerical column with the mean of all the other values. The practice of calculating or figuring out how to fill missing values in a dataset is called imputing. For a great resource on imputing missing values, I'd recommend refering to the Scikit-Learn user guide.</li> <li>Remove them - If a row or sample has missing values, you may opt to remove them from your dataset completely. However, this potentially results in using less data to build your model.</li> </ol> <p>Note: Dealing with missing values differs from problem to problem, meaning there's no 100% best way to fill missing values across datasets and problem types. It will often take careful experimentation and practice to figure out the best way to deal with missing values in your own datasets.</p> <p>To practice dealing with missing values, let's import a version of the <code>car_sales</code> dataset with several missing values.</p>"},{"location":"introduction-to-scikit-learn/#121-fill-missing-data-with-pandas","title":"1.2.1 Fill missing data with pandas\u00b6","text":"<p>Let's see how we might fill missing values with pandas.</p> <p>For categorical values, one of the simplest ways is to fill the missing fields with the string <code>\"missing\"</code>.</p> <p>We could do this for the <code>Make</code> and <code>Colour</code> features.</p> <p>As for the <code>Doors</code> feature, we could use <code>\"missing\"</code> or we could fill it with the most common option of <code>4</code>.</p> <p>With the <code>Odometer (KM)</code> feature, we can use the mean value of all the other values in the column.</p> <p>And finally, for those samples which are missing a <code>Price</code> value, we can remove them (since <code>Price</code> is the target value, removing probably causes less harm than imputing, however, you could design an experiment to test this).</p> <p>In summary:</p> Column/Feature Fill missing value with <code>Make</code> <code>\"missing\"</code> <code>Colour</code> <code>\"missing\"</code> <code>Doors</code> 4 (most common value) <code>Odometer (KM)</code> mean of <code>Odometer (KM)</code> <code>Price</code> (target) NA, remove samples missing <code>Price</code> <p>Note: The practice of filling missing data with given or calculated values is called imputation. And it's important to remember there's no perfect way to fill missing data (unless it's with data that should've actually been there in the first place). The methods we're using are only one of many. The techniques you use will depend heavily on your dataset. A good place to look would be searching for \"data imputation techniques\".</p> <p>Let's start with the <code>Make</code> column.</p> <p>We can use the pandas method <code>fillna(value=\"missing\", inplace=True)</code> to fill all the missing values with the string <code>\"missing\"</code>.</p>"},{"location":"introduction-to-scikit-learn/#122-filling-missing-data-and-transforming-categorical-data-with-scikit-learn","title":"1.2.2 Filling missing data and transforming categorical data with Scikit-Learn\u00b6","text":"<p>Now we've filled the missing columns using pandas functions, you might be thinking, \"Why pandas? I thought this was a Scikit-Learn introduction?\".</p> <p>Not to worry, Scikit-Learn provides a class called <code>sklearn.impute.SimpleImputer()</code> which allows us to do a similar thing.</p> <p><code>SimpleImputer()</code> transforms data by filling missing values with a given <code>strategy</code> parameter.</p> <p>And we can use it to fill the missing values in our DataFrame as above.</p> <p>At the moment, our dataframe has no mising values.</p>"},{"location":"introduction-to-scikit-learn/#2-choosing-the-right-estimatoralgorithm-for-your-problem","title":"2. Choosing the right estimator/algorithm for your problem\u00b6","text":"<p>Once you've got your data ready, the next step is to choose an appropriate machine learning algorithm or model to find patterns in your data.</p> <p>Some things to note:</p> <ul> <li>Scikit-Learn refers to machine learning models and algorithms as estimators.</li> <li>Classification problem - predicting a category (heart disease or not).<ul> <li>Sometimes you'll see <code>clf</code> (short for classifier) used as a classification estimator instance's variable name.</li> </ul> </li> <li>Regression problem - predicting a number (selling price of a car).</li> <li>Unsupervised problem (data with no labels) - clustering (grouping unlabelled samples with other similar unlabelled samples).</li> </ul> <p>If you know what kind of problem you're working with, one of the next places you should look at is the Scikit-Learn algorithm cheatsheet.</p> <p>This cheatsheet gives you a bit of an insight into the algorithm you might want to use for the problem you're working on.</p> <p>It's important to remember, you don't have to explicitly know what each algorithm is doing on the inside to start using them.</p> <p>If you start to apply different algorithms but they don't seem to be working (not performing as well as you'd like), that's when you'd start to look deeper into each one.</p> <p>Let's check out the cheatsheet and follow it for some of the problems we're working on.</p> <p>You can see it's split into four main categories. Regression, classification, clustering and dimensionality reduction. Each has their own different purpose but the Scikit-Learn team has designed the library so the workflows for each are relatively similar.</p>"},{"location":"introduction-to-scikit-learn/#21-picking-a-machine-learning-model-for-a-regression-problem","title":"2.1 Picking a machine learning model for a regression problem\u00b6","text":"<p>Let's start with a regression problem (trying to predict a number). We'll use the California Housing dataset built into Scikit-Learn's <code>datasets</code> module.</p> <p>The goal of the California Housing dataset is to predict a given district's median house value (in hundreds of thousands of dollars) on things like the age of the home, the number of rooms, the number of bedrooms, number of people living the home and more.</p>"},{"location":"introduction-to-scikit-learn/#22-picking-a-machine-learning-model-for-a-classification-problem","title":"2.2 Picking a machine learning model for a classification problem\u00b6","text":"<p>Now, let's check out the choosing process for a classification problem.</p> <p>Say you were trying to predict whether or not a patient had heart disease based on their medical records.</p> <p>The dataset in <code>../data/heart-disease.csv</code> contains data for just that problem.</p>"},{"location":"introduction-to-scikit-learn/#what-about-the-other-models","title":"What about the other models?\u00b6","text":"<p>Looking at the Scikit-Learn aglorithm cheat-sheet and the examples above, you may have noticed we've skipped a few models.</p> <p>Why?</p> <p>The first reason is time.</p> <p>Covering every single one would take a fair bit longer than what we've done here. And the second one is the effectiveness of ensemble methods.</p> <p>A little tidbit for modelling in machine learning is:</p> <ul> <li>If you have structured data (tables, spreadsheets or dataframes), use ensemble methods, such as, a Random Forest.</li> <li>If you have unstructured data (text, images, audio, things not in tables), use deep learning or transfer learning (see the ZTM TensorFlow and PyTorch courses for more on deep learning).</li> </ul> <p>For this notebook, we're focused on structured data, which is why the Random Forest has been our model of choice.</p> <p>If you'd like to learn more about the Random Forest and why it's the war horse of machine learning, check out these resources:</p> <ul> <li>Random Forest Wikipedia</li> <li>An Implementation and Explanation of the Random Forest in Python by Will Koehrsen</li> </ul>"},{"location":"introduction-to-scikit-learn/#experiment-until-something-works","title":"Experiment until something works\u00b6","text":"<p>The beautiful thing is, the way the Scikit-Learn API is designed, once you know the way with one model, using another is much the same.</p> <p>And since a big part of being a machine learning engineer or data scientist is experimenting, you might want to try out some of the other models on the cheat-sheet and see how you go. The more you can reduce the time between experiments, the better.</p>"},{"location":"introduction-to-scikit-learn/#3-fit-the-model-to-data-and-using-it-to-make-predictions","title":"3. Fit the model to data and using it to make predictions\u00b6","text":"<p>Now you've chosen a model, the next step is to have it learn from the data so it can be used for predictions in the future.</p> <p>If you've followed through, you've seen a few examples of this already.</p>"},{"location":"introduction-to-scikit-learn/#31-fitting-a-model-to-data","title":"3.1 Fitting a model to data\u00b6","text":"<p>In Scikit-Learn, the process of having a machine learning model learn patterns from a dataset involves calling the <code>fit()</code> method and passing it data, such as, <code>fit(X, y)</code>.</p> <p>Where <code>X</code> is a feature array and <code>y</code> is a target array.</p> <p>Other names for <code>X</code> include:</p> <ul> <li>Data</li> <li>Feature variables</li> <li>Features</li> </ul> <p>Other names for <code>y</code> include:</p> <ul> <li>Labels</li> <li>Target variable</li> </ul> <p>For supervised learning there is usually an <code>X</code> and <code>y</code>.</p> <p>For unsupervised learning, there's no <code>y</code> (no labels).</p> <p>Let's revisit the example of using patient data (<code>X</code>) to predict whether or not they have heart disease (<code>y</code>).</p>"},{"location":"introduction-to-scikit-learn/#32-making-predictions-using-a-machine-learning-model","title":"3.2 Making predictions using a machine learning model\u00b6","text":"<p>Now we've got a trained model, one which has hoepfully learned patterns in the data, you'll want to use it to make predictions.</p> <p>Scikit-Learn enables this in several ways.</p> <p>Two of the most common and useful are <code>predict()</code> and <code>predict_proba()</code>.</p> <p>Let's see them in action.</p>"},{"location":"introduction-to-scikit-learn/#4-evaluating-a-model","title":"4. Evaluating a model\u00b6","text":"<p>Once you've trained a model, you'll want a way to measure how trustworthy its predictions are.</p> <p>Across the board, the main idea of evaluating a model is to compare the model's predictions to what they should've ideally been (the truth labels).</p> <p>Scikit-Learn implements 3 different methods of evaluating models.</p> <ol> <li>The <code>score()</code> method. Calling <code>score()</code> on a model instance will return a metric assosciated with the type of model you're using. The metric depends on which model you're using.</li> <li>The <code>scoring</code> parameter. This parameter can be passed to methods such as <code>cross_val_score()</code> or <code>GridSearchCV()</code> to tell Scikit-Learn to use a specific type of scoring metric.</li> <li>Problem-specific metric functions available in <code>sklearn.metrics</code>. Similar to how the <code>scoring</code> parameter can be passed different scoring functions, Scikit-Learn implements these as stand alone functions.</li> </ol> <p>The scoring function you use will also depend on the problem you're working on.</p> <p>Classification problems have different evaluation metrics and scoring functions to regression problems.</p> <p>Let's look at some examples.</p>"},{"location":"introduction-to-scikit-learn/#41-general-model-evaluation-with-score","title":"4.1 General model evaluation with <code>score()</code>\u00b6","text":"<p>If we bring down the code from our previous classification problem (building a classifier to predict whether or not someone has heart disease based on their medical records).</p> <p>We can see the <code>score()</code> method come into play.</p>"},{"location":"introduction-to-scikit-learn/#42-evaluating-your-models-using-the-scoring-parameter","title":"4.2 Evaluating your models using the <code>scoring</code> parameter\u00b6","text":"<p>The next step up from using <code>score()</code> is to use a custom <code>scoring</code> parameter with <code>cross_val_score()</code> or <code>GridSearchCV</code>.</p> <p>As you may have guessed, the <code>scoring</code> parameter you set will be different depending on the problem you're working on.</p> <p>We'll see some specific examples of different parameters in a moment but first let's check out <code>cross_val_score()</code>.</p> <p>To do so, we'll copy the heart disease classification code from above and then add another line at the top.</p>"},{"location":"introduction-to-scikit-learn/#421-classification-model-evaluation-metrics","title":"4.2.1 Classification model evaluation metrics\u00b6","text":"<p>Four of the main evaluation metrics/methods you'll come across for classification models are:</p> <ol> <li>Accuracy</li> <li>Area under ROC curve (receiver operating characteristic curve)</li> <li>Confusion matrix</li> <li>Classification report</li> </ol> <p>Let's have a look at each of these. We'll bring down the classification code from above to go through some examples.</p>"},{"location":"introduction-to-scikit-learn/#accuracy","title":"Accuracy\u00b6","text":"<p>Accuracy is the default metric for the <code>score()</code> function within each of Scikit-Learn's classifier models. And it's probably the metric you'll see most often used for classification problems.</p> <p>However, we'll see in a second how it may not always be the best metric to use.</p> <p>Scikit-Learn returns accuracy as a decimal but you can easily convert it to a percentage.</p>"},{"location":"introduction-to-scikit-learn/#area-under-receiver-operating-characteristic-roc-curve","title":"Area Under Receiver Operating Characteristic (ROC) Curve\u00b6","text":"<p>If this one sounds like a mouthful, its because reading the full name is.</p> <p>It's usually referred to as AUC for Area Under Curve and the curve they're talking about is the Receiver Operating Characteristic or ROC for short.</p> <p>So if hear someone talking about AUC or ROC, they're probably talking about what follows.</p> <p>ROC curves are a comparison of true postive rate (tpr) versus false positive rate (fpr).</p> <p>For clarity:</p> <ul> <li>True positive = model predicts 1 when truth is 1</li> <li>False positive = model predicts 1 when truth is 0</li> <li>True negative = model predicts 0 when truth is 0</li> <li>False negative = model predicts 0 when truth is 1</li> </ul> <p>Now we know this, let's see one. Scikit-Learn lets you calculate the information required for a ROC curve using the <code>roc_curve</code> function.</p>"},{"location":"introduction-to-scikit-learn/#confusion-matrix","title":"Confusion matrix\u00b6","text":"<p>Another fantastic way to evaluate a classification model is by using a confusion matrix.</p> <p>A confusion matrix is a quick way to compare the labels a model predicts and the actual labels it was supposed to predict.</p> <p>In essence, giving you an idea of where the model is getting confused.</p>"},{"location":"introduction-to-scikit-learn/#creating-a-confusion-matrix-using-scikit-learn","title":"Creating a confusion matrix using Scikit-Learn\u00b6","text":"<p>Scikit-Learn has multiple different implementations of plotting confusion matrices:</p> <ol> <li><code>sklearn.metrics.ConfusionMatrixDisplay.from_estimator(estimator, X, y)</code> - this takes a fitted estimator (like our <code>clf</code> model), features (<code>X</code>) and labels (<code>y</code>), it then uses the trained estimator to make predictions on <code>X</code> and compares the predictions to <code>y</code> by displaying a confusion matrix.</li> <li><code>sklearn.metrics.ConfusionMatrixDisplay.from_predictions(y_true, y_pred)</code> - this takes truth labels and predicted labels and compares them by displaying a confusion matrix.</li> </ol> <p>Note: Both of these methods/classes require Scikit-Learn 1.0+. To check your version of Scikit-Learn run:</p> <pre>import sklearn\nsklearn.__version__\n</pre> <p>If you don't have 1.0+, you can upgrade at: https://scikit-learn.org/stable/install.html</p>"},{"location":"introduction-to-scikit-learn/#classification-report","title":"Classification report\u00b6","text":"<p>The final major metric you should consider when evaluating a classification model is a classification report.</p> <p>A classification report is more so a collection of metrics rather than a single one.</p> <p>You can create a classification report using Scikit-Learn's sklearn.metrics.classification_report` method.</p> <p>Let's see one.</p>"},{"location":"introduction-to-scikit-learn/#422-regression-model-evaluation-metrics","title":"4.2.2 Regression model evaluation metrics\u00b6","text":"<p>Similar to classification, there are several metrics you can use to evaluate your regression models.</p> <p>We'll check out the following.</p> <ol> <li>R^2 (pronounced r-squared) or coefficient of determination - Compares your models predictions to the mean of the targets. Values can range from negative infinity (a very poor model) to 1. For example, if all your model does is predict the mean of the targets, its R^2 value would be 0. And if your model perfectly predicts a range of numbers it's R^2 value would be 1. Higher is better.</li> <li>Mean absolute error (MAE) - The average of the absolute differences between predictions and actual values. It gives you an idea of how wrong your predictions were. Lower is better.</li> <li>Mean squared error (MSE) - The average squared differences between predictions and actual values. Squaring the errors removes negative errors. It also amplifies outliers (samples which have larger errors). Lower is better.</li> </ol> <p>Let's see them in action. First, we'll bring down our regression model code again.</p>"},{"location":"introduction-to-scikit-learn/#423-evaluating-a-model-using-the-scoring-parameter","title":"4.2.3 Evaluating a model using the <code>scoring</code> parameter\u00b6","text":"<p>We've covered a bunch of ways to evaluate a model's predictions but haven't even touched the <code>scoring</code> parameter...</p> <p>Not to worry, it's very similar to what we've been doing!</p> <p>As a refresh, the <code>scoring</code> parameter can be used with a function like <code>cross_val_score()</code> to tell Scikit-Learn what evaluation metric to return using cross-validation.</p> <p>Let's check it out with our classification model and the heart disease dataset.</p>"},{"location":"introduction-to-scikit-learn/#43-using-different-evaluation-metrics-with-scikit-learn","title":"4.3 Using different evaluation metrics with Scikit-Learn\u00b6","text":"<p>Remember the third way of evaluating Scikit-Learn functions?</p> <ol> <li>Problem-specific metric functions. Similar to how the <code>scoring</code> parameter can be passed different scoring functions, Scikit-Learn implements these as stand alone functions.</li> </ol> <p>Well, we've kind of covered this third way of using evaulation metrics with Scikit-Learn.</p> <p>In essence, all of the metrics we've seen previously have their own function in Scikit-Learn.</p> <p>They all work by comparing an array of predictions, usually called <code>y_preds</code> to an array of actual labels, usually called <code>y_test</code> or <code>y_true</code>.</p>"},{"location":"introduction-to-scikit-learn/#classification-functions","title":"Classification functions\u00b6","text":"<p>For:</p> <ul> <li>Accuracy we can use <code>sklearn.metrics.accuracy_score</code></li> <li>Precision we can use <code>sklearn.metrics.precision_score</code></li> <li>Recall we can use <code>sklearn.metrics.recall_score</code></li> <li>F1 we can use <code>sklearn.metrics.f1_score</code></li> </ul>"},{"location":"introduction-to-scikit-learn/#regression-metrics","title":"Regression metrics\u00b6","text":"<p>We can use a similar setup for our regression problem, just with different methods.</p> <p>For:</p> <ul> <li>R^2 we can use <code>sklearn.metrics.r2_score</code></li> <li>MAE (mean absolute error) we can use <code>sklearn.metrics.mean_absolute_error</code></li> <li>MSE (mean squared error) we can use <code>sklearn.metrics.mean_squared_error</code></li> </ul>"},{"location":"introduction-to-scikit-learn/#5-improving-model-predictions-through-experimentation-hyperparameter-tuning","title":"5. Improving model predictions through experimentation (hyperparameter tuning)\u00b6","text":"<p>The first predictions you make with a model are generally referred to as baseline predictions.</p> <p>It's similar for the first evaluation metrics you get. These are generally referred to as baseline metrics.</p> <p>Your next goal is to improve upon these baseline metrics.</p> <p>How?</p> <p>Experiment, experiment, experiment!</p> <p>Two of the main methods to improve baseline metrics are:</p> <ol> <li>From a data perspective.</li> <li>From a model perspective.</li> </ol> <p>From a data perspective asks:</p> <ul> <li>Could we collect more data? In machine learning, more data is generally better, as it gives a model more opportunities to learn patterns.</li> <li>Could we improve our data? This could mean filling in misisng values or finding a better encoding (turning data into numbers) strategy.</li> </ul> <p>From a model perspective asks:</p> <ul> <li>Is there a better model we could use? If you've started out with a simple model, could you use a more complex one? (we saw an example of this when looking at the Scikit-Learn machine learning map, ensemble methods are generally considered more complex models)</li> <li>Could we improve the current model? If the model you're using performs well straight out of the box, can the hyperparameters be tuned to make it even better?</li> </ul> <p>Note: Patterns in data are also often referred to as data parameters. The difference between parameters and hyperparameters is a machine learning model seeks to find parameters in data on its own, where as, hyperparameters are settings on a model which a person (you) can adjust.</p> <p>Since we have two existing datasets, we'll look at improving our results from a model perspective.</p> <p>More specifically, we'll look at how we could improve our <code>RandomForestClassifier</code> and <code>RandomForestRegressor</code> models through hyperparameter tuning.</p> <p>What even are hyperparameters?</p> <p>Good question, let's check them out.</p> <p>First, we'll instantiate a <code>RandomForestClassifier</code>.</p>"},{"location":"introduction-to-scikit-learn/#51-tuning-hyperparameters-by-hand","title":"5.1 Tuning hyperparameters by hand\u00b6","text":"<p>So far we've worked with training and test datasets.</p> <p>You train a model on a training set and evaluate it on a test dataset.</p> <p>But hyperparameter tuning introduces a thrid set, a validation set.</p> <p>Now the process becomes:</p> <ol> <li>Train a model on the training data.</li> <li>(Try to) improve the model's hyperparameters on the validation set.</li> <li>Evaluate the model on the test set.</li> </ol> <p>If our starting dataset contained 100 different patient records labels indicating who had heart disease and who didn't and we wanted to build a machine learning model to predict who had heart disease and who didn't, it might look like this:</p> <p>Since we know we're using a <code>RandomForestClassifier</code> and we know the hyperparameters we want to adjust, let's see what it looks like.</p> <p>First, let's remind ourselves of the base parameters.</p>"},{"location":"introduction-to-scikit-learn/#52-hyperparameter-tuning-with-randomizedsearchcv","title":"5.2 Hyperparameter tuning with <code>RandomizedSearchCV</code>\u00b6","text":"<p>Scikit-Learn's <code>sklearn.model_selection.RandomizedSearchCV</code> allows us to randomly search across different hyperparameters to see which work best.</p> <p>It also stores details about the ones which work best!</p> <p>Let's see it in action.</p> <p>First, we create a dictionary of parameter distributions (collections of different values for specific hyperparamters) we'd like to search over.</p> <p>This dictionary comes in the form:</p> <pre>param_distributions = {\"hyperparameter_name\": [values_to_randomly_try]}\n</pre> <p>Where <code>\"hyperparameter_name\"</code> is the value of a specific hyperparameter for a model and <code>[values_to_randomly_try]</code> is a list of values for that specific hyperparamter to randomly try.</p>"},{"location":"introduction-to-scikit-learn/#53-hyperparameter-tuning-with-gridsearchcv","title":"5.3 Hyperparameter tuning with <code>GridSearchCV</code>\u00b6","text":"<p>There's one more way we could try to improve our model's hyperparamters.</p> <p>And it's with <code>sklearn.model_selection.GridSearchCV</code>.</p> <p>The main difference between <code>GridSearchCV</code> and <code>RandomizedSearchCV</code> is <code>GridSearchCV</code> searches across a grid of hyperparamters exhaustively (it will try every combination possible), where as, <code>RandomizedSearchCV</code> searches across a grid of hyperparameters randomly (stopping after <code>n_iter</code> combinations).</p> <p><code>GridSearchCV</code> also refers to a dictionary of parameter distributions as a parameter grid (via the parameter <code>param_grid</code>).</p> <p>For example, let's see our dictionary of hyperparameters.</p>"},{"location":"introduction-to-scikit-learn/#6-saving-and-loading-trained-machine-learning-models","title":"6. Saving and loading trained machine learning models\u00b6","text":"<p>Our <code>GridSearchCV</code> model (<code>gs_clf</code>) has the best results so far, we'll export it and save it to file.</p>"},{"location":"introduction-to-scikit-learn/#61-saving-and-loading-a-model-with-pickle","title":"6.1 Saving and loading a model with <code>pickle</code>\u00b6","text":"<p>We saw right at the start, one way to save a model is using Python's <code>pickle</code> module.</p> <p>We'll use <code>pickle</code>'s <code>dump()</code> method and pass it our model, <code>gs_clf</code>, along with the <code>open()</code> function containing a string for the filename we want to save our model as, along with the <code>\"wb\"</code> string which stands for \"write binary\", which is the file type <code>open()</code> will write our model as.</p>"},{"location":"introduction-to-scikit-learn/#62-saving-and-loading-a-model-with-joblib","title":"6.2 Saving and loading a model with <code>joblib</code>\u00b6","text":"<p>The other way to load and save models is with <code>joblib</code>. Which works relatively the same as <code>pickle</code>.</p> <p>To save a model, we can use <code>joblib</code>'s <code>dump()</code> function, passing it the model (<code>gs_clf</code>) and the desired filename.</p>"},{"location":"introduction-to-scikit-learn/#7-revisiting-the-entire-pipeline","title":"7. Revisiting the entire pipeline\u00b6","text":"<p>We've covered a lot. And so far, it seems to be all over the place, which it is.</p> <p>But not to worry, machine learning projects often start out like this.</p> <p>A whole bunch of experimenting and code all over the place at the start and then once you've found something which works, the refinement process begins.</p> <p>What would this refinement process look like?</p> <p>We'll use the car sales regression problem (predicting the sale price of cars) as an example.</p> <p>To tidy things up, we'll be using Scikit-Learn's <code>sklearn.pipeline.Pipeline</code> class.</p> <p>You can imagine <code>Pipeline</code> as being a way to string a number of different Scikit-Learn processes together.</p>"},{"location":"introduction-to-scikit-learn/#71-creating-a-regression-pipeline","title":"7.1 Creating a regression <code>Pipeline</code>\u00b6","text":"<p>You might recall when, way back in Section 2: Getting Data Ready, we dealt with the car sales data, to build a regression model on it, we had to encode the categorical features into numbers and fill the missing data.</p> <p>The code we used worked, but it was a bit all over the place.</p> <p>Good news is, <code>Pipeline</code> can help us clean it up.</p> <p>Let's remind ourselves what the data looks like.</p>"},{"location":"introduction-to-scikit-learn/#where-to-next","title":"Where to next?\u00b6","text":"<p>If you've made it this far, congratulations! We've covered a lot of ground in the Scikit-Learn library.</p> <p>As you might've guessed, there's a lot more to be discovered.</p> <p>But for the time being, you should be equipped with some of the most useful features of the library to start trying to apply them to your own problems.</p> <p>Somewhere you might like to look next is to apply what you've learned above to a Kaggle competition.</p> <p>Kaggle competitions are great places to practice your data science and machine learning skills and compare your results with others.</p> <p>A great idea would be to try to combine the heart disease classification code, as well as the <code>Pipeline</code> code, to build a model for the Titanic dataset.</p> <p>Otherwise, if you'd like to figure out what else the Scikit-Learn library is capable of I'd highly recommend browsing through the Scikit-Learn User Guide and seeing what sparks your interest.</p> <p>Finally, as an extra-curriculum extension, you might want to look into trying out the CatBoost library for dealing with non-numerical data automatically.</p> <p>The CatBoost algorithm is advanced version of a decision tree (like a Random Forest with superpowers) and is used in production at several large technology companies, including Cloudflare.</p>"}]}